[
  {
    "objectID": "quarto-template.html",
    "href": "quarto-template.html",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "",
    "text": "#WOOOOOOOOP\noj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nnvars &lt;- format(round(ncol(oj), 0), \n                nsmall=0, \n                big.mark=\",\") \nnobs &lt;- format(round(nrow(oj), 0), \n                nsmall=0, \n                big.mark=\",\")\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red\nviolets are blue."
  },
  {
    "objectID": "quarto-template.html#data-summary",
    "href": "quarto-template.html#data-summary",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.1 Data Summary",
    "text": "2.1 Data Summary\n\nSummary statistics (Use skimr::skim())"
  },
  {
    "objectID": "quarto-template.html#data-visualization",
    "href": "quarto-template.html#data-visualization",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\noj %&gt;% \n  ggplot(aes(x = log(sales), \n             y = log(price),\n             color = brand)) +\n  geom_point(alpha = .1) +\n  geom_smooth(method = lm, se = F) +\n  facet_wrap(.~ad) +\n  theme_bw() +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "quarto-template.html#data-transformation",
    "href": "quarto-template.html#data-transformation",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.3 Data Transformation",
    "text": "2.3 Data Transformation\n\nob_sum1 &lt;- oj %&gt;% \n  group_by(brand, ad) %&gt;% \n  summarise(sales_tot = sum(sales, na.rm = T),\n            price_mean = round(mean(price, na.rm = T), 2))"
  },
  {
    "objectID": "quarto-template.html#analysis",
    "href": "quarto-template.html#analysis",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.4 Analysis",
    "text": "2.4 Analysis"
  },
  {
    "objectID": "quarto-template.html#quotes",
    "href": "quarto-template.html#quotes",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.5 Quotes",
    "text": "2.5 Quotes\n\nQuote with &gt;\n\n\n“The truth is rarely pure and never simple.”\n— Oscar Wilde"
  },
  {
    "objectID": "quarto-template.html#inserting-figures",
    "href": "quarto-template.html#inserting-figures",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.6 Inserting Figures",
    "text": "2.6 Inserting Figures\nFor a demonstration of a DANL tiger, see Figure 1.\n\n\n\n\n\nFigure 1: DANL Tiger"
  },
  {
    "objectID": "quarto-template.html#inserting-a-html-page",
    "href": "quarto-template.html#inserting-a-html-page",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.7 Inserting a HTML page",
    "text": "2.7 Inserting a HTML page"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome All!",
    "section": "",
    "text": "Welcome to my blog powered by Quarto and GitHub!!\n\nAs a Data Analytics major, it is helpful to hone my abilities by doing exploratory data analysis. With that, I will be posting little projects here and there on my website, feel free to take a look!"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#description-of-variables-for-spotify-data-frame",
    "href": "posts/Spotify_all/Spotify_All.html#description-of-variables-for-spotify-data-frame",
    "title": "Spotify Listener Data",
    "section": "Description of Variables for `spotify Data Frame:",
    "text": "Description of Variables for `spotify Data Frame:\n\npid: A unique ID for a specific playlist\nplaylist_name: The name of a specific playlist\npos: Position of the track within a playlist (starting from 0)\nartist_name: Name of the artist on the track\ntrack_name: name of the track\nduration_ms: duration of the track (in milliseconds)\nalbum_name: name of the track’s album"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#summary-statistics-for-spotify",
    "href": "posts/Spotify_all/Spotify_All.html#summary-statistics-for-spotify",
    "title": "Spotify Listener Data",
    "section": "Summary Statistics for spotify:",
    "text": "Summary Statistics for spotify:\n\nskim(spotify)\n\n\nData summary\n\n\nName\nspotify\n\n\nNumber of rows\n198005\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nplaylist_name\n0\n1\n2\n55\n0\n2184\n0\n\n\nartist_name\n0\n1\n1\n117\n0\n18866\n0\n\n\ntrack_name\n0\n1\n1\n216\n0\n64115\n0\n\n\nalbum_name\n0\n1\n1\n255\n0\n35497\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npid\n0\n1\n323435.50\n466890.21\n0\n738\n1476\n999210\n999998\n▇▁▁▁▃\n\n\npos\n0\n1\n54.39\n48.35\n0\n17\n40\n79\n248\n▇▃▂▁▁\n\n\nduration_ms\n0\n1\n234740.84\n132918.60\n0\n198000\n224693\n258533\n20744575\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#top-5-artists-in-artist_name",
    "href": "posts/Spotify_all/Spotify_All.html#top-5-artists-in-artist_name",
    "title": "Spotify Listener Data",
    "section": "Top 5 Artists in artist_name",
    "text": "Top 5 Artists in artist_name\n\nspot1 &lt;- spotify %&gt;% \n  group_by(artist_name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(5) %&gt;% \n  mutate(prop = n/sum(n),\n         art_fct = reorder(artist_name, prop))\nView(spot1)\n\nggplot(data = spot1)+\n  geom_bar(aes(x = art_fct, y = prop, fill = artist_name), stat = 'identity')+\n  theme(axis.text.x=element_text(angle=45,hjust=1,vjust=0.5))+\n  ggtitle('Frequency of Top 5 Artists')+\n  xlab('Artist Name')+\n  ylab('Proportion')\n\n\n\nlength(unique(spotify$artist_name))\n\n[1] 18866\n\n\n\nComments on analyses…\n\nOut of the 18866 different artist names in the spotify data frame, the top five are…\n\nDrake\nKanye West\nKendrick Lamar\nRihanna\nThe Weeknd\n\n\nThese are also in order of greatest to least frequency out of the top 5*"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#top-5-songs-in-track_name",
    "href": "posts/Spotify_all/Spotify_All.html#top-5-songs-in-track_name",
    "title": "Spotify Listener Data",
    "section": "Top 5 songs in track_name",
    "text": "Top 5 songs in track_name\n\nspot2 &lt;- spotify %&gt;% \n  group_by(track_name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(5) %&gt;% \n  mutate(prop = n/sum(n),\n         track_fct = reorder(track_name, prop))\n\nggplot(data = spot2)+\n  geom_bar(aes(x = track_fct, y = prop), stat = 'identity', fill = 'blue')+\n  theme(axis.text.x=element_text(angle=45,hjust=1,vjust=0.5))+\n  ggtitle('Top 5 Songs')+\n  xlab('Song Name')+\n  ylab('Proportion')\n\n\n\nlength(unique(spotify$track_name))\n\n[1] 64115\n\n\n\nComments on analyses…\n\nThere are a total of 64115 unique values in track_name\nOut of those 64115 values, the top 5 are…\n\nCloser\nOne Dance\nHUMBLE\nHome\nRoses\n\n\nIn order of decreasing proportion within the top 5 songs*"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#most-popular-song-in-track_name-for-each-of-the-top-5-artists-in-artist_name",
    "href": "posts/Spotify_all/Spotify_All.html#most-popular-song-in-track_name-for-each-of-the-top-5-artists-in-artist_name",
    "title": "Spotify Listener Data",
    "section": "Most Popular Song in track_name for Each of the Top 5 Artists in artist_name",
    "text": "Most Popular Song in track_name for Each of the Top 5 Artists in artist_name\n\nspot3 &lt;- spotify %&gt;% \n  filter(artist_name %in% spot1$art_fct) %&gt;% \n  group_by(artist_name, track_name) %&gt;% \n  summarise(n = n())\nspot3.5 &lt;- spot3 %&gt;% \n  group_by(artist_name) %&gt;% \n  slice_max(order_by = n, n = 1) %&gt;%  #Selects the top n value for each value of artist, the top n rows of top n values\n  ungroup() %&gt;% \n  arrange(-n)\n  \nggplot(data = spot3.5) + \n  geom_bar(aes(x = artist_name, y = n, fill = track_name), stat = 'identity')+\n  labs(title = 'Most Popular Song For Each Top 5 Artist',\n       x = 'Artist',\n       y = 'Count')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nComments on analyses…\n\nAbove is the top songtrack_name value and it’s count in the spotify data frame for each of the top 5 values of artist_name\n\nDrake : One Dance\nKanye West : GOLD DIGGER\nKendrick Lamar : HUMBLE\nRihanna : Needed Me\nThe Weeknd : Starboy"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#description-of-variables-for-nyc_rest_ins-data-frame",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#description-of-variables-for-nyc_rest_ins-data-frame",
    "title": "NYC Restaurant Inspections",
    "section": "Description of Variables for nyc_rest_ins data frame",
    "text": "Description of Variables for nyc_rest_ins data frame\n\nCAMIS: A unique identifier number for each restaurant\nDBA: Stands for “doing business as”, name of the restaurant\nBORO: Indentifies the NYC Borough that the restaurant is located in\n\nManhattan\nBronx\nBrooklyn\nQueens\nStaten Island\n\nSTREET: Street address of the restaurant\nCUISINE DESCRIPTION: Type of cuisine the restaurant sells\nINSPECTION DATE: The date the inspection was performed\nACTION: Indicated the action(s) taken as a result of the inspection\nVIOLATION CODE: Violation associated with the restaurant’s inspection\nVIOLATION DESCRIPTION: Descriprion of the violation associated with a restaurant’s inspection\nCRITICAL FLAG: Indicator of a critical violation as result of inspection\n\nCritical\nNot Critical\nNot Applicaple\n\nSCORE: Numeric Score for the inspection\nGrade: Letter Grade for the Inspection"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#summary-statisics-for-nyc_rest_ins",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#summary-statisics-for-nyc_rest_ins",
    "title": "NYC Restaurant Inspections",
    "section": "Summary Statisics for nyc_rest_ins:",
    "text": "Summary Statisics for nyc_rest_ins:\n\nskim(nyc_rest_ins)\n\n\nData summary\n\n\nName\nnyc_rest_ins\n\n\nNumber of rows\n17633\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDBA\n0\n1\n1\n75\n0\n13914\n0\n\n\nBORO\n0\n1\n5\n13\n0\n5\n0\n\n\nSTREET\n0\n1\n5\n40\n0\n2038\n0\n\n\nCUISINE.DESCRIPTION\n0\n1\n4\n30\n0\n87\n0\n\n\nINSPECTION.DATE\n0\n1\n10\n10\n0\n751\n0\n\n\nACTION\n0\n1\n33\n47\n0\n2\n0\n\n\nVIOLATION.CODE\n0\n1\n3\n5\n0\n60\n0\n\n\nVIOLATION.DESCRIPTION\n0\n1\n19\n940\n0\n101\n0\n\n\nCRITICAL.FLAG\n0\n1\n8\n12\n0\n2\n0\n\n\nGRADE\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCAMIS\n0\n1\n47112537.51\n4197913.0\n30191841\n41551354\n50049290\n50094455\n50133690\n▁▁▃▁▇\n\n\nSCORE\n0\n1\n10.44\n5.9\n0\n7\n10\n12\n86\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-boro",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-boro",
    "title": "NYC Restaurant Inspections",
    "section": "GRADE and BORO",
    "text": "GRADE and BORO\nGraphical representation of the proportion of each GRADE value in each of the 5 values in BORO\n\nnyc1 &lt;- nyc_rest_ins %&gt;% \n  group_by(BORO, GRADE) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\n\nggplot(data = nyc1,\n       mapping = aes(x = GRADE, y = prop))+\n  geom_bar(aes(fill = GRADE), stat = 'identity')+\n  facet_wrap(.~BORO)\n\n\n\n\n\nComments on analyses…\n\nAll five boroughs seem to have around equal proportions of each Grade"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#critical-flag-and-boro",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#critical-flag-and-boro",
    "title": "NYC Restaurant Inspections",
    "section": "CRITICAL FLAG and BORO",
    "text": "CRITICAL FLAG and BORO\nGraphical representation of the count of each value of CRITICAL FLAG per each value of BORO\n\nnyc2 &lt;- nyc_rest_ins %&gt;% \n  group_by(BORO, CRITICAL.FLAG) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\nView(nyc2)\n\nggplot(data = nyc2,\n       mapping = aes(x = CRITICAL.FLAG, y = n))+\n  geom_bar(aes(fill = CRITICAL.FLAG), stat = 'identity')+\n  facet_wrap(.~BORO)\n\n\n\n\n\nComments on analyses…\n\nThe proportion of each value of CRITICAL.FLAG within each of the values of BORO seems to be about equal\nThis faceted bar chart also gives info about the number of restaurants in each value of BORO within this data set. In order of most to least…\n\nManhattan has the most restaurants\nBrooklyn has the second most\nQueens the third most\nBronx second to last\nStaten Island is last"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-critical-flag",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-critical-flag",
    "title": "NYC Restaurant Inspections",
    "section": "GRADE and CRITICAL FLAG",
    "text": "GRADE and CRITICAL FLAG\nGraphical representation of the proportion of each value of GRADE per value of CRITICAL FLAG\n\nnyc3 &lt;- nyc_rest_ins %&gt;% \n  group_by(GRADE, CRITICAL.FLAG, BORO) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\nView(nyc3)\n\nggplot(data = nyc3)+\n  geom_bar(mapping = aes(x = CRITICAL.FLAG, y = prop, fill = GRADE), stat = 'identity', position = 'dodge')\n\n\n\n\n\nComments on analyses…\n\nThe proportion of each value of GRADE seems to be almost equal per value of CRITICAL FLAG\nThe proportion of grade A seems to be slightly less for restaurants with No Critical Flags\nThe proportions of grades B and C appear to be slightly less for restautants with Critical Flags"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#description-of-variables-for-beer-data-frame",
    "href": "posts/Beer_Mkts/beer_mkts.html#description-of-variables-for-beer-data-frame",
    "title": "Beer Market Data",
    "section": "Description of Variables for beer Data Frame:",
    "text": "Description of Variables for beer Data Frame:\n\nhh: Identification number of the household;\nX_purchase_desc: details of item purchased by hh\nquantity: number of items purchased by hh\nbrand: Brand of beer purchased by hh\n\nBud Light,\nBusch Light,\nCoors Light,\nMiller Lite, or\nNatural Light\n\ndollar_spent: Dollar value total of purchase;\nbeer_floz: volume of beer (fl oz);\nprice_per_floz: price per fl oz\ncontainer: Type of container of beer\npromo: Was the item promoted? (coupon, etc…)\nmarket: Scan-track market (or state if rural);\nOther demographic data is present in further variables as well…"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#summary-statistics-of-beer-data-frame",
    "href": "posts/Beer_Mkts/beer_mkts.html#summary-statistics-of-beer-data-frame",
    "title": "Beer Market Data",
    "section": "Summary Statistics of beer Data Frame:",
    "text": "Summary Statistics of beer Data Frame:\n\nskim(beer)\n\n\nData summary\n\n\nName\nbeer\n\n\nNumber of rows\n73115\n\n\nNumber of columns\n24\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nlogical\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nX_purchase_desc\n0\n1\n12\n29\n0\n115\n0\n\n\nbrand\n0\n1\n9\n13\n0\n5\n0\n\n\ncontainer\n0\n1\n3\n30\n0\n7\n0\n\n\nmarket\n0\n1\n5\n20\n0\n92\n0\n\n\nbuyertype\n0\n1\n4\n7\n0\n3\n0\n\n\nincome\n0\n1\n5\n8\n0\n5\n0\n\n\nage\n0\n1\n3\n5\n0\n4\n0\n\n\nemployment\n0\n1\n4\n4\n0\n3\n0\n\n\ndegree\n0\n1\n2\n7\n0\n4\n0\n\n\ncow\n0\n1\n4\n25\n0\n4\n0\n\n\nrace\n0\n1\n5\n8\n0\n5\n0\n\n\ntvcable\n0\n1\n4\n7\n0\n3\n0\n\n\nnpeople\n0\n1\n1\n5\n0\n5\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npromo\n0\n1\n0.20\nFAL: 58563, TRU: 14552\n\n\nchildrenUnder6\n0\n1\n0.07\nFAL: 68109, TRU: 5006\n\n\nchildren6to17\n0\n1\n0.20\nFAL: 58155, TRU: 14960\n\n\nmicrowave\n0\n1\n0.99\nTRU: 72676, FAL: 439\n\n\ndishwasher\n0\n1\n0.73\nTRU: 53258, FAL: 19857\n\n\nsinglefamilyhome\n0\n1\n0.81\nTRU: 59058, FAL: 14057\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhh\n0\n1\n17407721.61\n11582147.34\n2000235.00\n8223438.00\n8413624.00\n30171315.00\n30440718.00\n▂▇▁▁▇\n\n\nquantity\n0\n1\n1.32\n1.15\n1.00\n1.00\n1.00\n1.00\n48.00\n▇▁▁▁▁\n\n\ndollar_spent\n0\n1\n13.78\n8.72\n0.51\n8.97\n12.99\n16.38\n159.13\n▇▁▁▁▁\n\n\nbeer_floz\n0\n1\n265.93\n199.52\n12.00\n144.00\n216.00\n360.00\n9216.00\n▇▁▁▁▁\n\n\nprice_per_floz\n0\n1\n0.06\n0.01\n0.00\n0.05\n0.06\n0.06\n0.23\n▃▇▁▁▁"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#distribution-of-brand",
    "href": "posts/Beer_Mkts/beer_mkts.html#distribution-of-brand",
    "title": "Beer Market Data",
    "section": "Distribution of brand",
    "text": "Distribution of brand\n\nbeer1 &lt;- beer %&gt;% \n  group_by(brand) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(prop = n/sum(n),\n         brand_fct = reorder(brand, prop))\n\nggplot(data = beer1)+\n  geom_bar(aes(x = brand_fct, y = prop), stat = 'identity', fill = 'blue')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Frequency of Beer Brands',\n       x = 'Brand',\n       y = 'Proportion')\n\n\n\n\n\nComments on analyses…\n\nThe brands above are in an increasing order of proportion among the brands in beer data frame\n\nThe brands are ordered as follows…\n\nBusch Light\nNatural Light\nCoors Light\nMiller Lite\nBud Light"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#brand-and-dollar-spent",
    "href": "posts/Beer_Mkts/beer_mkts.html#brand-and-dollar-spent",
    "title": "Beer Market Data",
    "section": "brand and dollar spent",
    "text": "brand and dollar spent\n\nbeer2 &lt;- beer %&gt;% \n  group_by(brand) %&gt;% \n  summarise(dollar_spent_tot = sum(dollar_spent)) %&gt;% \n  mutate(brand_fct = reorder(brand, dollar_spent_tot)) %&gt;% \n  arrange(-dollar_spent_tot)\nView(beer2)\n\nggplot(beer2)+\n  geom_bar(aes(x = brand_fct, y = dollar_spent_tot), stat = 'identity', fill = 'red')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Dollars Spent Per Brand',\n       x = 'Brand',\n       y = 'Dollars Spent')\n\n\n\n\n\nComments on analyses….\n\nThe order is the same as the distribution of brand within the beer data frame\n\nJust now the y-axis is measuring dollars spent by each value of hh on each brand\n\nBud Light seems to be most popular all around"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#container-and-brand",
    "href": "posts/Beer_Mkts/beer_mkts.html#container-and-brand",
    "title": "Beer Market Data",
    "section": "container and brand",
    "text": "container and brand\n\nbeer3 &lt;- beer %&gt;% \n  group_by(brand, container) %&gt;% \n  summarise(n = n())\nbeer3.5 &lt;- beer3 %&gt;% \n  group_by(brand) %&gt;% \n  slice_max(order_by = n, n = 2)\nggplot(data = beer3.5)+\n  geom_bar(aes(x = brand, y = n, fill = container), stat = 'identity', position = 'dodge')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Frequency of Cans and Bottles for Each Brand',\n       x = 'Brand',\n       y = 'Count')\n\n\n\nView(beer3.5)\n\n\nComments on analyses…\n\nThe bar chart above depicts the frequency of two container types (can or non refillable bottle) per each brand\n\nAll brands seem to sell more cans than bottles\n\nOf them Bud Light sells the most cans\n\nBud light also sells the most bottles"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html",
    "href": "personal_posts/New Post/Cancer_DF_Post.html",
    "title": "Cancer Data",
    "section": "",
    "text": "import pandas as pd\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.available\n\n['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']\n\n\n\nplt.style.use('seaborn-v0_8-darkgrid')"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html#normalizing-the-geographic-locations",
    "href": "personal_posts/New Post/Cancer_DF_Post.html#normalizing-the-geographic-locations",
    "title": "Cancer Data",
    "section": "Normalizing the geographic locations",
    "text": "Normalizing the geographic locations\n\n# Below I normalized the geographic locations since there are a number of them that are in avg_hh_size, but not in can_reg\n  # So below I have filtered the avg_hh_size df to only include those in the can_reg df, so that any analyses are going off of the same number of observations\n\n\ncan_reg_locs = can_reg['geography']\ncan_reg_locs\n\n0          Kitsap County, Washington\n1        Kittitas County, Washington\n2       Klickitat County, Washington\n3           Lewis County, Washington\n4         Lincoln County, Washington\n                    ...             \n3042        Ellsworth County, Kansas\n3043           Finney County, Kansas\n3044             Ford County, Kansas\n3045         Franklin County, Kansas\n3046            Geary County, Kansas\nName: geography, Length: 3047, dtype: object\n\n\n\navg_hh_size['geography']\n\n0           Aleutians East Borough, Alaska\n1       Aleutians West Census Area, Alaska\n2           Anchorage Municipality, Alaska\n3               Bethel Census Area, Alaska\n4              Bristol Bay Borough, Alaska\n                       ...                \n3215            Sweetwater County, Wyoming\n3216                 Teton County, Wyoming\n3217                 Uinta County, Wyoming\n3218              Washakie County, Wyoming\n3219                Weston County, Wyoming\nName: geography, Length: 3220, dtype: object\n\n\n\ngeo_cond = avg_hh_size['geography'].isin(can_reg_locs)\navg_hh_size_adj = avg_hh_size[geo_cond]\navg_hh_size_adj['geography'].nunique() # Now have done this to normalize\n\n3047"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html#splitting-the-geography-variable-in-each-df",
    "href": "personal_posts/New Post/Cancer_DF_Post.html#splitting-the-geography-variable-in-each-df",
    "title": "Cancer Data",
    "section": "Splitting the geography variable in each df",
    "text": "Splitting the geography variable in each df\n\n# Within each df there is a variable `geography`, which is a combination of county and state, separated by a comma\n  # If I want to do any county/state level analyses independently, this format is not great\n    # So I have split this var in each df to create two separate `state` and `county` variables\n\n\navg_hh_size_adj = (\n    avg_hh_size_adj\n    .assign(\n        county = lambda x: x['geography'].str.split(', ').str[0],\n        state = lambda x: x['geography'].str.split(', ').str[1]\n    )\n    .drop(columns = 'geography')\n)\n\navg_hh_size_adj.sort_values(['state','county'])\n\n\n  \n    \n\n\n\n\n\n\nstatefips\ncountyfips\navghouseholdsize\ncounty\nstate\n\n\n\n\n29\n1\n1\n2.68\nAutauga County\nAlabama\n\n\n30\n1\n3\n2.60\nBaldwin County\nAlabama\n\n\n31\n1\n5\n2.61\nBarbour County\nAlabama\n\n\n32\n1\n7\n2.95\nBibb County\nAlabama\n\n\n33\n1\n9\n2.74\nBlount County\nAlabama\n\n\n...\n...\n...\n...\n...\n...\n\n\n3215\n56\n37\n2.64\nSweetwater County\nWyoming\n\n\n3216\n56\n39\n2.63\nTeton County\nWyoming\n\n\n3217\n56\n41\n2.76\nUinta County\nWyoming\n\n\n3218\n56\n43\n2.34\nWashakie County\nWyoming\n\n\n3219\n56\n45\n2.27\nWeston County\nWyoming\n\n\n\n\n\n3047 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncan_reg_adj = (\n    can_reg\n    .assign(\n        county = lambda x: x['geography'].str.split(', ').str[0],\n        state = lambda x: x['geography'].str.split(', ').str[1] # Had an issue with merging at first since I failed to put a space after the comma here as I did in the adj above.\n    )\n    .drop(columns = 'geography')\n)\n\ncan_reg_adj.sort_values(['state','county'])\n\nWarning: Total number of columns (34) exceeds max_columns (20). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\navganncount\navgdeathsperyear\ntarget_deathrate\nincidencerate\nmedincome\npopest2015\npovertypercent\nstudypercap\nbinnedinc\nmedianage\n...\npctpubliccoverage\npctpubliccoveragealone\npctwhite\npctblack\npctasian\npctotherrace\npctmarriedhouseholds\nbirthrate\ncounty\nstate\n\n\n\n\n500\n266.0\n99\n178.3\n475.0\n54366\n55347\n13.1\n0.000000\n(51046.4, 54545.6]\n37.7\n...\n30.0\n14.3\n77.399902\n18.679488\n0.967023\n0.925373\n56.069818\n5.872131\nAutauga County\nAlabama\n\n\n501\n1072.0\n424\n174.3\n454.6\n49626\n203709\n13.0\n14.726890\n(48021.6, 51046.4]\n42.2\n...\n33.7\n17.5\n86.431496\n9.601734\n0.669841\n0.958380\n53.935010\n5.381478\nBaldwin County\nAlabama\n\n\n1007\n155.0\n62\n192.7\n477.5\n34971\n26489\n25.4\n0.000000\n(34218.1, 37413.8]\n38.8\n...\n46.6\n26.5\n47.363731\n46.765929\n0.905985\n3.445715\n43.276946\n6.266977\nBarbour County\nAlabama\n\n\n1416\n125.0\n53\n212.4\n494.6\n39546\n22583\n18.1\n929.903024\n(37413.8, 40362.7]\n38.9\n...\n39.4\n22.7\n76.654574\n21.438683\n0.092904\n0.030968\n57.335990\n4.179861\nBibb County\nAlabama\n\n\n2684\n291.0\n120\n175.4\n429.9\n45567\n57673\n17.5\n0.000000\n(45201, 48021.6]\n40.7\n...\n35.9\n19.0\n95.097903\n1.531797\n0.143823\n0.961705\n59.439854\n6.894123\nBlount County\nAlabama\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1189\n155.0\n49\n141.1\n403.5\n72604\n44626\n9.8\n0.000000\n(61494.5, 125635]\n33.6\n...\n21.9\n11.3\n92.133476\n0.866613\n0.857679\n1.684088\n54.385755\n8.929776\nSweetwater County\nWyoming\n\n\n1190\n83.0\n23\n136.9\n402.7\n75348\n23125\n7.7\n0.000000\n(61494.5, 125635]\n38.0\n...\n14.4\n5.5\n93.160325\n0.322711\n1.900408\n2.783380\n50.580188\n1.745232\nTeton County\nWyoming\n\n\n1191\n68.0\n24\n132.8\n359.3\n56800\n20822\n10.0\n0.000000\n(54545.6, 61494.5]\n34.6\n...\n24.4\n13.0\n94.233158\n0.195891\n0.119446\n1.538462\n57.451346\n6.918372\nUinta County\nWyoming\n\n\n1192\n50.0\n21\n182.3\n448.0\n50802\n8328\n12.6\n0.000000\n(48021.6, 51046.4]\n43.4\n...\n32.2\n13.9\n91.238095\n0.678571\n0.166667\n4.142857\n51.708428\n5.370370\nWashakie County\nWyoming\n\n\n1193\n37.0\n17\n182.6\n397.1\n55520\n7234\n13.2\n0.000000\n(54545.6, 61494.5]\n42.1\n...\n29.4\n12.4\n94.728747\n0.209732\n0.531320\n0.069911\n52.344273\n17.401046\nWeston County\nWyoming\n\n\n\n\n\n3047 rows × 34 columns"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html#merging-the-two-dfs-together",
    "href": "personal_posts/New Post/Cancer_DF_Post.html#merging-the-two-dfs-together",
    "title": "Cancer Data",
    "section": "Merging the two DFs together",
    "text": "Merging the two DFs together\n\n# These two dfs have two key variables now, 'state', and 'county'\n  # Using these two keys, a merge can now be performed\n\n\ncan_reg_adj['county'].value_counts().to_frame()\n\n\n  \n    \n\n\n\n\n\n\ncount\n\n\ncounty\n\n\n\n\n\nWashington County\n30\n\n\nJefferson County\n25\n\n\nLincoln County\n23\n\n\nFranklin County\n23\n\n\nJackson County\n21\n\n\n...\n...\n\n\nAmelia County\n1\n\n\nAlbemarle County\n1\n\n\nAccomack County\n1\n\n\nWindsor County\n1\n\n\nGeary County\n1\n\n\n\n\n\n1819 rows × 1 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\navg_hh_size_adj['county'].value_counts().to_frame()\n\n\n  \n    \n\n\n\n\n\n\ncount\n\n\ncounty\n\n\n\n\n\nWashington County\n30\n\n\nJefferson County\n25\n\n\nLincoln County\n23\n\n\nFranklin County\n23\n\n\nJackson County\n21\n\n\n...\n...\n\n\nWinn Parish\n1\n\n\nWest Feliciana Parish\n1\n\n\nWest Carroll Parish\n1\n\n\nWest Baton Rouge Parish\n1\n\n\nWeston County\n1\n\n\n\n\n\n1819 rows × 1 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncan_reg_w_avg_hh_size = can_reg_adj.merge(avg_hh_size_adj, on = ['county', 'state'], how = 'left')\ncan_reg_w_avg_hh_size # This is now the aggregate data frame of both the two obtained from the kaggle page\n# The only difference now is that the cancer data includes average household size per each geographic area now, allowing for analyses to be made with that variable as well\n\nWarning: Total number of columns (37) exceeds max_columns (20). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\navganncount\navgdeathsperyear\ntarget_deathrate\nincidencerate\nmedincome\npopest2015\npovertypercent\nstudypercap\nbinnedinc\nmedianage\n...\npctblack\npctasian\npctotherrace\npctmarriedhouseholds\nbirthrate\ncounty\nstate\nstatefips\ncountyfips\navghouseholdsize\n\n\n\n\n0\n1397.000000\n469\n164.9\n489.800000\n61898\n260131\n11.2\n499.748204\n(61494.5, 125635]\n39.3\n...\n2.594728\n4.821857\n1.843479\n52.856076\n6.118831\nKitsap County\nWashington\n53\n35\n2.54\n\n\n1\n173.000000\n70\n161.3\n411.600000\n48127\n43269\n18.6\n23.111234\n(48021.6, 51046.4]\n33.0\n...\n0.969102\n2.246233\n3.741352\n45.372500\n4.333096\nKittitas County\nWashington\n53\n37\n2.34\n\n\n2\n102.000000\n50\n174.7\n349.700000\n49348\n21026\n14.6\n47.560164\n(48021.6, 51046.4]\n45.0\n...\n0.739673\n0.465898\n2.747358\n54.444868\n3.729488\nKlickitat County\nWashington\n53\n39\n2.62\n\n\n3\n427.000000\n202\n194.8\n430.400000\n44243\n75882\n17.1\n342.637253\n(42724.4, 45201]\n42.8\n...\n0.782626\n1.161359\n1.362643\n51.021514\n4.603841\nLewis County\nWashington\n53\n41\n2.52\n\n\n4\n57.000000\n26\n144.4\n350.100000\n49955\n10321\n12.5\n0.000000\n(48021.6, 51046.4]\n48.3\n...\n0.270192\n0.665830\n0.492135\n54.027460\n6.796657\nLincoln County\nWashington\n53\n43\n2.34\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3042\n1962.667684\n15\n149.6\n453.549422\n46961\n6343\n12.4\n0.000000\n(45201, 48021.6]\n44.2\n...\n3.837754\n0.327613\n1.700468\n51.063830\n7.773512\nEllsworth County\nKansas\n20\n53\n2.08\n\n\n3043\n1962.667684\n43\n150.1\n453.549422\n48609\n37118\n18.8\n377.175494\n(48021.6, 51046.4]\n30.4\n...\n2.326771\n4.044920\n14.130288\n52.007937\n8.186470\nFinney County\nKansas\n20\n55\n2.90\n\n\n3044\n1962.667684\n46\n153.9\n453.549422\n51144\n34536\n15.0\n1968.959926\n(51046.4, 54545.6]\n30.9\n...\n2.313188\n1.316472\n5.680705\n55.153949\n7.809192\nFord County\nKansas\n20\n57\n3.04\n\n\n3045\n1962.667684\n52\n175.0\n453.549422\n50745\n25609\n13.3\n0.000000\n(48021.6, 51046.4]\n39.0\n...\n1.176562\n0.244632\n2.131790\n58.484232\n7.582938\nFranklin County\nKansas\n20\n59\n2.56\n\n\n3046\n1962.667684\n48\n213.6\n453.549422\n41193\n37030\n13.9\n0.000000\n(40362.7, 42724.4]\n26.2\n...\n16.590100\n3.177753\n1.356457\n56.040242\n8.981723\nGeary County\nKansas\n20\n61\n2.83\n\n\n\n\n\n3047 rows × 37 columns"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html#summary-statistics",
    "href": "personal_posts/New Post/Cancer_DF_Post.html#summary-statistics",
    "title": "Cancer Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nnum_only = can_reg_w_avg_hh_size.select_dtypes(exclude = 'object') # Separation of DF by data type\nnonnum_only = can_reg_w_avg_hh_size.select_dtypes(include = 'object')\nnum_only_first17 = num_only.iloc[:, 0:17]\nnum_only_last17 = num_only.iloc[:, 17:] # Done to allow the pandas DF to fully display the summary stats\n\n\nNumeric Variables Only\n\nSeparated into two DFs with 17 variables each\n\n\nnum_only_first17.describe()\n\n\n  \n    \n\n\n\n\n\n\navganncount\navgdeathsperyear\ntarget_deathrate\nincidencerate\nmedincome\npopest2015\npovertypercent\nstudypercap\nmedianage\nmedianagemale\nmedianagefemale\npercentmarried\npctnohs18_24\npcths18_24\npctsomecol18_24\npctbachdeg18_24\npcths25_over\n\n\n\n\ncount\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3.047000e+03\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n762.000000\n3047.000000\n3047.000000\n\n\nmean\n606.338544\n185.965868\n178.664063\n448.268586\n47063.281917\n1.026374e+05\n16.878175\n155.399415\n45.272333\n39.570725\n42.145323\n51.773679\n18.224450\n35.002068\n40.977034\n6.158287\n34.804660\n\n\nstd\n1416.356223\n504.134286\n27.751511\n54.560733\n12040.090836\n3.290592e+05\n6.409087\n529.628366\n45.304480\n5.226017\n5.292849\n6.896928\n8.093064\n9.069722\n11.115805\n4.529059\n7.034924\n\n\nmin\n6.000000\n3.000000\n59.700000\n201.300000\n22640.000000\n8.270000e+02\n3.200000\n0.000000\n22.300000\n22.400000\n22.300000\n23.100000\n0.000000\n0.000000\n7.100000\n0.000000\n7.500000\n\n\n25%\n76.000000\n28.000000\n161.200000\n420.300000\n38882.500000\n1.168400e+04\n12.150000\n0.000000\n37.700000\n36.350000\n39.100000\n47.750000\n12.800000\n29.200000\n34.000000\n3.100000\n30.400000\n\n\n50%\n171.000000\n61.000000\n178.100000\n453.549422\n45207.000000\n2.664300e+04\n15.900000\n0.000000\n41.000000\n39.600000\n42.400000\n52.400000\n17.100000\n34.700000\n40.400000\n5.400000\n35.300000\n\n\n75%\n518.000000\n149.000000\n195.200000\n480.850000\n52492.000000\n6.867100e+04\n20.400000\n83.650776\n44.000000\n42.500000\n45.300000\n56.400000\n22.700000\n40.700000\n46.400000\n8.200000\n39.650000\n\n\nmax\n38150.000000\n14010.000000\n362.800000\n1206.900000\n125635.000000\n1.017029e+07\n47.400000\n9762.308998\n624.000000\n64.700000\n65.700000\n72.500000\n64.100000\n72.500000\n79.000000\n51.800000\n54.800000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nnum_only_last17.describe()\n\n\n  \n    \n\n\n\n\n\n\npctbachdeg25_over\npctemployed16_over\npctunemployed16_over\npctprivatecoverage\npctprivatecoveragealone\npctempprivcoverage\npctpubliccoverage\npctpubliccoveragealone\npctwhite\npctblack\npctasian\npctotherrace\npctmarriedhouseholds\nbirthrate\nstatefips\ncountyfips\navghouseholdsize\n\n\n\n\ncount\n3047.000000\n2895.000000\n3047.000000\n3047.000000\n2438.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n3047.000000\n\n\nmean\n13.282015\n54.152642\n7.852412\n64.354939\n48.453774\n41.196324\n36.252642\n19.240072\n83.645286\n9.107978\n1.253965\n1.983523\n51.243872\n5.640306\n30.272071\n101.802429\n2.529682\n\n\nstd\n5.394756\n8.315064\n3.452371\n10.647057\n10.083006\n9.447687\n7.841741\n6.113041\n16.380025\n14.534538\n2.610276\n3.517710\n6.572814\n1.985816\n15.054873\n104.315612\n0.248449\n\n\nmin\n2.500000\n17.600000\n0.400000\n22.300000\n15.700000\n13.500000\n11.200000\n2.600000\n10.199155\n0.000000\n0.000000\n0.000000\n22.992490\n0.000000\n1.000000\n1.000000\n1.860000\n\n\n25%\n9.400000\n48.600000\n5.500000\n57.200000\n41.000000\n34.500000\n30.900000\n14.850000\n77.296180\n0.620675\n0.254199\n0.295172\n47.763063\n4.521419\n19.000000\n35.000000\n2.380000\n\n\n50%\n12.300000\n54.500000\n7.600000\n65.100000\n48.700000\n41.100000\n36.300000\n18.800000\n90.059774\n2.247576\n0.549812\n0.826185\n51.669941\n5.381478\n29.000000\n79.000000\n2.500000\n\n\n75%\n16.100000\n60.300000\n9.700000\n72.100000\n55.600000\n47.700000\n41.550000\n23.100000\n95.451693\n10.509732\n1.221037\n2.177960\n55.395132\n6.493677\n45.000000\n133.000000\n2.640000\n\n\nmax\n42.200000\n80.100000\n29.400000\n92.300000\n78.900000\n70.700000\n65.100000\n46.600000\n100.000000\n85.947799\n42.619425\n41.930251\n78.075397\n21.326165\n56.000000\n840.000000\n3.970000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nNon-Numeric Variables Only\n\nnonnum_only.describe()\n\n\n  \n    \n\n\n\n\n\n\nbinnedinc\ncounty\nstate\n\n\n\n\ncount\n3047\n3047\n3047\n\n\nunique\n10\n1819\n51\n\n\ntop\n(54545.6, 61494.5]\nWashington County\nTexas\n\n\nfreq\n306\n30\n233"
  },
  {
    "objectID": "personal_posts/New Post/Cancer_DF_Post.html#frequency-of-each-state-in-the-data-frame",
    "href": "personal_posts/New Post/Cancer_DF_Post.html#frequency-of-each-state-in-the-data-frame",
    "title": "Cancer Data",
    "section": "Frequency of Each State in the Data Frame",
    "text": "Frequency of Each State in the Data Frame\n\nstates = (\n    nonnum_only\n    .groupby('state')\n    .agg(\n        n = ('state', 'size')\n    )\n    .sort_values('n', ascending = True)\n)\n\nstates_order = states.index\n\n\nsns.countplot(data = nonnum_only,\n              x = 'state',\n              order = states_order)\nplt.xticks(rotation = 90)\nplt.figure(figsize = (10, 50))\nplt.show()\n\n\n\n\n&lt;Figure size 1000x5000 with 0 Axes&gt;"
  },
  {
    "objectID": "class_projects.html",
    "href": "class_projects.html",
    "title": "Class Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDANL 210 Project\n\n\nYfinance Data Collection and ESG Data Exploration\n\n\n\n\n\n\nJan 28, 2024\n\n\nDaniel Noone\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nDANL 200 Project\n\n\nAir Quality Data as per the EPA regarding Criteria Air Pollutants\n\n\n\n\n\n\nJan 28, 2024\n\n\nDaniel Noone\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCancer Data\n\n\nData found on kaggle.com (here)\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nWeb Scraping using Beautiful Soup\n\n\nFrom a tutorial on YouTube\n\n\n\n\n\n\nFeb 9, 2024\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "basic-python-intro.html",
    "href": "basic-python-intro.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#what-is-python",
    "href": "basic-python-intro.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#variables-and-data-types",
    "href": "basic-python-intro.html#variables-and-data-types",
    "title": "Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "basic-python-intro.html#control-structures",
    "href": "basic-python-intro.html#control-structures",
    "title": "Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "basic-python-intro.html#functions",
    "href": "basic-python-intro.html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "basic-python-intro.html#lists-and-dictionaries",
    "href": "basic-python-intro.html#lists-and-dictionaries",
    "title": "Introduction to Python",
    "section": "Lists and Dictionaries",
    "text": "Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "class_blog.html",
    "href": "class_blog.html",
    "title": "Class Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSpotify HW2\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPython Basics\n\n\nA quick look at the basics of python programming\n\n\n\n\n\n\nFeb 10, 2024\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nSpotify Listener Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nHW5 - NFL Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nBeer Market Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nNYC Restaurant Inspections\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome All!\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Daniel Noone is a Junior at SUNY Geneseo and is majoring in Data Analytics.  He was born and raised in Ontario, New York and loves to spend time outdoors."
  },
  {
    "objectID": "index.html#who-is-daniel-noone",
    "href": "index.html#who-is-daniel-noone",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Daniel Noone is a Junior at SUNY Geneseo and is majoring in Data Analytics.  He was born and raised in Ontario, New York and loves to spend time outdoors."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel A. Noone",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Data Analytics | Aug 2022 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel A. Noone",
    "section": "Experience",
    "text": "Experience\nExperience programming in R, as well as basic level knowledge of Python. Extensive experience with Microsoft Office programs such as Excel, Word, and PowerPoint."
  },
  {
    "objectID": "personal_posts/web_scraping1/wep_scrape_tutorial.html",
    "href": "personal_posts/web_scraping1/wep_scrape_tutorial.html",
    "title": "Web Scraping using Beautiful Soup",
    "section": "",
    "text": "First you have to import the required modules\n\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n\nHave to set the URL to the website and"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html",
    "title": "HW5 - NFL Data",
    "section": "",
    "text": "My repository can be found here"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#description-of-variables-in-nfl-data-frame",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#description-of-variables-in-nfl-data-frame",
    "title": "HW5 - NFL Data",
    "section": "Description of Variables in NFL Data Frame:",
    "text": "Description of Variables in NFL Data Frame:\n\nplay_id: Play Id number (numeric), when combined with game_id provides id for single play\ngame_id: 10 digit id for an NFL game\ndrive: Drive number (numeric)\nweek: Season week\nposteam: abv for team with possession\nqtr: Quarter in the game (qtr 5 means overtime)\nhalf_seconds_remaining: seconds remaining in the half\ndown: Down for a given play\npass: 1 if a pass play, 0 if not\nwp: Estimate for winning probability for posteam given situation at beginning of the play"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#summary-statistics-for-nfl-data-frame",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#summary-statistics-for-nfl-data-frame",
    "title": "HW5 - NFL Data",
    "section": "Summary Statistics for NFL Data Frame",
    "text": "Summary Statistics for NFL Data Frame\n\nskim(NFL)\n\n\nData summary\n\n\nName\nNFL\n\n\nNumber of rows\n50147\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngame_id\n0\n1.00\n13\n15\n0\n284\n0\n\n\nposteam\n3720\n0.93\n2\n3\n0\n32\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nplay_id\n0\n1.00\n2057.86\n1194.22\n1\n1039.00\n2034.00\n3065.50\n5523\n▇▇▇▅▁\n\n\ndrive\n450\n0.99\n11.48\n6.59\n1\n6.00\n11.00\n17.00\n35\n▇▇▇▂▁\n\n\nweek\n0\n1.00\n9.91\n5.61\n1\n5.00\n10.00\n15.00\n22\n▇▆▆▆▃\n\n\nqtr\n0\n1.00\n2.58\n1.14\n1\n2.00\n3.00\n4.00\n5\n▆▇▆▇▁\n\n\ndown\n8543\n0.83\n2.00\n1.00\n1\n1.00\n2.00\n3.00\n4\n▇▆▁▃▂\n\n\nhalf_seconds_remaining\n0\n1.00\n796.94\n564.41\n0\n255.00\n774.00\n1285.00\n1800\n▇▅▅▅▅\n\n\npass\n0\n1.00\n0.45\n0.50\n0\n0.00\n0.00\n1.00\n1\n▇▁▁▁▆\n\n\nwp\n284\n0.99\n0.51\n0.29\n0\n0.29\n0.52\n0.73\n1\n▆▆▇▆▆"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2a",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2a",
    "title": "HW5 - NFL Data",
    "section": "Q2a",
    "text": "Q2a\n\nIn data.frame, NFL2022_stuffs, remove observations for which values of posteam is missing.\n\nAnswer:\n\nNFL &lt;- NFL %&gt;% \n  filter(!(is.na(posteam)))"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2b",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2b",
    "title": "HW5 - NFL Data",
    "section": "Q2b",
    "text": "Q2b\n\nSummarize the mean value of pass for each posteam when all the following conditions hold:\n\nwp is greater than 20% and less than 75%;\ndown is less than or equal to 2; and\nhalf_seconds_remaining is greater than 120.\n\n\nAnswer:\n\nNFL_2b &lt;- NFL %&gt;% \n  filter(wp &gt; 0.2 & wp &lt; 0.75 & down &lt;= 2 & half_seconds_remaining &gt; 120)"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2c",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2c",
    "title": "HW5 - NFL Data",
    "section": "Q2c",
    "text": "Q2c\n\nProvide both (1) a ggplot code with geom_point() using the resulting data.frame in Q2b and (2) a simple comments to describe the mean value of pass for each posteam.\n\nIn the ggplot, reorder the posteam categories based on the mean value of pass in ascending or in descending order.\n\n\nAnswer\n\nNFL_2c &lt;- NFL_2b %&gt;%\n  group_by(posteam) %&gt;% \n  mutate(mean_pass = mean(pass)) %&gt;% \n  ungroup() %&gt;% \n  mutate(posteam_fct = reorder(posteam, mean_pass))\n\nggplot(data = NFL_2c, mapping = aes(x = mean_pass, y = posteam_fct))+\n  geom_point()+\n  labs(title = 'Possessing Team and Pass Plays',\n       x = 'Percentage of Pass Plays',\n       y = 'Team with possession')+\n  theme(axis.text.y = element_text(hjust = 1))+\n  theme_minimal()\n\n\n\n\nOn the scatter plot there is a positive relationship between posteam and the percentage of pass plays  Teams such as CIN, KC, LAC. and BUF have a higher occurance of pass plays,  While teams such as ATL, WAS, CHI, and NO have a lower occurance of pass plays"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2d",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2d",
    "title": "HW5 - NFL Data",
    "section": "Q2d",
    "text": "Q2d\n\nConsider the following data.frame, NFL2022_epa:\n\n\nNFL2022_epa &lt;- read_csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\n\nCreate the data.frame, NFL2022_stuffs_EPA, that includes\n\nAll the variables in the data.frame, NFL;\nThe variables, passer, receiver, and epa, from the data.frame, NFL2022_epa. by joining the two data.frames.\n\nIn the resulting data.frame, NFL2022_stuffs_EPA, remove observations with NA in passer.\n\nAnswer:\n\nNFL2022_stuffs_EPA &lt;- NFL %&gt;% \n  left_join(NFL2022_epa)\nView(NFL2022_stuffs_EPA)\n\nNFL2022_stuffs_EPA &lt;- NFL2022_stuffs_EPA %&gt;% \n  filter(!(is.na(passer)))"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2e",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2e",
    "title": "HW5 - NFL Data",
    "section": "Q2e",
    "text": "Q2e\n\nProvide both (1) a single ggplot and (2) a simple comment to describe the NFL weekly trend of weekly mean value of epa for each of the following two passers,\n\n\"J.Allen\"\n\"P.Mahomes\"\n\n\nAnswer:\n\nNFL_2e &lt;- NFL2022_stuffs_EPA %&gt;% \n  filter(passer == \"J.Allen\" | passer == \"P.Mahomes\") %&gt;% \n  group_by(passer, week) %&gt;% \n  summarise(mean_epa = mean(epa))\n\nggplot(data = NFL_2e,\n       mapping = aes(x = week, y = mean_epa, color = passer))+\n  geom_line(size = 1, linejoin = 'round')+\n  geom_point(size = 1.5)+\n  theme(legend.position = 'top')+\n  labs(title = \"Mean EPA for Two NFL Passers\",\n       x = 'Week',\n       y = 'Mean EPA')+\n  scale_color_manual(values = c('blue', 'red'))\n\n\n\n\nP.Mahomes seems to have a higher mean EPA most of the weeks, while J.Allen seems to lag behind"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2f",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2f",
    "title": "HW5 - NFL Data",
    "section": "Q2f",
    "text": "Q2f\n\nCalculate the difference between the mean value of epa for \"J.Allen\" the mean value of epa for \"P.Mahomes\" for each value of week\n\nAnswer:\n\nNFL_2f &lt;- NFL_2e %&gt;% \n  spread(passer, mean_epa) %&gt;% \n  mutate(mean_epa_dif = J.Allen - P.Mahomes)"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2g",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2g",
    "title": "HW5 - NFL Data",
    "section": "Q2g",
    "text": "Q2g\n\nSummarize the resulting data.frame in Q2d, with the following four variables:\n\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop, and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks.)\nmean_epa: Mean value of epa in 2022 for each passer\nn_pass: Number of observations for each passer\n\nThen find the top 10 NFL passers in 2022 in terms of the mean value of epa, conditioning that n_pass must be greater than or equal to the third quantile level of n_pass.\n\nAnswer:\n\nNFL_2g &lt;- NFL2022_stuffs_EPA %&gt;% \n  group_by(posteam, passer) %&gt;% \n  summarise(mean_epa = mean(epa),\n            n_pass = n())\n\nQ3_n_pass &lt;- quantile(NFL_2g$n_pass, 0.75)\n\nNFL_2g_2 &lt;- NFL_2g %&gt;% \n  filter(n_pass &gt;= Q3_n_pass) %&gt;% \n  arrange(-mean_epa) %&gt;% \n  head(10)"
  },
  {
    "objectID": "posts/Py_Basics/python_basics_HW1.html",
    "href": "posts/Py_Basics/python_basics_HW1.html",
    "title": "Python Basics",
    "section": "",
    "text": "1 Introduction\nThis blog post is meant to give a brief introduction of some of the main topics in basic python programming\n\n\n2 Variables & Assignment\n\nIn python programming, variables are just names for values not actual storage places. \nThese values can be singular such as a numeric value of 10, but there can also be multiple values of the same or varying data types \nan equal sign = is used to assign values to their respective variables (or objects)\n\n\n# Single value\nx = 10\n\n# Multiple values\ny = [1,2,3]\n\n#Multiple types\nz = ['a','b','c',1,2,3]\n\n\n\n3 Data Types in Python\nThere are multiple different data types recognized by the python language.\nThe most basic types are as follows:\n\n# Integer, numeric values with no decimals\nx = 10\n\n# Float, numeric values with decimals\ny = 10.99\n\n# String, character values surrounded by '' or \"\"\nz = 'hello, world' #OR \nz = \"hello, world\"\n\n# Boolean, logical values\nx = True\ny = False\n\n# Nothing, no value\nz = None\n\n# List, data container for any data type (even multiple types at once)\nlist_ = [10, 1.55, 'apple', True, False, None]\n\n# Tuple, non mutable data container\ntup = (2, 3 , 4)\n\n# Set\nset_ = {'a', 'b'}\n\n# Dictionary\ndict_ = {'first': 'a', 'second': 'b'}\n\n\n\n4 Operators\nMathematical operations can also be done in python using preset operators:\n\n# Addition\nx = 5 + 5 # x would have value of 10\ny = x + 5 # y would have value of 15\n\n# Subtraction\nx = 5 - 1 # x would have value of 4\ny = x - 2 # y would have value of 2\n\n# Multiplication and Exponents (* for multiplication, ** for exponents)\nx = 5 * 2**2 # x would have value of 20\ny = x**2 * 5 # y would have value of 2000\n\n# Division and Integer Division (if want int type out of operation, use //)\nx = 4/2 # x would have value of 2.0\ny = x/2 # y would have value of 1.0\n\nx = 8//2 # x would have value of 4 (int type not float)\ny = x//2 # y would have value of 2 (again, int not float)\n\n# Can also perform string concatenation\nstr_one = 'hello,'\nstr_two = ' world'\nstr_ = str_one + str_two # Result of summation is concatenation of str_one and str_two \n                          # = 'hello, world'\n\n\n\n5 Conditionals\nCan perform conditional operations in python\n\nIf statements\nIf else\netc…\n\n\n# Can use if and elif and else to formulate conditional operations\n# For example...\n\nname = 'David'\nscore = 99\n\nif name == 'David' and score &gt;= 90:\n  print('Great job, David!')\nelif name != 'David' and score &gt;= 90:\n  print('Great job, stranger!')\nelif name == 'David' and score &lt; 90:\n  print('You failed, David!')\nelif name != 'David' and score &lt; 90:\n  print('You failed, stranger!')\n\nGreat job, David!\n\n\n-&gt; Here the result is 'Great job, David!' since the name is 'David' and the score is 99\n\n\n6 Casting Variables\nCasting is when we explicitly assign a data type to a variable\n\nstr() will convert to string type\nint() will convert to integer type\nfloat() will convert to float type\n\n\nx = 10 # integer type\ny = '20' # string type\nz = 1.55 # float type\n\nx_ = float(x) # makes 10 into float type = 10.0\ny_ = int(y) # makes '20' into int type = 20\nz_ = int(z) # makes 1.55 into int type = 1 (DOES NOT ROUND)\nx_str = str(x) # makes 10 into str type = '10'\n\n\n\n7 Slicing Methods\n\nPython uses indexing to slice data\nThe index starts at 0, not 1\n\n\n\nAs seen in the image, negative values can also be used\nThe syntax is as follows\n\n[:] - will return the whole value\n[start :] - will return from a starting index value until the end\n[: end] - will return from the start until the end index value\n[start : end] - Will return from start value to end value\n[start : end: step] - will return from start to end by a certain step amount\n\n\n\nstring = 'abcdefghij'\n\nstring[:5] # returns 'abcde' (first 5 characters of string)\nstring[5:] # returns 'fghij' (last 5 characters of string)\nstring[0:5:2] # returns 'ace' (first 5 characters, but in steps of 2)\n\nlist_ = ['apple','banana','pear','strawberry','papaya','pineapple']\n\nlist_[:3] # returns ['apple', 'banana', 'pear'] (first 3 elements of list)\nlist_[3:] # returns ['strawberry', 'papaya', 'pineapple'] (last 3 elements of list)\nlist_[0::2] # returns ['apple', 'pear', 'papaya'] (whole list by steps of 2)\n\n['apple', 'pear', 'papaya']\n\n\n\n\n8 Importing Modules, Packages and Libraries\n\nModule - a bunch of related code saved in a file with extension .py\nPackage - directory of a collection of modules\nLibrary - a collection of packages\nimport is called on a module to be able to use it\n\nThen have to call module along with any functions associated\n\n\n\nimport pandas\n# could then use pandas.read_csv() to read a csv\n\n\nimport as is called on a module to give it a name to make things more efficient\n\n\nimport pandas as pd\n# Then could use pd.read_csv() instead of pandas.read_csv()\n\n\nfrom and import is used to load a specific function from a module\n\n\nfrom pandas import read_csv\n# then can just use read_csv() as if it were a base function\n\n\nIn order to install a module, package or library - have to call pip install in terminal or !pip install if on Google Colab\n\n!pip install is also used for Rstudio\n\n\n\n# !pip install itables"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html",
    "title": "Spotify HW2",
    "section": "",
    "text": "import pandas as pd\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#johann-sebastian-bach",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#johann-sebastian-bach",
    "title": "Spotify HW2",
    "section": "2.1 Johann Sebastian Bach",
    "text": "2.1 Johann Sebastian Bach\n\njsb = (classical.loc[['Johann Sebastian Bach']])\n# I used the .loc[] accessor to access only obs. for Johann Sebastian Bach,\n  # and will do the same in the sections to follow for the respective composers\njsb.drop_duplicates(subset = ['track_name'], keep = 'first')\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nJohann Sebastian Bach\n377\nClassic\n2\nToccata and Fugue in D Minor, BWV 538\n149080\nGreat Composers - J.S. Bach\n\n\nJohann Sebastian Bach\n377\nClassic\n4\nJesu, Joy Of Man's Desiring', BWV 147\n366080\nGreat Composers - J.S. Bach\n\n\nJohann Sebastian Bach\n377\nClassic\n5\nBrandenburg Concerto No.1 In F Major, BWV 1046...\n107075\nGreat Composers - J.S. Bach\n\n\nJohann Sebastian Bach\n377\nClassic\n6\nBrandenburg Concerto No.3 in G, BWV 1048: I. A...\n391800\nJ.S.Bach: The 6 Brandenburg Concertos\n\n\nJohann Sebastian Bach\n377\nClassic\n7\nBrandenburg Concerto No.3 in G, BWV 1048: II. ...\n203666\nJ.S.Bach: The 6 Brandenburg Concertos\n\n\nJohann Sebastian Bach\n377\nClassic\n8\nBrandenburg Concerto No.4 in G, BWV 1049: I. A...\n434333\nJ.S.Bach: The 6 Brandenburg Concertos\n\n\nJohann Sebastian Bach\n377\nClassic\n9\nBrandenburg Concerto No.5 In D Major, BWV 1050...\n377077\nGreat Composers - J.S. Bach\n\n\nJohann Sebastian Bach\n377\nClassic\n11\nConcerto for Violin No. 1 in A minor, BWV 1041...\n246840\nBaroque Violin Concertos\n\n\nJohann Sebastian Bach\n377\nClassic\n12\nViolin Concerto In G Minor, BWV 1056R: II: Largo\n212062\nGreat Composers - J.S. Bach\n\n\nJohann Sebastian Bach\n447\nClassical\n7\nMinuet in G Major (The Lovers Concerto)\n36466\nClassical Relaxation Collection - The Greatest...\n\n\nJohann Sebastian Bach\n447\nClassical\n32\nMusette In D Major 'Anna Magdalena\n40866\nThis Is Classical - The Universe's Best Easy L...\n\n\nJohann Sebastian Bach\n447\nClassical\n49\nGavotte In G Minor (unidentified)\n66106\nClassical Relaxation Collection - The Greatest...\n\n\nJohann Sebastian Bach\n735\nmy songs\n44\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n152280\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n956\nClassical\n33\nJesu, Joy of Man's Desiring - \"Jesu bleibet me...\n149426\nMidwinter's Eve - Music for Christmas\n\n\nJohann Sebastian Bach\n1296\nHALLOWEEN\n79\nToccata and Fugue in D Minor, BWV 565: Toccata\n185200\nBach: The Four Great Toccatas and Fugues\n\n\nJohann Sebastian Bach\n1305\nJ'Adore\n27\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n227026\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n1393\nClass\n3\nConcerto for Harpsichord, Strings, and Continu...\n174586\nBach, J.S.: 3 Piano Concertos\n\n\nJohann Sebastian Bach\n1393\nClass\n7\nBach: Goldberg Variations, BWV 988: Aria\n185546\nBach: Goldberg Variations & Italian Concerto etc\n\n\nJohann Sebastian Bach\n1393\nClass\n9\nBach: Keyboard Concerto No. 5 in F Minor, BWV ...\n208066\nBach: Keyboard Concertos - French Suite No.5\n\n\nJohann Sebastian Bach\n1393\nClass\n28\nHerz und Mund und Tat und Leben, Cantata BWV 1...\n208466\nOrpheus Chamber Orchestra - Baroque Highlights\n\n\nJohann Sebastian Bach\n1393\nClass\n30\nSuite No.3 in D, BWV 1068: 2. Air\n432000\nBach, J.S.: The 4 Orchestral Suites/The Violin...\n\n\nJohann Sebastian Bach\n1393\nClass\n31\nViolin Concerto No.1 in A minor, BWV 1041: 2. ...\n306933\nBach, J.S.: Violin Concertos 1 & 2\n\n\nJohann Sebastian Bach\n1951\nClassical\n38\nArioso (Adagio in G) from Cantata BWV 156 (Arr...\n171000\nCello Song\n\n\nJohann Sebastian Bach\n1951\nClassical\n56\nCello Suite No.1 In G Major, BWV 1007: 1. Prélude\n170000\nBach: 6 Cello Suites BWV 1007, 1008, 1009, 101...\n\n\nJohann Sebastian Bach\n1951\nClassical\n61\nConcerto for Harpsichord, Strings, and Continu...\n176160\nBach, J.S.: Concerti BWV 1052-58\n\n\nJohann Sebastian Bach\n999000\nStudy\n3\nCello Suite No. 1 in G Major, BWV 1007: I. Pre...\n132120\nBach, J.S.: 6 Suites for Solo Cello, Bwv 1007-...\n\n\nJohann Sebastian Bach\n999003\nWriting Playlist\n119\nPartita For Violin Solo No.3 In E Major, BWV 1...\n237000\nJ.S. Bach: Sonatas & Partitas\n\n\nJohann Sebastian Bach\n999003\nWriting Playlist\n120\nViolin Partita No. 3 In E Major, Bwv 1006 - Ii...\n191866\nBach: Violin Sonatas Nos. 1-3 / Partitas Nos. 1-3\n\n\nJohann Sebastian Bach\n999003\nWriting Playlist\n121\nViolin Partita No. 2 In D Minor, Bwv 1004 - I....\n311266\nBach: Violin Sonatas Nos. 1-3 / Partitas Nos. 1-3\n\n\nJohann Sebastian Bach\n999289\nClassical\n7\nWachet aur, ruft uns die Stimme, BWV 140 (arr....\n277000\nBach, J.S.: Orchestral Suites Nos. 1 and 2, Bw...\n\n\nJohann Sebastian Bach\n999609\nInstrumental\n16\nSuite For Cello Solo No.1 In G Major, BWV 1007...\n151906\nBach: Cello Suites Vol. I\n\n\nJohann Sebastian Bach\n999634\nclassic\n22\nOrchestral Suite No. 3 in D Major, BWV 1068: I...\n327080\nGran Turismo 5 - Original Game Soundtrack play...\n\n\nJohann Sebastian Bach\n999634\nclassic\n38\nAve Maria\n163266\nSongs from the Arc of Life\n\n\nJohann Sebastian Bach\n999634\nclassic\n47\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n145426\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n48\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n176413\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n49\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n185413\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n50\nUnaccompanied Cello Suite No. 1 in G Major, BW...\n114306\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n51\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n378173\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n52\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n322306\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n53\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n116026\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n54\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n198426\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n55\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n254880\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n56\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n158440\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n57\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n251506\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n58\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n422040\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n59\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n223453\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n60\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n271026\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n61\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n207826\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999634\nclassic\n62\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n232493\nBach: Cello Suites Nos. 1, 5 & 6\n\n\nJohann Sebastian Bach\n999647\nFall 2014\n13\nGavotte\n274070\nRed Hot + Bach (Deluxe Version)\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\njsb['track_name'].nunique()\n# There are 50 unique tracks by J.S. Bach\n\n50\n\n\n\n(\n    jsb[['track_name','duration_ms']]\n    .sort_values(['duration_ms'], ascending = False)\n    .head()\n) # Here are the 5 longest pieces by J.S. Bach\n\n\n  \n    \n\n\n\n\n\n\ntrack_name\nduration_ms\n\n\nartist_name\n\n\n\n\n\n\nJohann Sebastian Bach\nBrandenburg Concerto No.4 in G, BWV 1049: I. A...\n434333\n\n\nJohann Sebastian Bach\nSuite No.3 in D, BWV 1068: 2. Air\n432000\n\n\nJohann Sebastian Bach\nUnaccompanied Cello Suite No. 6 in D Major, BW...\n422040\n\n\nJohann Sebastian Bach\nBrandenburg Concerto No.3 in G, BWV 1048: I. A...\n391800\n\n\nJohann Sebastian Bach\nUnaccompanied Cello Suite No. 5 in C Minor, BW...\n378173"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#ludwig-van-beethoven",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#ludwig-van-beethoven",
    "title": "Spotify HW2",
    "section": "2.2 Ludwig van Beethoven",
    "text": "2.2 Ludwig van Beethoven\n\nlvb = (classical.loc[['Ludwig van Beethoven']])\nlvb.drop_duplicates(subset = ['track_name'], keep = 'first')\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nLudwig van Beethoven\n447\nClassical\n78\nBagatelle in A Minor, WoO 59, \"Fur Elise\"\n204000\nPontinen, Roland: Music for A Rainy Day\n\n\nLudwig van Beethoven\n798\nmellow\n15\nSonata No. 14 \"Moonlight\" in C-Sharp Minor\", O...\n315426\nBeethoven: Piano Sonatas, Vol.3\n\n\nLudwig van Beethoven\n1053\nPiano\n64\nSonata No. 8 in C Minor, Op. 13: Rondo: Allegro\n276173\nRubinstein Collection, Vol. 10: Beethoven: Pat...\n\n\nLudwig van Beethoven\n1053\nPiano\n65\nSonata No. 8 in C Minor, Op. 13: Grave - Alleg...\n404000\nRubinstein Collection, Vol. 10: Beethoven: Pat...\n\n\nLudwig van Beethoven\n1053\nPiano\n66\nSonata No. 8 in C Minor, Op. 13: Adagio cantabile\n313173\nRubinstein Collection, Vol. 10: Beethoven: Pat...\n\n\nLudwig van Beethoven\n1053\nPiano\n68\nBeethoven : Piano Concerto No.1 in D major Op....\n1153600\nBeethoven : Piano Concertos Nos 1 - 5\n\n\nLudwig van Beethoven\n1393\nClass\n11\nPiano Sonata No.14 in C sharp minor, Op.27 No....\n411200\nBeethoven: Piano Sonatas Nos.8, 14 \"Moonlight\"...\n\n\nLudwig van Beethoven\n1393\nClass\n33\nBagatelle in A Minor, WoO 59 -\"Für Elise\"\n177626\nBeethoven: Piano Concerto No. 3; Andante favor...\n\n\nLudwig van Beethoven\n1393\nClass\n48\nSymphony No.6 in F, Op.68 -\"Pastoral\": 1. Erwa...\n690466\nBeethoven: Symphonies Nos. 5 & 6/The Creatures...\n\n\nLudwig van Beethoven\n1393\nClass\n49\nSymphony No.1 in C, Op.21: 1. Adagio molto - A...\n556000\nBeethoven: The Symphonies\n\n\nLudwig van Beethoven\n1951\nClassical\n58\nPiano Sonata No.14 In C Sharp Minor, Op.27 No....\n367250\nBeethoven: Piano Sonatas \"Moonlight\"; \"Appassi...\n\n\nLudwig van Beethoven\n1951\nClassical\n100\nPiano Sonata No.8 In C Minor Opus 13 \"Pathetiq...\n282093\nBeethoven Piano Sonatas\n\n\nLudwig van Beethoven\n1951\nClassical\n117\nPiano Concerto No.5 in E flat major Op.73 -\"Em...\n464840\nThe Art of Ashkenazy\n\n\nLudwig van Beethoven\n999054\nBed time\n0\nPiano Sonata No. 8 in C Minor, Op. 13, \"Pathet...\n295493\nPiano by Starlight\n\n\nLudwig van Beethoven\n999289\nClassical\n0\nPiano Sonata No. 3 in C Major, Op. 2 No. 3: I....\n668653\nBeethoven: Complete piano sonatas\n\n\nLudwig van Beethoven\n999289\nClassical\n1\nPiano Sonata No. 1 in F Minor, Op. 2 No. 1: I....\n277240\nBeethoven: Complete piano sonatas\n\n\nLudwig van Beethoven\n999289\nClassical\n2\nPiano Sonata No. 23 in F Minor, Op. 57 \"Appass...\n518746\nBeethoven: Complete piano sonatas\n\n\nLudwig van Beethoven\n999289\nClassical\n3\nPiano Sonata No. 10 in G Major, Op. 14 No. 2: ...\n386773\nBeethoven: Complete piano sonatas\n\n\nLudwig van Beethoven\n999289\nClassical\n4\nPiano Sonata No. 8 in C Minor, Op. 13 \"Pathéti...\n564933\nBeethoven: Complete piano sonatas\n\n\nLudwig van Beethoven\n999461\nchillout\n17\nBeethoven: Piano Sonata No. 8 in C Minor, Op. ...\n333933\nBeethoven: Piano Sonatas Nos. 8-11\n\n\nLudwig van Beethoven\n999461\nchillout\n18\nPiano Sonata No. 14 in C-Sharp Minor, Op. 27 N...\n315200\nBeethoven: Piano Sonatas Nos. 8, 14 and 23\n\n\nLudwig van Beethoven\n999609\nInstrumental\n6\nBagatelle in A minor, WoO 59 -\"Für Elise\" - Li...\n196173\nValentina Lisitsa Live At The Royal Albert Hall\n\n\nLudwig van Beethoven\n999634\nclassic\n65\nSymphony No.9 in D minor, Op.125 - \"Choral\": 2...\n856053\nBeethoven: Symphony No.9\n\n\nLudwig van Beethoven\n999818\nSTUDY\n12\nPiano Sonata No. 23 in F Minor, Op. 57 \"Appass...\n430467\nBeethoven, Chopin & Rachmaninoff\n\n\nLudwig van Beethoven\n999958\n11\n0\nPiano Sonata No.14 in C sharp minor, Op.27 No....\n443360\nBeethoven: Piano Sonatas Nos. 14, 15, 17, 21-2...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nlvb['track_name'].nunique()\n # There are 25 pieces by Beethoven in the data set\n\n25\n\n\n\n(\n    lvb[['track_name','duration_ms']]\n    .sort_values(['track_name'], ascending = False)\n    .head()\n) # Here are the 5 longest pieces by Beethoven\n\n\n  \n    \n\n\n\n\n\n\ntrack_name\nduration_ms\n\n\nartist_name\n\n\n\n\n\n\nLudwig van Beethoven\nSymphony No.9 in D minor, Op.125 - \"Choral\": 2...\n856053\n\n\nLudwig van Beethoven\nSymphony No.6 in F, Op.68 -\"Pastoral\": 1. Erwa...\n690466\n\n\nLudwig van Beethoven\nSymphony No.1 in C, Op.21: 1. Adagio molto - A...\n556000\n\n\nLudwig van Beethoven\nSonata No. 8 in C Minor, Op. 13: Rondo: Allegro\n276173\n\n\nLudwig van Beethoven\nSonata No. 8 in C Minor, Op. 13: Grave - Alleg...\n404000"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#wolfgang-amadeus-mozart",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#wolfgang-amadeus-mozart",
    "title": "Spotify HW2",
    "section": "2.3 Wolfgang Amadeus Mozart",
    "text": "2.3 Wolfgang Amadeus Mozart\n\nwam = classical.loc[['Wolfgang Amadeus Mozart']]\nwam.drop_duplicates(subset = ['track_name'], keep = 'first')\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n27\nRequiem in D Minor, K. 626: Lacrimosa\n179400\nGreat Composers - Mozart 2\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n30\nSymphony No. 40 in G Minor, KV. 550: I: Molto ...\n542955\nMozart\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n32\nThe Marriage of Figaro Overture, KV 492\n260693\nMozart: Marriage of Figaro Overture - Magic Fl...\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n34\nRequiem in D minor, K.626: 3. Sequentia: Dies ...\n99133\nMozart: Requiem; Maurerische Trauermusik\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n37\nDie Zauberflöte (The Magic Flute) K. 620: Der ...\n183506\n50 Essential Classical Film Moments\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n38\nClarinet Concerto in A Major, K. 622: II. Adagio\n127000\nGreat Composers - Mozart 2\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n45\nEine Kleine Nachtmusik in G major, K. 525: II....\n123000\nGreat Composers - Mozart 2\n\n\nWolfgang Amadeus Mozart\n667\nclassical\n46\nConcerto for Flute, Harp, and Orchestra in C m...\n279000\nGreat Composers - Mozart 2\n\n\nWolfgang Amadeus Mozart\n956\nClassical\n34\nSleigh Ride, No. 3\n162320\nPops Christmas Party\n\n\nWolfgang Amadeus Mozart\n1199\nmeh\n27\nViolin Sonata No. 32 in B-Flat Major, K. 454: ...\n466000\nMozart: Sonatas for Piano & Violin\n\n\nWolfgang Amadeus Mozart\n1199\nmeh\n28\nViolin Sonata No. 32 in B-Flat Major, K. 454: ...\n415013\nMozart: Sonatas for Piano & Violin\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n1\nClarinet Concerto in A, K.622: 2. Adagio\n474733\nMozart: Clarinet Concerto / Oboe Concerto\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n10\nSymphony No.25 in G minor, K.183: 2. Andante\n357760\nMozart: Symphonies Nos. 25 & 29\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n27\nPiano Concerto No.21 in C Major, K.467: 2. And...\n404426\nMozart: Piano Concertos Nos.21 & 20\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n29\nSymphony No. 41 In C Major, K.551 - \"Jupiter\":...\n554266\nMozart: Symphonies Nos. 41 & 34\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n35\nSymphony No.40 in G minor, K.550 - (2nd versio...\n859426\nMozart: The Symphonies\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n41\nDivertimento in D, K.334: 4. Adagio\n659906\nMozart: Divertimento, K. 344; March in D, K. 445\n\n\nWolfgang Amadeus Mozart\n1393\nClass\n50\nSymphony No.35 in D, K.385 \"Haffner\": 2. Andante\n521706\nMozart: The Symphonies\n\n\nWolfgang Amadeus Mozart\n1444\nStudy\n4\nSymphony No. 40 in G Minor, K. 550: Symphony N...\n908106\nMozart: Symphonies Nos.39 & 40\n\n\nWolfgang Amadeus Mozart\n1642\ndriving music\n191\nSerenade In G Major: Eine Kleine Nachtmusik, K...\n468133\nMozart: Night Music\n\n\nWolfgang Amadeus Mozart\n1753\nclassico\n7\nConcerto No. 24 in C minor for Piano and Orche...\n806213\nMozart: The Piano Concertos\n\n\nWolfgang Amadeus Mozart\n1951\nClassical\n81\nSerenade in G, K.525 \"Eine kleine Nachtmusik\":...\n366000\nMozart: Eine kleine Nachtmusik.\n\n\nWolfgang Amadeus Mozart\n1951\nClassical\n99\nPiano Sonata No.12 In F, K.332: 2. Adagio\n284000\nMozart: Piano Sonatas K.309, K.332 & K.570\n\n\nWolfgang Amadeus Mozart\n999620\nthoughts\n34\nRequiem in D minor, K.626: 3. Sequentia: Rex t...\n121866\nMozart: Requiem; Maurerische Trauermusik\n\n\nWolfgang Amadeus Mozart\n999627\nMy favs\n67\nConcerto pour piano et orchestre No. 21 en Ut ...\n308680\nMozart: Concertos pour piano No. 21 & 24\n\n\nWolfgang Amadeus Mozart\n999634\nclassic\n23\nRequiem: Lacrimosa\n169666\nMozart: Requiem Realisations\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nwam['track_name'].nunique()\n# There are 26 pieces by Mozart\n\n26\n\n\n\n(\n    wam[['track_name','duration_ms']]\n    .sort_values(['duration_ms'], ascending = False)\n    .head()\n)\n# Here are the 5 longest pieces by Mozart\n\n\n  \n    \n\n\n\n\n\n\ntrack_name\nduration_ms\n\n\nartist_name\n\n\n\n\n\n\nWolfgang Amadeus Mozart\nSymphony No. 40 in G Minor, K. 550: Symphony N...\n908106\n\n\nWolfgang Amadeus Mozart\nSymphony No.40 in G minor, K.550 - (2nd versio...\n859426\n\n\nWolfgang Amadeus Mozart\nConcerto No. 24 in C minor for Piano and Orche...\n806213\n\n\nWolfgang Amadeus Mozart\nDivertimento in D, K.334: 4. Adagio\n659906\n\n\nWolfgang Amadeus Mozart\nSymphony No. 41 In C Major, K.551 - \"Jupiter\":...\n554266"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#sergei-rachmaninoff",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2_danl210.html#sergei-rachmaninoff",
    "title": "Spotify HW2",
    "section": "2.4 Sergei Rachmaninoff",
    "text": "2.4 Sergei Rachmaninoff\n\nsr = classical.loc[['Sergei Rachmaninoff']]\nsr.drop_duplicates(subset = ['track_name'], keep = 'first')\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nSergei Rachmaninoff\n108\nThe Piano Guys\n65\nRock Meets Rachmaninoff\n204813\nRock Meets Rachmaninoff\n\n\nSergei Rachmaninoff\n447\nClassical\n43\n10 Preludes, Op. 23: No. 5 in G Minor: Alla ma...\n237986\nRachmaninov: Complete Preludes\n\n\nSergei Rachmaninoff\n534\nNew Music\n12\n10 Preludes, Op. 23: No. 4 in D Major: Andante...\n304746\nRachmaninov: Preludes for Piano (Complete)\n\n\nSergei Rachmaninoff\n1053\nPiano\n75\nPiano Concerto No.2 in C minor, Op.18: 1. Mode...\n616647\nRachmaninov: Piano Concerto No.2\n\n\nSergei Rachmaninoff\n1053\nPiano\n76\nPiano Concerto No.2 in C minor, Op.18: 2. Adag...\n644272\nRachmaninov: Piano Concerto No.2\n\n\nSergei Rachmaninoff\n1053\nPiano\n77\nPiano Concerto No.2 in C minor, Op.18: 3. Alle...\n702018\nRachmaninov: Piano Concerto No.2\n\n\nSergei Rachmaninoff\n1053\nPiano\n78\nRhapsody On A Theme Of Paganini, Op.43: Variat...\n170600\nRachmaninov: The Piano Concertos; Paganini Rha...\n\n\nSergei Rachmaninoff\n1053\nPiano\n108\nPiano Concerto No. 2 in C Minor, Op. 18: I. Mo...\n670360\nTchaikovsky / Rachmaninov: Piano Concertos\n\n\nSergei Rachmaninoff\n1053\nPiano\n109\nPiano Concerto No. 2 In C Minor, Op. 18: Piano...\n682000\n101 Great Orchestral Classics, Vol. 5\n\n\nSergei Rachmaninoff\n1053\nPiano\n110\nPiano Concerto No. 2 in C Minor, Op. 18: III. ...\n691040\nTchaikovsky / Rachmaninov: Piano Concertos\n\n\nSergei Rachmaninoff\n1393\nClass\n34\nSymphony No.2 in E minor, Op.27: 1. Largo - Al...\n1182500\nRachmaninov: Symphony No.2/The Rock\n\n\nSergei Rachmaninoff\n1819\nMom\n3\nRhapsody on a Theme of Paganini: I. Introducti...\n94040\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n4\nRhapsody on a Theme of Paganini: II. Variation...\n290413\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n5\nRhapsody on a Theme of Paganini: III. Variatio...\n184400\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n6\nRhapsody on a Theme of Paganini: IV. Variation...\n300733\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n7\nRhapsody on a Theme of Paganini: V. Variation ...\n163413\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n8\nRhapsody on a Theme of Paganini: VI. Variation...\n313560\nRachmaninov: Piano Concerto No. 3 & Rhapsody o...\n\n\nSergei Rachmaninoff\n1819\nMom\n10\nAll-Night Vigil, Op. 37: IV. Gladsome Light\n154200\nRachmaninov: All-Night Vigil (Vespers)\n\n\nSergei Rachmaninoff\n1951\nClassical\n55\nRhapsody On A Theme By Paganini, Op.43: Variat...\n168573\nRachmaninov: Piano Concerto No.2; Rhapsody on ...\n\n\nSergei Rachmaninoff\n999609\nInstrumental\n0\nRachmaninoff: Piano Concerto No. 2 in C Minor,...\n272626\nThe Most Relaxing Classical Album in The World...\n\n\nSergei Rachmaninoff\n999609\nInstrumental\n9\nVocalise, Op.34, No.14\n202500\nYellow Lounge\n\n\nSergei Rachmaninoff\n999818\nSTUDY\n14\nRhapsody on a Theme of Paganini, Op. 43\n1434040\nRachmaninov: Piano Concerto No. 2 / Rhapsody O...\n\n\nSergei Rachmaninoff\n999818\nSTUDY\n19\nFive Preludes from Op.23 (1904) and Op.32 (191...\n147480\nVoyages\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsr['track_name'].nunique()\n# There are 23 unique pieces by Rachmaninoff in the data set\n\n23\n\n\n\n(\n    sr[['track_name','duration_ms']]\n    .sort_values(['duration_ms'], ascending = False)\n    .head()\n)\n# These are the 5 longest pieces by Rachmaninoff in the data set\n\n\n  \n    \n\n\n\n\n\n\ntrack_name\nduration_ms\n\n\nartist_name\n\n\n\n\n\n\nSergei Rachmaninoff\nRhapsody on a Theme of Paganini, Op. 43\n1434040\n\n\nSergei Rachmaninoff\nSymphony No.2 in E minor, Op.27: 1. Largo - Al...\n1182500\n\n\nSergei Rachmaninoff\nPiano Concerto No.2 in C minor, Op.18: 3. Alle...\n702018\n\n\nSergei Rachmaninoff\nPiano Concerto No.2 in C minor, Op.18: 3. Alle...\n702018\n\n\nSergei Rachmaninoff\nPiano Concerto No. 2 in C Minor, Op. 18: III. ...\n691040"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html",
    "href": "posts2/DANL200 Project/project.html",
    "title": "DANL 200 Project",
    "section": "",
    "text": "These data sets were obtained from the EPA website here\nThese data sets (four in total) measure daily levels of four different gases in different locations\n\nThe gases measured are Ozone, SO2 (sulfur dioxide), CO (carbon monoxide), and NO2 (nitrogen dioxide)\nThese gases play a large role in the measurement of the Air Quality Index (AQI)\nThey are considered to be atmospheric pollutants\nMore information on Criteria Air Pollutants can be found here\n\nThis project will take a look at and analyze the measurements of these four atmospheric gases per state\n\nThese measurements were taken from 01/01/2023 to 09/30/2023\n\n\n\n\n\nozone &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\ozone_measures.csv\")\n\nso2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\so2_measures.csv\")\n\nco &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\co_measures.csv\")\n\nno2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\no2_measures.csv\")\n\n\nrmarkdown::paged_table(ozone)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(co)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2)\n\n\n\n  \n\n\n\n\n\n\n\nThe data sets include a lot of information that is not needed for my analysis\n\nHere I will alter the data sets to include only the information of importance\nThe data sets will then be row bound using rbind() to create a single data set from the four\n\nFor the purpose of continuity, the top 10 most frequent states in ozone data set will also be selected in the other three data sets\n\n\n\n\nst_top10_oz &lt;- ozone %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(10) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nozone_alt &lt;- ozone %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_oz$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\nView(ozone_alt)\n\n\nThe new data set ozone_alt includes data for only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_oz)\n\n\n\n  \n\n\nrmarkdown::paged_table(ozone_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_so2 &lt;- so2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;%\n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nso2_alt &lt;- so2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_so2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nThe new data set so2_alt includes data for again, only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_co &lt;- co %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nco_alt &lt;- co %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_co$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_co)\n\n\n\n  \n\n\nrmarkdown::paged_table(co_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_no2 &lt;- no2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nno2_alt &lt;- no2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_no2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_no2)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2_alt)\n\n\n\n  \n\n\n\n\n\n\n\n\nSince the variables are the same, all four data sets can be row bound together using rbind()\n\n\nair_qual &lt;- rbind(ozone_alt, so2_alt, co_alt, no2_alt)\nrmarkdown::paged_table(air_qual)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#loading-the-data-sets",
    "href": "posts2/DANL200 Project/project.html#loading-the-data-sets",
    "title": "DANL 200 Project",
    "section": "",
    "text": "ozone &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\ozone_measures.csv\")\n\nso2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\so2_measures.csv\")\n\nco &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\co_measures.csv\")\n\nno2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\no2_measures.csv\")\n\n\nrmarkdown::paged_table(ozone)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(co)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#altering-the-four-data-sets",
    "href": "posts2/DANL200 Project/project.html#altering-the-four-data-sets",
    "title": "DANL 200 Project",
    "section": "",
    "text": "The data sets include a lot of information that is not needed for my analysis\n\nHere I will alter the data sets to include only the information of importance\nThe data sets will then be row bound using rbind() to create a single data set from the four\n\nFor the purpose of continuity, the top 10 most frequent states in ozone data set will also be selected in the other three data sets\n\n\n\n\nst_top10_oz &lt;- ozone %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(10) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nozone_alt &lt;- ozone %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_oz$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\nView(ozone_alt)\n\n\nThe new data set ozone_alt includes data for only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_oz)\n\n\n\n  \n\n\nrmarkdown::paged_table(ozone_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_so2 &lt;- so2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;%\n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nso2_alt &lt;- so2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_so2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nThe new data set so2_alt includes data for again, only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_co &lt;- co %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nco_alt &lt;- co %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_co$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_co)\n\n\n\n  \n\n\nrmarkdown::paged_table(co_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_no2 &lt;- no2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nno2_alt &lt;- no2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_no2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_no2)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2_alt)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#binding-all-four-data-sets-ozone_alt-so2_alt-co_alt-and-no2_alt-to-create-the-air_qual-data-set",
    "href": "posts2/DANL200 Project/project.html#binding-all-four-data-sets-ozone_alt-so2_alt-co_alt-and-no2_alt-to-create-the-air_qual-data-set",
    "title": "DANL 200 Project",
    "section": "",
    "text": "Since the variables are the same, all four data sets can be row bound together using rbind()\n\n\nair_qual &lt;- rbind(ozone_alt, so2_alt, co_alt, no2_alt)\nrmarkdown::paged_table(air_qual)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#description-of-the-variables",
    "href": "posts2/DANL200 Project/project.html#description-of-the-variables",
    "title": "DANL 200 Project",
    "section": "2.1 Description of the Variables",
    "text": "2.1 Description of the Variables\n\nState.Name: A character variable whose values are one of the 10 states most frequent in the ozone data set\n\nArizona\nCalifornia\nColorado\nFlorida\nIllinois\nIndiana\nOhio\nPennsylvania\nTexas\nUtah\n\nDate.Local: The date the measurement of the atmospheric gas was taken\nParameter.Name: The atmospheric gas being measured\nUnits.of.Measure: Units in which the atmospheric gas was measured\n\nParts Per Million : Ozone, Carbon Monoxide\nParts Per Billion : Sulfur Dioxide, Nitrogen Dioxide\n\nMeasure: The mean measurement of the gas in the units provided in Units.of.Measure"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#distribution-of-each-value-of-state.name-per-value-of-parameter.name",
    "href": "posts2/DANL200 Project/project.html#distribution-of-each-value-of-state.name-per-value-of-parameter.name",
    "title": "DANL 200 Project",
    "section": "2.2 Distribution of each value of State.Name per value of Parameter.Name",
    "text": "2.2 Distribution of each value of State.Name per value of Parameter.Name\n\nThe distribution of each state in the ten provided, per each atmospheric gas being measured\n\n\nair_qual_state_dist &lt;- air_qual %&gt;% \n  group_by(State.Name, Parameter.Name) %&gt;% \n  summarise(n = n())\nView(air_qual_state_dist)\n\nggplot(data = air_qual_state_dist)+\n  geom_bar(aes(y = State.Name, x = n, fill = State.Name), stat = 'identity')+\n  facet_wrap(.~Parameter.Name)+\n  theme_gray()+\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n  labs(title = 'State Distribution per Gas',\n       x = 'Count',\n       y = 'State')\n\n\n\n\nAnalyses\n\nDepicted in the above figure is a faceted bar chart depicting the count of each state in which each atmospheric gas was measured\n\nIt seems as though carbon monoxide (CO), nitrogen dioxide (NO2), and ozone were measured most often in California\nSulfur dioxide (SO2) was measured most frequently in Ohio and secondly in Texas"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-ozone-measurement-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-ozone-measurement-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.3 The average measurement of the OZONE measurement per each State.Name value",
    "text": "2.3 The average measurement of the OZONE measurement per each State.Name value\n\nThe average measurement of ozone in parts per million (ppm) per each state in the top ten most frequent\n\n\nozone_alt_per_state &lt;- ozone_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppm = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppm))\n\nggplot(data = ozone_alt_per_state)+\n  geom_bar(aes(y = state_fct, x = mean_ppm, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Ozone Levels in ppm per State',\n       x = ' Mean ppm Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nIn the bar chart shown above depicts a decent variation in Ozone levels between the 10 states\n\nTexas, Florida, and California seem to have the lowest levels of Ozone\n\nColorado, Utah, and Arizona seem to have the highest levels of Ozone"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-sulfur-dioxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-sulfur-dioxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.4 The average measurement of the SULFUR DIOXIDE measurment per each State.Name value",
    "text": "2.4 The average measurement of the SULFUR DIOXIDE measurment per each State.Name value\n\nThe average measurment of sulfur dioxide (SO2) in parts per billion (ppb) per each state in the top ten most frequent\n\n\nso2_alt_per_state &lt;- so2_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppb = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppb))\n\nggplot(data = so2_alt_per_state)+\n  geom_bar(aes(x = mean_ppb, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Sulfur Dioxide Levels in ppb per State',\n       x = 'Mean ppb Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nThere seems to be even greater variation between each of the state’s Sulfur Dioxide (SO2) levels\n\nCalifornia, Arizona, and Ohio seem to have the lowest levels of SO2\n\nColorado, Texas, and Utah seem to have the highest levels of SO2"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-carbon-monoxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-carbon-monoxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.5 The average measurement of the CARBON MONOXIDE measurment per each State.Name value",
    "text": "2.5 The average measurement of the CARBON MONOXIDE measurment per each State.Name value\n\nThe average measurment of carbon monoxide (CO) in parts per million (ppm) per each state in the top ten most frequent\n\n\nco_alt_per_state &lt;- co_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppm = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppm))\n\nggplot(data = co_alt_per_state)+\n  geom_bar(aes(x = mean_ppm, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Carbon Monoxide Levels in ppm per State',\n       x = 'Mean ppm Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nIn the bar chart, there seems to be less variation in Carbon Monoxide (CO) levels as compared to Sulfur Dioxide (SO2)\n\nUtah, Ohio, and Illinois seem to have the lowest levels of CO\n\nIndiana, Florida, and Pennsylvania seem to have the highest levels of CO"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-nitrogen-dioxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-nitrogen-dioxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.6 The average measurement of the NITROGEN DIOXIDE measurment per each State.Name value",
    "text": "2.6 The average measurement of the NITROGEN DIOXIDE measurment per each State.Name value\n\nThe average measurement of nitrogen dioxide (NO2) in parts per billion (ppb) per each state in the top ten most frequent\n\n\nno2_alt_per_state &lt;- no2_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppb = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppb))\n\nggplot(data = no2_alt_per_state)+\n  geom_bar(aes(x = mean_ppb, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Nitrogen Dioxide Levels in ppb per State',\n       x = 'Mean ppb Value',\n       y = ' State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nThe variation of Nitrogen Dioxide (NO2) seems to be similar to that of Sulfur Dioxide (SO2)\n\nIndiana, Texas, and Utah seem to have the lowest levels of NO2\n\nArizona, Illinois, and Colorado seem to have the highest levels of NO2"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#what-do-these-levels-even-mean",
    "href": "posts2/DANL200 Project/project.html#what-do-these-levels-even-mean",
    "title": "DANL 200 Project",
    "section": "3.1 What do these levels even mean?",
    "text": "3.1 What do these levels even mean?\n\nThese four gases (Ozone, SO2, CO, and NO2) are considered to be Criteria Air Pollutants by the EPA\n\nBeing Criteria Air Pollutants (along with Lead (Pb), and Particulate Matter (PM)), they pose adverse health and environmental effects.\n\n\n\n\n3.1.1 Ozone\n\nGround-level ozone (as measured in the data), comes from reaction between pollutants\n\nThese pollutants are emitted by industrial facilities, electric utilities, and motor vehicles\n\nThis Ground-level ozone (in contrast with ozone that protects from UV rays), can pose health risks\n\nThese health risks include…\n\nInflammation of lining of the lungs\nReduced lung function\nCough, wheezing, chest pain, shortness of breath\nIncreased susceptibility to respiratory infection, and\nPremature mortality, to name a few.\n\n\n\nAnalyses Pertaining to the Data:\n\nFor the states such as Colorado, Utah, and Arizona some of these health effects could potentially be more frequent\n\nHowever, in states such as Texas, Florida, and California these health effects will most likely not be as prevalent\n\n\n\n\n3.1.2 Sulfur Dioxide (SO2)\n\nThis air pollutant comes primarily from fossil fuel combustion by industrial and electrical facilities\nShort-term exposure by asthmatic individuals may result in…\n\nWheezing\nChest Tightness\nShortness of Breath\nEtc…\n\nMostly affects the respiratory system\n\nAnalyses Pertaining to the Data:\n\nColorado would most likely experience the highest frequency of these health effects as the average measure of SO2 over the time frame is almost double that of Texas (the state with the second highest measurement)\n\nCalifornia is the state with the lowest average level of SO2, and would most likely see the least health effects\n\n\n\n\n3.1.3 Carbon Monoxide (CO)\n\nThe primary source of Carbon Monoxide is found to be fuel propelled means of transportation\nPrimary health risks as a result of exposure are…\n\nDecreased capacity of blood to carry oxygen\n\nWhich can cause myocardial ischemia (reduced oxygen to heart)\nand angina (chest pain)\n\n\n\nAnalyses Pertaining to the Data:\n\nStates such as Indiana, Florida, and Pennsylvania which had the highest average levels of CO, would most likely see the highest frequency of health effects\n\nUtah, which had the lowest average level of CO (almost half the average level of Indiana), would most likely experience the lowest frequency of health effects\n\n\n\n\n3.1.4 Nitrogen Dioxide (NO2)\n\nPrimarily emitted from cars, trucks, buses, power plants, and other engines/equipment\n\nNitrogen Oxide (NO) is emitted first and then rapidly oxidized into NO2 in the atmosphere\n\nHealth risks include mostly respiratory related symptoms and hospital visits\n\nAnalyses Pertaining to the Data:\n\nStates such as Arizona and Illinois had the highest average level of NO2 recorded, so they would most likely experience the highest frequency of these health effects\n\nStates such as Utah and Texas had an average recorded level of NO2 about half that of Arizona and Illinois, and would most likely experience the lowest frequency of health effects"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html",
    "href": "posts2/DANL210 Project/project_210.html",
    "title": "DANL 210 Project",
    "section": "",
    "text": "This project is an exploration into the relationships between esg data for a slew of firms, and their respective financial data. This exploration will allow for greater insights into the connections that may be present between these two areas.\n\n\nBelow the data frames including esg data, historical financial data, income statement data, and balance sheet data will be loaded for use.\n\nesg = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\esg_complete.csv\")\nhist = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-history.csv\")\ninc = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-income-stmt.csv\")\nbal = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-balance-sheets.csv\")"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#below-the-data-frames-including-esg-data-historical-financial-data-income-statement-data-and-balance-sheet-data-will-be-loaded-for-use.",
    "href": "posts2/DANL210 Project/project_210.html#below-the-data-frames-including-esg-data-historical-financial-data-income-statement-data-and-balance-sheet-data-will-be-loaded-for-use.",
    "title": "DANL 210 Project",
    "section": "",
    "text": "esg = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\danl_210_yfin_data_FINAL.csv\")\nhist = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-history.csv\")\ninc = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-income-stmt.csv\")\nbal = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-balance-sheets.csv\")\n\nesg['tot_ESG'] = esg['ERS'] + esg['SRS'] + esg['GRS']"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#esg-stats",
    "href": "posts2/DANL210 Project/project_210.html#esg-stats",
    "title": "DANL 210 Project",
    "section": "2.1 ESG Stats",
    "text": "2.1 ESG Stats\n\n2.1.1 General\n\nesg.describe()\n\n\n\n\n\n\n\n\nMarket Cap\nERS\nSRS\nGRS\nCL\ntot_esg\n\n\n\n\ncount\n6.360000e+02\n612.000000\n612.000000\n612.000000\n580.000000\n612.000000\n\n\nmean\n7.161278e+10\n5.819281\n9.016176\n6.826961\n1.956897\n21.662418\n\n\nstd\n2.268143e+11\n5.318299\n3.566610\n2.395565\n0.797148\n6.985062\n\n\nmin\n1.986640e+08\n0.000000\n0.800000\n2.400000\n0.000000\n6.700000\n\n\n25%\n1.099163e+10\n1.775000\n6.700000\n5.200000\n1.000000\n16.400000\n\n\n50%\n2.397381e+10\n4.000000\n8.900000\n6.300000\n2.000000\n21.250000\n\n\n75%\n5.889363e+10\n9.000000\n11.125000\n7.900000\n2.000000\n26.025000\n\n\nmax\n3.019135e+12\n27.300000\n22.500000\n19.400000\n5.000000\n44.900000\n\n\n\n\n\n\n\n\n\n2.1.2 Distribution Plots of Differing ESG Scores and Market Capitalization\n\nsns.histplot(data =esg, x = 'ERS')\nplt.title('Distribution of Envt Risk Score')\n\nText(0.5, 1.0, 'Distribution of Envt Risk Score')\n\n\n\n\n\nComments: One can see here that the occurrence of high ERS is very low in the firms evaluated, low ERS values are much more common\n\nsns.histplot(data =esg, x = 'SRS')\nplt.title('Distribution of Social Risk Score')\n\nText(0.5, 1.0, 'Distribution of Social Risk Score')\n\n\n\n\n\nComments: One can see here that the values of SRS are centered around 10, which makes the distribution of SRS values look similar to a normal curve\n\nsns.histplot(data =esg, x = 'GRS')\nplt.title('Distribution of Gov Risk Score')\n\nText(0.5, 1.0, 'Distribution of Gov Risk Score')\n\n\n\n\n\nComments: One can see here that the values of GRS are centered around 6, which makes its distribution also look similar to a normal curve\n\nsns.histplot(data =esg, x = 'CL')\nplt.title('Distribution of Controversy Level')\n\nText(0.5, 1.0, 'Distribution of Controversy Level')\n\n\n\n\n\nComments: One can see here that CL values of 2 are most common, with values of 1 and 5 being almost nonexistant\n\nsns.histplot(data =esg, x = 'Market Cap', bins = 50)\nplt.title('Distribution of Market Capitalization')\n\nText(0.5, 1.0, 'Distribution of Market Capitalization')\n\n\n\n\n\nComments: One can see here that market capitalization values are more frequently on the lower end (when compared with all of the firms in the data frame)"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#loading-the-data-frames",
    "href": "posts2/DANL210 Project/project_210.html#loading-the-data-frames",
    "title": "DANL 210 Project",
    "section": "",
    "text": "Below the data frames including esg data, historical financial data, income statement data, and balance sheet data will be loaded for use.\n\nesg = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\esg_complete.csv\")\nhist = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-history.csv\")\ninc = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-income-stmt.csv\")\nbal = pd.read_csv(\"C:/Users\\Dan\\OneDrive\\Documents\\dan-noone.github.io\\Data\\yfinance-balance-sheets.csv\")"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#income-stats",
    "href": "posts2/DANL210 Project/project_210.html#income-stats",
    "title": "DANL 210 Project",
    "section": "2.2 Income Stats",
    "text": "2.2 Income Stats\nHere I am selecting certain income data points since there are too many in the original data frame\n\ninc = inc[['EBIT', 'Net Income', 'Total Revenue', 'Total Expenses']]\n\n\n2.2.1 General\n\ninc.describe()\n\n\n\n\n\n\n\n\nEBIT\nNet Income\nTotal Revenue\nTotal Expenses\n\n\n\n\ncount\n2.530000e+03\n2.736000e+03\n2.736000e+03\n2.539000e+03\n\n\nmean\n9.715337e+08\n7.368192e+08\n6.590925e+09\n5.686589e+09\n\n\nstd\n2.524586e+09\n2.196511e+09\n1.331325e+10\n1.214236e+10\n\n\nmin\n-1.109100e+10\n-1.191100e+10\n-8.031840e+08\n-1.029000e+09\n\n\n25%\n1.754510e+08\n1.086375e+08\n1.277048e+09\n1.049772e+09\n\n\n50%\n3.884395e+08\n2.658860e+08\n2.679200e+09\n2.141600e+09\n\n\n75%\n9.485000e+08\n7.060000e+08\n5.580721e+09\n4.450818e+09\n\n\nmax\n4.037300e+10\n3.391600e+10\n1.699610e+11\n1.567520e+11\n\n\n\n\n\n\n\n\n\n2.2.2 Distribution Plots\n\nsns.histplot(data =inc, x = 'EBIT', bins = 50)\nplt.title('Distribution of EBIT')\n\nText(0.5, 1.0, 'Distribution of EBIT')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Net Income', bins = 50)\nplt.title('Distribution of Net Income')\n\nText(0.5, 1.0, 'Distribution of Net Income')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Total Revenue', bins = 50)\nplt.title('Distribution of Total Revenue')\n\nText(0.5, 1.0, 'Distribution of Total Revenue')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Total Expenses', bins = 50)\nplt.title('Distribution of Total Expenses')\n\nText(0.5, 1.0, 'Distribution of Total Expenses')\n\n\n\n\n\nComments:"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#income-statement-stats",
    "href": "posts2/DANL210 Project/project_210.html#income-statement-stats",
    "title": "DANL 210 Project",
    "section": "2.2 Income Statement Stats",
    "text": "2.2 Income Statement Stats\nHere I am selecting certain income data points since there are too many in the original data frame\n\ninc = inc[['ticker','EBIT', 'Net Income', 'Total Revenue', 'Total Expenses']]\n\n\n2.2.1 General\n\ninc.describe()\n\n\n\n\n\n\n\n\nEBIT\nNet Income\nTotal Revenue\nTotal Expenses\n\n\n\n\ncount\n2.530000e+03\n2.736000e+03\n2.736000e+03\n2.539000e+03\n\n\nmean\n9.715337e+08\n7.368192e+08\n6.590925e+09\n5.686589e+09\n\n\nstd\n2.524586e+09\n2.196511e+09\n1.331325e+10\n1.214236e+10\n\n\nmin\n-1.109100e+10\n-1.191100e+10\n-8.031840e+08\n-1.029000e+09\n\n\n25%\n1.754510e+08\n1.086375e+08\n1.277048e+09\n1.049772e+09\n\n\n50%\n3.884395e+08\n2.658860e+08\n2.679200e+09\n2.141600e+09\n\n\n75%\n9.485000e+08\n7.060000e+08\n5.580721e+09\n4.450818e+09\n\n\nmax\n4.037300e+10\n3.391600e+10\n1.699610e+11\n1.567520e+11\n\n\n\n\n\n\n\n\n\n2.2.2 Distribution Plots\n\nsns.histplot(data =inc, x = 'EBIT', bins = 50)\nplt.title('Distribution of EBIT')\n\nText(0.5, 1.0, 'Distribution of EBIT')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Net Income', bins = 50)\nplt.title('Distribution of Net Income')\n\nText(0.5, 1.0, 'Distribution of Net Income')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Total Revenue', bins = 50)\nplt.title('Distribution of Total Revenue')\n\nText(0.5, 1.0, 'Distribution of Total Revenue')\n\n\n\n\n\n\nsns.histplot(data =inc, x = 'Total Expenses', bins = 50)\nplt.title('Distribution of Total Expenses')\n\nText(0.5, 1.0, 'Distribution of Total Expenses')"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#balance-sheet-stats",
    "href": "posts2/DANL210 Project/project_210.html#balance-sheet-stats",
    "title": "DANL 210 Project",
    "section": "2.3 Balance Sheet Stats",
    "text": "2.3 Balance Sheet Stats\nHere I will also select a few data points to focus on\n\nbal = bal[['ticker','Total Debt', 'Common Stock', 'Total Assets']]\n\n\n2.3.1 General\n\nbal.describe()\n\n\n\n\n\n\n\n\nTotal Debt\nCommon Stock\nTotal Assets\n\n\n\n\ncount\n2.705000e+03\n2.714000e+03\n2.738000e+03\n\n\nmean\n1.890101e+10\n2.647317e+09\n9.027200e+10\n\n\nstd\n4.175898e+10\n9.641319e+09\n2.906681e+11\n\n\nmin\n1.404000e+06\n0.000000e+00\n1.891360e+08\n\n\n25%\n3.227232e+09\n1.529250e+06\n1.055588e+10\n\n\n50%\n7.030000e+09\n8.100000e+06\n2.435150e+10\n\n\n75%\n1.723300e+10\n3.710000e+08\n6.271950e+10\n\n\nmax\n4.421400e+11\n9.919300e+10\n4.090727e+12\n\n\n\n\n\n\n\n\nsns.histplot(data =bal, x = 'Total Debt', bins = 50)\nplt.title('Distribution of Total Debt')\n\nText(0.5, 1.0, 'Distribution of Total Debt')\n\n\n\n\n\n\nsns.histplot(data =bal, x = 'Common Stock', bins = 50)\nplt.title('Distribution of Common Stock')\n\nText(0.5, 1.0, 'Distribution of Common Stock')\n\n\n\n\n\n\nsns.histplot(data =bal, x = 'Total Assets', bins = 50)\nplt.title('Distribution of Total Assets')\n\nText(0.5, 1.0, 'Distribution of Total Assets')"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#historical-stats",
    "href": "posts2/DANL210 Project/project_210.html#historical-stats",
    "title": "DANL 210 Project",
    "section": "2.4 Historical Stats",
    "text": "2.4 Historical Stats\n\nhist = hist[['Close', 'Volume']]\n\n\n2.4.1 General\n\nhist.describe()\n\n\n\n\n\n\n\n\nClose\nVolume\n\n\n\n\ncount\n197796.000000\n1.977960e+05\n\n\nmean\n149.424528\n4.134482e+06\n\n\nstd\n330.094224\n9.289442e+06\n\n\nmin\n0.980000\n0.000000e+00\n\n\n25%\n40.010676\n8.323000e+05\n\n\n50%\n82.121376\n1.707600e+06\n\n\n75%\n155.078556\n3.854425e+06\n\n\nmax\n8099.959961\n3.160112e+08\n\n\n\n\n\n\n\n\n\n2.4.2 Distribution Plots\n\nsns.histplot(data =hist, x = 'Close', bins = 50)\nplt.title('Distribution of Close')\n\nText(0.5, 1.0, 'Distribution of Close')\n\n\n\n\n\n\nsns.histplot(data =hist, x = 'Volume', bins = 50)\nplt.title('Distribution of Volume')\n\nText(0.5, 1.0, 'Distribution of Volume')"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#total-esg-score-by-sector",
    "href": "posts2/DANL210 Project/project_210.html#total-esg-score-by-sector",
    "title": "DANL 210 Project",
    "section": "3.1 Total ESG Score by Sector",
    "text": "3.1 Total ESG Score by Sector\nFirst, the esg data frame has to be filtered for the chosen sectors to analyze\n\nsectors = ['Consumer Discretionary', 'Industrials','Financial Services']\nesg_ = esg[esg['Sector'].isin(sectors)]\n\n\nsns.FacetGrid(data = esg_, row = 'Sector').map(sns.histplot, 'tot_esg')\n\n\n\n\n\n3.1.1 Comments\nFrom these plots of total ESG score by sector,  one can see that industrials has a lower average total ESG score than both consumer discretionary and financial services, as well as a more varied total ESG score."
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#relationship-between-total-expenses-and-total-esg-score",
    "href": "posts2/DANL210 Project/project_210.html#relationship-between-total-expenses-and-total-esg-score",
    "title": "DANL 210 Project",
    "section": "3.2 Relationship between Total Expenses and Total ESG Score",
    "text": "3.2 Relationship between Total Expenses and Total ESG Score\nFirst, the data frames containint income statement data and esg data have to be joined together\n\ninc_esg = esg_.merge(inc, on = 'ticker', how = 'left')\n\n\nsns.lmplot(data = inc_esg, x = 'tot_esg', y = 'Total Expenses')\n\n\n\n\n\n3.2.1 Comments\nThere seems to be a slight positive relationship present between Total ESG Score and Total Expenses. Meaning that as Total Expenses increase, so does Total ESG Score.\n\n\n3.2.2 Relationship by Sector\n\nsns.lmplot(data = inc_esg, x = 'tot_esg', y = 'Total Expenses', row = 'Sector')\n\n\n\n\n\n3.2.2.1 Comments\nThere seems to be very insignificant relationships in both industrials and financial services sectors. Howver, there is a relatively significant positive relationship in the consumer discretionary sector."
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#relationship-by-sector",
    "href": "posts2/DANL210 Project/project_210.html#relationship-by-sector",
    "title": "DANL 210 Project",
    "section": "3.3 Relationship by Sector",
    "text": "3.3 Relationship by Sector\n\nsns.lmplot(data = inc_esg, x = 'tot_esg', y = 'Total Expenses', row = 'Sector')"
  },
  {
    "objectID": "posts2/DANL210 Project/project_210.html#relationship-between-total-assets-and-total-esg-score",
    "href": "posts2/DANL210 Project/project_210.html#relationship-between-total-assets-and-total-esg-score",
    "title": "DANL 210 Project",
    "section": "3.3 Relationship between Total Assets and Total ESG Score",
    "text": "3.3 Relationship between Total Assets and Total ESG Score\nFirst, the data frames have to again be joined. This time, the data frame containing balance sheet data and esg data are joined.\n\nbal_esg = esg_.merge(bal, on = 'ticker', how = 'left')\n\n\nsns.lmplot(data = bal_esg, x = 'tot_esg', y = 'Total Assets')\n\n\n\n\n\n3.3.1 Comments\nThere seems to also be a slight positive relationship between Total Assets and Total ESG Score. One that is more significant that the relationship between Total Expenses and Total ESG Score.\n\n\n3.3.2 Relationship by Sector\n\nsns.lmplot(data = bal_esg, x = 'tot_esg', y = 'Total Assets', row = 'Sector')\n\n\n\n\n\n3.3.2.1 Comments\nThere seems to be no significant relationship in the Industrials or Consumer Discretionary sectors. However there is a significant positive relationship in the Financial Services Sector"
  }
]