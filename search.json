[
  {
    "objectID": "quarto-template.html",
    "href": "quarto-template.html",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "",
    "text": "#WOOOOOOOOP\noj &lt;- read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nnvars &lt;- format(round(ncol(oj), 0), \n                nsmall=0, \n                big.mark=\",\") \nnobs &lt;- format(round(nrow(oj), 0), \n                nsmall=0, \n                big.mark=\",\")\nThe number of variables is 4; the number of observations is 28,947.\nRoses are red\nviolets are blue."
  },
  {
    "objectID": "quarto-template.html#data-summary",
    "href": "quarto-template.html#data-summary",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.1 Data Summary",
    "text": "2.1 Data Summary\n\nSummary statistics (Use skimr::skim())"
  },
  {
    "objectID": "quarto-template.html#data-visualization",
    "href": "quarto-template.html#data-visualization",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\n\noj %&gt;% \n  ggplot(aes(x = log(sales), \n             y = log(price),\n             color = brand)) +\n  geom_point(alpha = .1) +\n  geom_smooth(method = lm, se = F) +\n  facet_wrap(.~ad) +\n  theme_bw() +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "quarto-template.html#data-transformation",
    "href": "quarto-template.html#data-transformation",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.3 Data Transformation",
    "text": "2.3 Data Transformation\n\nob_sum1 &lt;- oj %&gt;% \n  group_by(brand, ad) %&gt;% \n  summarise(sales_tot = sum(sales, na.rm = T),\n            price_mean = round(mean(price, na.rm = T), 2))"
  },
  {
    "objectID": "quarto-template.html#analysis",
    "href": "quarto-template.html#analysis",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.4 Analysis",
    "text": "2.4 Analysis"
  },
  {
    "objectID": "quarto-template.html#quotes",
    "href": "quarto-template.html#quotes",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.5 Quotes",
    "text": "2.5 Quotes\n\nQuote with &gt;\n\n\n“The truth is rarely pure and never simple.”\n— Oscar Wilde"
  },
  {
    "objectID": "quarto-template.html#inserting-figures",
    "href": "quarto-template.html#inserting-figures",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.6 Inserting Figures",
    "text": "2.6 Inserting Figures\nFor a demonstration of a DANL tiger, see Figure 1.\n\n\n\n\n\nFigure 1: DANL Tiger"
  },
  {
    "objectID": "quarto-template.html#inserting-a-html-page",
    "href": "quarto-template.html#inserting-a-html-page",
    "title": "DANL 200: Introduction to Data AnalyticsProject",
    "section": "2.7 Inserting a HTML page",
    "text": "2.7 Inserting a HTML page"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome All!",
    "section": "",
    "text": "Welcome to my blog powered by Quarto and GitHub!!\n\nAs a Data Analytics major, it is helpful to hone my abilities by doing exploratory data analysis. With that, I will be posting little projects here and there on my website, feel free to take a look!"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#description-of-variables-for-spotify-data-frame",
    "href": "posts/Spotify_all/Spotify_All.html#description-of-variables-for-spotify-data-frame",
    "title": "Spotify Listener Data",
    "section": "Description of Variables for `spotify Data Frame:",
    "text": "Description of Variables for `spotify Data Frame:\n\npid: A unique ID for a specific playlist\nplaylist_name: The name of a specific playlist\npos: Position of the track within a playlist (starting from 0)\nartist_name: Name of the artist on the track\ntrack_name: name of the track\nduration_ms: duration of the track (in milliseconds)\nalbum_name: name of the track’s album"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#summary-statistics-for-spotify",
    "href": "posts/Spotify_all/Spotify_All.html#summary-statistics-for-spotify",
    "title": "Spotify Listener Data",
    "section": "Summary Statistics for spotify:",
    "text": "Summary Statistics for spotify:\n\nskim(spotify)\n\n\nData summary\n\n\nName\nspotify\n\n\nNumber of rows\n198005\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nplaylist_name\n0\n1\n2\n55\n0\n2184\n0\n\n\nartist_name\n0\n1\n1\n117\n0\n18866\n0\n\n\ntrack_name\n0\n1\n1\n216\n0\n64115\n0\n\n\nalbum_name\n0\n1\n1\n255\n0\n35497\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npid\n0\n1\n323435.50\n466890.21\n0\n738\n1476\n999210\n999998\n▇▁▁▁▃\n\n\npos\n0\n1\n54.39\n48.35\n0\n17\n40\n79\n248\n▇▃▂▁▁\n\n\nduration_ms\n0\n1\n234740.84\n132918.60\n0\n198000\n224693\n258533\n20744575\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#top-5-artists-in-artist_name",
    "href": "posts/Spotify_all/Spotify_All.html#top-5-artists-in-artist_name",
    "title": "Spotify Listener Data",
    "section": "Top 5 Artists in artist_name",
    "text": "Top 5 Artists in artist_name\n\nspot1 &lt;- spotify %&gt;% \n  group_by(artist_name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(5) %&gt;% \n  mutate(prop = n/sum(n),\n         art_fct = reorder(artist_name, prop))\nView(spot1)\n\nggplot(data = spot1)+\n  geom_bar(aes(x = art_fct, y = prop, fill = artist_name), stat = 'identity')+\n  theme(axis.text.x=element_text(angle=45,hjust=1,vjust=0.5))+\n  ggtitle('Frequency of Top 5 Artists')+\n  xlab('Artist Name')+\n  ylab('Proportion')\n\n\n\nlength(unique(spotify$artist_name))\n\n[1] 18866\n\n\n\nComments on analyses…\n\nOut of the 18866 different artist names in the spotify data frame, the top five are…\n\nDrake\nKanye West\nKendrick Lamar\nRihanna\nThe Weeknd\n\n\nThese are also in order of greatest to least frequency out of the top 5*"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#top-5-songs-in-track_name",
    "href": "posts/Spotify_all/Spotify_All.html#top-5-songs-in-track_name",
    "title": "Spotify Listener Data",
    "section": "Top 5 songs in track_name",
    "text": "Top 5 songs in track_name\n\nspot2 &lt;- spotify %&gt;% \n  group_by(track_name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(5) %&gt;% \n  mutate(prop = n/sum(n),\n         track_fct = reorder(track_name, prop))\n\nggplot(data = spot2)+\n  geom_bar(aes(x = track_fct, y = prop), stat = 'identity', fill = 'blue')+\n  theme(axis.text.x=element_text(angle=45,hjust=1,vjust=0.5))+\n  ggtitle('Top 5 Songs')+\n  xlab('Song Name')+\n  ylab('Proportion')\n\n\n\nlength(unique(spotify$track_name))\n\n[1] 64115\n\n\n\nComments on analyses…\n\nThere are a total of 64115 unique values in track_name\nOut of those 64115 values, the top 5 are…\n\nCloser\nOne Dance\nHUMBLE\nHome\nRoses\n\n\nIn order of decreasing proportion within the top 5 songs*"
  },
  {
    "objectID": "posts/Spotify_all/Spotify_All.html#most-popular-song-in-track_name-for-each-of-the-top-5-artists-in-artist_name",
    "href": "posts/Spotify_all/Spotify_All.html#most-popular-song-in-track_name-for-each-of-the-top-5-artists-in-artist_name",
    "title": "Spotify Listener Data",
    "section": "Most Popular Song in track_name for Each of the Top 5 Artists in artist_name",
    "text": "Most Popular Song in track_name for Each of the Top 5 Artists in artist_name\n\nspot3 &lt;- spotify %&gt;% \n  filter(artist_name %in% spot1$art_fct) %&gt;% \n  group_by(artist_name, track_name) %&gt;% \n  summarise(n = n())\nspot3.5 &lt;- spot3 %&gt;% \n  group_by(artist_name) %&gt;% \n  slice_max(order_by = n, n = 1) %&gt;%  #Selects the top n value for each value of artist, the top n rows of top n values\n  ungroup() %&gt;% \n  arrange(-n)\n  \nggplot(data = spot3.5) + \n  geom_bar(aes(x = artist_name, y = n, fill = track_name), stat = 'identity')+\n  labs(title = 'Most Popular Song For Each Top 5 Artist',\n       x = 'Artist',\n       y = 'Count')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nComments on analyses…\n\nAbove is the top songtrack_name value and it’s count in the spotify data frame for each of the top 5 values of artist_name\n\nDrake : One Dance\nKanye West : GOLD DIGGER\nKendrick Lamar : HUMBLE\nRihanna : Needed Me\nThe Weeknd : Starboy"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html",
    "href": "posts/pandas/Pandas_Basics.html",
    "title": "Pandas Basics",
    "section": "",
    "text": "Loading Data Frame\nGetting a Summary of Data Frame\nSelecting Variables\nCounting Variables\nSorting Data Frame\nIndexing and Renaming Data Frame.\nLocating Data Within\n\n\n\n\n## Series is a collection of a one dimensional object containing a collection of values of one data type\n## Data Frame is a collection of series columns with an index\n\n\n\n\n\n# CSV file is a plain-text file using commas to separate values\n# use pandas function read_csv()\n\nimport pandas as pd\nnba = pd.read_csv(\"https://bcdanl.github.io/data/nba.csv\",\n                  parse_dates=[\"Birthday\"]) # as done in here, can use parse parameter to force data into date/time type\n\ntype(nba) #pandas data frame type\nnba\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n0\nShake Milton\nPhiladelphia 76ers\nSG\n1996-09-26\n1445697.0\n\n\n1\nChristian Wood\nDetroit Pistons\nPF\n1995-09-27\n1645357.0\n\n\n2\nPJ Washington\nCharlotte Hornets\nPF\n1998-08-23\n3831840.0\n\n\n3\nDerrick Rose\nDetroit Pistons\nPG\n1988-10-04\n7317074.0\n\n\n4\nMarial Shayok\nPhiladelphia 76ers\nG\n1995-07-26\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n445\nAustin Rivers\nHouston Rockets\nPG\n1992-08-01\n2174310.0\n\n\n446\nHarry Giles\nSacramento Kings\nPF\n1998-04-22\n2578800.0\n\n\n447\nRobin Lopez\nMilwaukee Bucks\nC\n1988-04-01\n4767000.0\n\n\n448\nCollin Sexton\nCleveland Cavaliers\nPG\n1999-01-04\n4764960.0\n\n\n449\nRicky Rubio\nPhoenix Suns\nPG\n1990-10-21\n16200000.0\n\n\n\n\n\n450 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\nnba # After running the code above, data tables look better formatted using google colab\n\n!pip install itables\nfrom itables import show,init_notebook_mode\n\nRequirement already satisfied: itables in /usr/local/lib/python3.10/dist-packages (1.7.1)\nRequirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from itables) (7.34.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from itables) (1.5.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from itables) (1.25.2)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (67.7.2)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.19.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (3.0.43)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (2.16.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.9.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2023.4)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;IPython-&gt;itables) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;IPython-&gt;itables) (0.2.13)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;itables) (1.16.0)\n\n\n\nshow(nba) #gives an interactive table for df\n\n\n\n\n\n\n    \n      \n      Name\n      Team\n      Position\n      Birthday\n      Salary\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n# The dot operator (ie; DataFrame.) is used for an attribute or method on object\n# A method is a function that we can call on a df (DataFrame.method())\n    ## ie; nba.info()\n# Attribute is a property that provides info about a df's structure or content without modifying it (DataFrame.attribute)\n    ## ie; nba.dtype (NO ()s)\n\n\n\n\n\nimport pandas as pd\nnba = pd.read_csv(\"https://bcdanl.github.io/data/nba.csv\",\n                  parse_dates=[\"Birthday\"])\n\nnba.info() # method | gives structural info about data frame (object data type is string)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 450 entries, 0 to 449\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   Name      450 non-null    object        \n 1   Team      450 non-null    object        \n 2   Position  450 non-null    object        \n 3   Birthday  450 non-null    datetime64[ns]\n 4   Salary    449 non-null    float64       \ndtypes: datetime64[ns](1), float64(1), object(3)\nmemory usage: 17.7+ KB\n\n\n\nnba.shape # attribute | gives number of variables/observations in tuple (obs, vars)\nnba.dtypes # attribute | gives data type of each variable\nnba.columns # attribute | gives variable names in list\nnba.count() # method | gives number of NON missing values in each variable (NaN is used to represent missing values)\n\nName        450\nTeam        450\nPosition    450\nBirthday    450\nSalary      449\ndtype: int64\n\n\n\nnba.describe() # method | generates descriptive stats for each numeric variable\nnba.describe(include = 'all') # method | gives descriptive stats for ALL variables, not just numeric\n\nFutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n  nba.describe(include = 'all') # method | gives descriptive stats for ALL variables, not just numeric\n\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\ncount\n450\n450\n450\n450\n4.490000e+02\n\n\nunique\n450\n30\n9\n430\nNaN\n\n\ntop\nShake Milton\nPhiladelphia 76ers\nPG\n1995-09-28 00:00:00\nNaN\n\n\nfreq\n1\n17\n98\n3\nNaN\n\n\nfirst\nNaN\nNaN\nNaN\n1977-01-26 00:00:00\nNaN\n\n\nlast\nNaN\nNaN\nNaN\n2000-12-23 00:00:00\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n7.670452e+06\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n9.292269e+06\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n7.956800e+04\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n1.618520e+06\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n3.321029e+06\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n1.013391e+07\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n4.023176e+07\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n\n\n# To get a series of selected variable --&gt; x = data_frame['Var_Name']\nnba_player_name_1 = nba['Name'] # Returns a SERIES with JUST the specified variable 'Name'\n\n# To get the same var in a data frame --&gt; y = data_frame[['Var_Name']]\nnba_player_name_2 = nba[['Name']] # Returns a DATA FRAME\n\n# Can select multiple variables by separating with commas in list\nnba_name_team = nba[['Name', 'Team']]\n\n# Can also use select_dtypes() to select multiple variables\n  # inclue = and exclude = parameters are accepted\n\nonly_str = nba.select_dtypes(include = 'object') # ONLY has variables with string data type\n\nno_string_int = nba.select_dtypes(exclude = ['object', 'int']) # Excludes variables with string OR int data type\n\n\n\n\n\n### .count() counts number of NON missing values in a series or data frame\n\nnba['Salary'].count() # returns number of non missing values in salary variable\n\nnba[['Salary', 'Name']].count() # double brackets to see results for more than one variable\n\n### .value_counts() counts number of occurances of each unique value in series or data frame\n\nnba['Team'].value_counts() # returns number of occurances of each Team, so number of players on each team\n\nnba[['Team']].value_counts() # no difference in data type (series or data frame)\n\nnba[['Team', 'Name']].value_counts() # Gives count of each unique combination of two variables\n\n### .nunique() counts number of unique values in each variable in a data frame\n\nnba[['Team']].nunique() # Gives number of nba teams in data frame\n\nnba.nunique() # gives number of unique values for EVERY variable in data frame\n\nName        450\nTeam         30\nPosition      9\nBirthday    430\nSalary      269\ndtype: int64\n\n\n\n\n\n\nimport pandas as pd\nnfl = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\")\n\n# Q1\nnfl = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\",\n                  parse_dates = ['Birthday'])\n\n\n# Q2\nnfl.shape\n  # 1,655 observations\nnfl.describe(include = 'all')\n\nFutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n  nfl.describe(include = 'all')\n\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\ncount\n1655\n1655\n1655\n1655\n1.655000e+03\n\n\nunique\n1643\n32\n21\n1316\nNaN\n\n\ntop\nRyan Anderson\nNew York Jets\nWR\n1995-03-21 00:00:00\nNaN\n\n\nfreq\n3\n58\n183\n6\nNaN\n\n\nfirst\nNaN\nNaN\nNaN\n1977-08-03 00:00:00\nNaN\n\n\nlast\nNaN\nNaN\nNaN\n1998-12-19 00:00:00\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n1.861569e+06\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n2.748519e+06\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n3.780000e+05\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n5.700000e+05\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n8.000000e+05\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n1.839952e+06\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n2.750000e+07\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Q3\nnfl['Team'].value_counts() # num players per team\n\nnfl['Team'].nunique() # 32 teams\n\n32"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#series-and-data-frames",
    "href": "posts/pandas/Pandas_Basics.html#series-and-data-frames",
    "title": "Pandas Basics",
    "section": "",
    "text": "## Series is a collection of a one dimensional object containing a collection of values of one data type\n## Data Frame is a collection of series columns with an index"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#importing-a-data-set-with-read_csv",
    "href": "posts/pandas/Pandas_Basics.html#importing-a-data-set-with-read_csv",
    "title": "Pandas Basics",
    "section": "",
    "text": "# CSV file is a plain-text file using commas to separate values\n# use pandas function read_csv()\n\nimport pandas as pd\nnba = pd.read_csv(\"https://bcdanl.github.io/data/nba.csv\",\n                  parse_dates=[\"Birthday\"]) # as done in here, can use parse parameter to force data into date/time type\n\ntype(nba) #pandas data frame type\nnba\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n0\nShake Milton\nPhiladelphia 76ers\nSG\n1996-09-26\n1445697.0\n\n\n1\nChristian Wood\nDetroit Pistons\nPF\n1995-09-27\n1645357.0\n\n\n2\nPJ Washington\nCharlotte Hornets\nPF\n1998-08-23\n3831840.0\n\n\n3\nDerrick Rose\nDetroit Pistons\nPG\n1988-10-04\n7317074.0\n\n\n4\nMarial Shayok\nPhiladelphia 76ers\nG\n1995-07-26\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n445\nAustin Rivers\nHouston Rockets\nPG\n1992-08-01\n2174310.0\n\n\n446\nHarry Giles\nSacramento Kings\nPF\n1998-04-22\n2578800.0\n\n\n447\nRobin Lopez\nMilwaukee Bucks\nC\n1988-04-01\n4767000.0\n\n\n448\nCollin Sexton\nCleveland Cavaliers\nPG\n1999-01-04\n4764960.0\n\n\n449\nRicky Rubio\nPhoenix Suns\nPG\n1990-10-21\n16200000.0\n\n\n\n\n\n450 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\nnba # After running the code above, data tables look better formatted using google colab\n\n!pip install itables\nfrom itables import show,init_notebook_mode\n\nRequirement already satisfied: itables in /usr/local/lib/python3.10/dist-packages (1.7.1)\nRequirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from itables) (7.34.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from itables) (1.5.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from itables) (1.25.2)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (67.7.2)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.19.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (3.0.43)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (2.16.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.9.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2023.4)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;IPython-&gt;itables) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;IPython-&gt;itables) (0.2.13)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;itables) (1.16.0)\n\n\n\nshow(nba) #gives an interactive table for df\n\n\n\n\n\n\n    \n      \n      Name\n      Team\n      Position\n      Birthday\n      Salary\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#dot-operators-methods-and-attributes",
    "href": "posts/pandas/Pandas_Basics.html#dot-operators-methods-and-attributes",
    "title": "Pandas Basics",
    "section": "",
    "text": "# The dot operator (ie; DataFrame.) is used for an attribute or method on object\n# A method is a function that we can call on a df (DataFrame.method())\n    ## ie; nba.info()\n# Attribute is a property that provides info about a df's structure or content without modifying it (DataFrame.attribute)\n    ## ie; nba.dtype (NO ()s)"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#getting-summary-of-data-frame-with-.info",
    "href": "posts/pandas/Pandas_Basics.html#getting-summary-of-data-frame-with-.info",
    "title": "Pandas Basics",
    "section": "",
    "text": "import pandas as pd\nnba = pd.read_csv(\"https://bcdanl.github.io/data/nba.csv\",\n                  parse_dates=[\"Birthday\"])\n\nnba.info() # method | gives structural info about data frame (object data type is string)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 450 entries, 0 to 449\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   Name      450 non-null    object        \n 1   Team      450 non-null    object        \n 2   Position  450 non-null    object        \n 3   Birthday  450 non-null    datetime64[ns]\n 4   Salary    449 non-null    float64       \ndtypes: datetime64[ns](1), float64(1), object(3)\nmemory usage: 17.7+ KB\n\n\n\nnba.shape # attribute | gives number of variables/observations in tuple (obs, vars)\nnba.dtypes # attribute | gives data type of each variable\nnba.columns # attribute | gives variable names in list\nnba.count() # method | gives number of NON missing values in each variable (NaN is used to represent missing values)\n\nName        450\nTeam        450\nPosition    450\nBirthday    450\nSalary      449\ndtype: int64\n\n\n\nnba.describe() # method | generates descriptive stats for each numeric variable\nnba.describe(include = 'all') # method | gives descriptive stats for ALL variables, not just numeric\n\nFutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n  nba.describe(include = 'all') # method | gives descriptive stats for ALL variables, not just numeric\n\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\ncount\n450\n450\n450\n450\n4.490000e+02\n\n\nunique\n450\n30\n9\n430\nNaN\n\n\ntop\nShake Milton\nPhiladelphia 76ers\nPG\n1995-09-28 00:00:00\nNaN\n\n\nfreq\n1\n17\n98\n3\nNaN\n\n\nfirst\nNaN\nNaN\nNaN\n1977-01-26 00:00:00\nNaN\n\n\nlast\nNaN\nNaN\nNaN\n2000-12-23 00:00:00\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n7.670452e+06\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n9.292269e+06\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n7.956800e+04\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n1.618520e+06\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n3.321029e+06\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n1.013391e+07\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n4.023176e+07"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#selecting-variable-by-name",
    "href": "posts/pandas/Pandas_Basics.html#selecting-variable-by-name",
    "title": "Pandas Basics",
    "section": "",
    "text": "# To get a series of selected variable --&gt; x = data_frame['Var_Name']\nnba_player_name_1 = nba['Name'] # Returns a SERIES with JUST the specified variable 'Name'\n\n# To get the same var in a data frame --&gt; y = data_frame[['Var_Name']]\nnba_player_name_2 = nba[['Name']] # Returns a DATA FRAME\n\n# Can select multiple variables by separating with commas in list\nnba_name_team = nba[['Name', 'Team']]\n\n# Can also use select_dtypes() to select multiple variables\n  # inclue = and exclude = parameters are accepted\n\nonly_str = nba.select_dtypes(include = 'object') # ONLY has variables with string data type\n\nno_string_int = nba.select_dtypes(exclude = ['object', 'int']) # Excludes variables with string OR int data type"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#counting-with-.count-.value_counts-and-.nunique",
    "href": "posts/pandas/Pandas_Basics.html#counting-with-.count-.value_counts-and-.nunique",
    "title": "Pandas Basics",
    "section": "",
    "text": "### .count() counts number of NON missing values in a series or data frame\n\nnba['Salary'].count() # returns number of non missing values in salary variable\n\nnba[['Salary', 'Name']].count() # double brackets to see results for more than one variable\n\n### .value_counts() counts number of occurances of each unique value in series or data frame\n\nnba['Team'].value_counts() # returns number of occurances of each Team, so number of players on each team\n\nnba[['Team']].value_counts() # no difference in data type (series or data frame)\n\nnba[['Team', 'Name']].value_counts() # Gives count of each unique combination of two variables\n\n### .nunique() counts number of unique values in each variable in a data frame\n\nnba[['Team']].nunique() # Gives number of nba teams in data frame\n\nnba.nunique() # gives number of unique values for EVERY variable in data frame\n\nName        450\nTeam         30\nPosition      9\nBirthday    430\nSalary      269\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#classwork-5-q1-q3",
    "href": "posts/pandas/Pandas_Basics.html#classwork-5-q1-q3",
    "title": "Pandas Basics",
    "section": "",
    "text": "import pandas as pd\nnfl = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\")\n\n# Q1\nnfl = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\",\n                  parse_dates = ['Birthday'])\n\n\n# Q2\nnfl.shape\n  # 1,655 observations\nnfl.describe(include = 'all')\n\nFutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n  nfl.describe(include = 'all')\n\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\ncount\n1655\n1655\n1655\n1655\n1.655000e+03\n\n\nunique\n1643\n32\n21\n1316\nNaN\n\n\ntop\nRyan Anderson\nNew York Jets\nWR\n1995-03-21 00:00:00\nNaN\n\n\nfreq\n3\n58\n183\n6\nNaN\n\n\nfirst\nNaN\nNaN\nNaN\n1977-08-03 00:00:00\nNaN\n\n\nlast\nNaN\nNaN\nNaN\n1998-12-19 00:00:00\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n1.861569e+06\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n2.748519e+06\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n3.780000e+05\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n5.700000e+05\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n8.000000e+05\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n1.839952e+06\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n2.750000e+07\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Q3\nnfl['Team'].value_counts() # num players per team\n\nnfl['Team'].nunique() # 32 teams\n\n32"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#selecting-firstlast-n-obs.-with-.head-and-.tail",
    "href": "posts/pandas/Pandas_Basics.html#selecting-firstlast-n-obs.-with-.head-and-.tail",
    "title": "Pandas Basics",
    "section": "Selecting first/last n obs. with .head() and .tail()",
    "text": "Selecting first/last n obs. with .head() and .tail()\n\n# .head() method is used to return first values (5 by default, but n can be specified)\nnba.head() # first 5 obs\nnba.head(10) # first 10 obs\n\n# .tail() method is used to return last values\nnba.tail()\nnba.tail(10)\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n440\nJared Dudley\nLos Angeles Lakers\nPF\n1985-07-10\n2564753.0\n\n\n441\nMax Strus\nChicago Bulls\nSG\n1996-03-28\n79568.0\n\n\n442\nKevon Looney\nGolden State Warriors\nC\n1996-02-06\n4464286.0\n\n\n443\nWilly Hernangomez\nCharlotte Hornets\nC\n1994-05-27\n1557250.0\n\n\n444\nMelvin Frazier\nOrlando Magic\nSG\n1996-08-30\n1416852.0\n\n\n445\nAustin Rivers\nHouston Rockets\nPG\n1992-08-01\n2174310.0\n\n\n446\nHarry Giles\nSacramento Kings\nPF\n1998-04-22\n2578800.0\n\n\n447\nRobin Lopez\nMilwaukee Bucks\nC\n1988-04-01\n4767000.0\n\n\n448\nCollin Sexton\nCleveland Cavaliers\nPG\n1999-01-04\n4764960.0\n\n\n449\nRicky Rubio\nPhoenix Suns\nPG\n1990-10-21\n16200000.0"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#sorting-by-a-single-variable-with-.sort_values",
    "href": "posts/pandas/Pandas_Basics.html#sorting-by-a-single-variable-with-.sort_values",
    "title": "Pandas Basics",
    "section": "Sorting by a single variable with .sort_values()",
    "text": "Sorting by a single variable with .sort_values()\n\nnba.sort_values(['Name']) # sorts name in alphabetical order, in ascending order\nnba.sort_values(['Name'], ascending = False) # reverse alphebetical order\n\ndf = nba.sort_values(['Salary'])\ndf.head() # returns lowest 5 player salaries\ndf.tail() # returns highest 5 player salaries, NaN values at END\n\ndf_desc = nba.sort_values(['Salary'], ascending = False)\ndf_desc.head() # gives top 5 highest salaries, no NaN since still at bottom\ndf_desc.tail() # now here is lowest 5 salaries\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n280\nJohnathan Motley\nLos Angeles Clippers\nPF\n1995-05-04\n79568.0\n\n\n383\nJosh Gray\nNew Orleans Pelicans\nPG\n1993-09-09\n79568.0\n\n\n197\nCharlie Brown\nAtlanta Hawks\nSG\n1997-02-02\n79568.0\n\n\n283\nGarrison Mathews\nWashington Wizards\nSG\n1996-10-24\n79568.0\n\n\n4\nMarial Shayok\nPhiladelphia 76ers\nG\n1995-07-26\nNaN"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#method-chaining",
    "href": "posts/pandas/Pandas_Basics.html#method-chaining",
    "title": "Pandas Basics",
    "section": "Method Chaining",
    "text": "Method Chaining\n\n# Allows us to call methods sequentially without need to store intermediates\n  ## ie; df =, then df.head()\n\n(\n    nba\n    .sort_values(['Salary'], ascending = False)\n    .head(10)\n)\n\n# Can also chain like this...\n\nnba.sort_values(['Salary'], ascending = False).head(10)\n\n  # BUT if have a lot of methods or attributes can get very long\n  # Using parentheses (as shown in example above) allows for better organization\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n205\nStephen Curry\nGolden State Warriors\nPG\n1988-03-14\n40231758.0\n\n\n219\nRussell Westbrook\nHouston Rockets\nPG\n1988-11-12\n38506482.0\n\n\n38\nChris Paul\nOklahoma City Thunder\nPG\n1985-05-06\n38506482.0\n\n\n264\nJames Harden\nHouston Rockets\nPG\n1989-08-26\n38199000.0\n\n\n251\nJohn Wall\nWashington Wizards\nPG\n1990-09-06\n38199000.0\n\n\n408\nLeBron James\nLos Angeles Lakers\nPF\n1984-12-30\n37436858.0\n\n\n95\nKevin Durant\nBrooklyn Nets\nPF\n1988-09-29\n37199000.0\n\n\n317\nBlake Griffin\nDetroit Pistons\nPF\n1989-03-16\n34449964.0\n\n\n323\nKyle Lowry\nToronto Raptors\nPG\n1986-03-25\n33296296.0\n\n\n397\nPaul George\nLos Angeles Clippers\nSF\n1990-05-02\n33005556.0"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#sorting-by-single-variable-using-.nsmallest-and-.nlargest",
    "href": "posts/pandas/Pandas_Basics.html#sorting-by-single-variable-using-.nsmallest-and-.nlargest",
    "title": "Pandas Basics",
    "section": "Sorting by Single Variable Using .nsmallest() and .nlargest()",
    "text": "Sorting by Single Variable Using .nsmallest() and .nlargest()\n\n# .nsmallest() useful to get first n obs ordered by variable in ascending order\nnba.nsmallest(5, 'Salary') # 5 smallest salaries\n\n# .nlargest() useful to get first n obs ordered by a variable in descending order\nnba.nlargest(5, 'Salary') # 5 largest salaries\n\nnba.nsmallest(4, 'Salary', keep = 'all') # keep = 'all' keeps all duplicates, even if means selecting more than n obs\n\n\n  \n    \n\n\n\n\n\n\nName\nTeam\nPosition\nBirthday\nSalary\n\n\n\n\n12\nNorvel Pelle\nPhiladelphia 76ers\nFC\n1993-02-03\n79568.0\n\n\n24\nJaylen Hoard\nPortland Trail Blazers\nSF\n1999-03-30\n79568.0\n\n\n25\nTyler Cook\nCleveland Cavaliers\nPF\n1997-09-23\n79568.0\n\n\n31\nMichael Frazier\nHouston Rockets\nG\n1994-03-08\n79568.0\n\n\n37\nDean Wade\nCleveland Cavaliers\nPF\n1996-11-20\n79568.0\n\n\n45\nRobert Franks\nCharlotte Hornets\nF\n1996-12-18\n79568.0\n\n\n73\nJustin Wright-Foreman\nUtah Jazz\nG\n1997-10-27\n79568.0\n\n\n81\nAdam Mokoka\nChicago Bulls\nG\n1998-07-18\n79568.0\n\n\n97\nChris Silva\nMiami Heat\nPF\n1996-09-19\n79568.0\n\n\n100\nBrian Bowen\nIndiana Pacers\nSG\n1998-10-02\n79568.0\n\n\n105\nJohn Konchar\nMemphis Grizzlies\nSG\n1996-03-22\n79568.0\n\n\n125\nQuinndary Weatherspoon\nSan Antonio Spurs\nG\n1996-09-10\n79568.0\n\n\n131\nLuguentz Dort\nOklahoma City Thunder\nG\n1999-04-19\n79568.0\n\n\n143\nYuta Watanabe\nMemphis Grizzlies\nSF\n1994-10-13\n79568.0\n\n\n145\nKelan Martin\nMinnesota Timberwolves\nSF\n1995-08-03\n79568.0\n\n\n160\nTremont Waters\nBoston Celtics\nPG\n1998-01-10\n79568.0\n\n\n185\nKy Bowman\nGolden State Warriors\nPG\n1997-06-16\n79568.0\n\n\n188\nJordan McLaughlin\nMinnesota Timberwolves\nPG\n1996-04-09\n79568.0\n\n\n197\nCharlie Brown\nAtlanta Hawks\nSG\n1997-02-02\n79568.0\n\n\n216\nPJ Dozier\nDenver Nuggets\nPG\n1996-10-25\n79568.0\n\n\n218\nJared Harper\nPhoenix Suns\nPG\n1997-09-14\n79568.0\n\n\n221\nOshae Brissett\nToronto Raptors\nSF\n1998-06-20\n79568.0\n\n\n227\nAmir Coffey\nLos Angeles Clippers\nG\n1997-06-17\n79568.0\n\n\n229\nIvan Rabb\nNew York Knicks\nPF\n1997-02-04\n79568.0\n\n\n248\nZylan Cheatham\nNew Orleans Pelicans\nSF\n1995-11-17\n79568.0\n\n\n270\nJarrell Brantley\nUtah Jazz\nPF\n1996-06-07\n79568.0\n\n\n276\nBrandon Goodwin\nAtlanta Hawks\nPG\n1995-10-02\n79568.0\n\n\n279\nHenry Ellenson\nBrooklyn Nets\nPF\n1997-01-13\n79568.0\n\n\n280\nJohnathan Motley\nLos Angeles Clippers\nPF\n1995-05-04\n79568.0\n\n\n283\nGarrison Mathews\nWashington Wizards\nSG\n1996-10-24\n79568.0\n\n\n295\nWenyen Gabriel\nSacramento Kings\nPF\n1997-03-26\n79568.0\n\n\n302\nZach Norvell\nLos Angeles Lakers\nSG\n1997-12-09\n79568.0\n\n\n322\nJosh Magette\nOrlando Magic\nPG\n1989-11-28\n79568.0\n\n\n333\nAntonius Cleveland\nDallas Mavericks\nSG\n1994-02-02\n79568.0\n\n\n334\nDamion Lee\nGolden State Warriors\nSG\n1992-10-21\n79568.0\n\n\n353\nChris Chiozza\nWashington Wizards\nPG\n1995-11-21\n79568.0\n\n\n363\nDaryl Macon\nMiami Heat\nSG\n1995-11-29\n79568.0\n\n\n370\nBol Bol\nDenver Nuggets\nC\n1999-11-16\n79568.0\n\n\n383\nJosh Gray\nNew Orleans Pelicans\nPG\n1993-09-09\n79568.0\n\n\n385\nKobi Simmons\nCharlotte Hornets\nPG\n1997-07-04\n79568.0\n\n\n390\nDrew Eubanks\nSan Antonio Spurs\nPF\n1997-02-01\n79568.0\n\n\n395\nTacko Fall\nBoston Celtics\nC\n1995-12-10\n79568.0\n\n\n400\nLouis King\nDetroit Pistons\nF\n1999-04-06\n79568.0\n\n\n401\nKostas Antetokounmpo\nLos Angeles Lakers\nPF\n1997-11-20\n79568.0\n\n\n411\nChris Clemons\nHouston Rockets\nSG\n1997-07-23\n79568.0\n\n\n434\nKyle Guy\nSacramento Kings\nG\n1997-08-11\n79568.0\n\n\n435\nKadeem Allen\nNew York Knicks\nPG\n1993-01-15\n79568.0\n\n\n441\nMax Strus\nChicago Bulls\nSG\n1996-03-28\n79568.0"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#isin-method",
    "href": "posts/pandas/Pandas_Basics.html#isin-method",
    "title": "Pandas Basics",
    "section": "isin() method",
    "text": "isin() method\n\n# The method above is not very convenient\n  # Can use easier method using list\n\nstar_teams = ['Sales', 'Legal', 'Finance']\non_star_teams = emp['Team'].isin(star_teams)\nemp[on_star_teams]['Team'].value_counts() # Gives same result as method above, only finance\n\nFinance         102\nSales            94\nLegal            88\nBusiness Dev      0\nDistribution      0\nEngineering       0\nHR                0\nIT                0\nMarketing         0\nProduct           0\nName: Team, dtype: int64"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#between-conditions",
    "href": "posts/pandas/Pandas_Basics.html#between-conditions",
    "title": "Pandas Basics",
    "section": "Between Conditions",
    "text": "Between Conditions\n\nh_90k = emp['Salary'] &gt;= 90000\nl_100k = emp['Salary'] &lt; 100000\nemp[h_90k & l_100k]['Salary'] # Value of Salary will be greater than or equal to 90k and less than 100k\n\n0       90655\n8       95570\n16      90370\n22      90816\n24      97950\n        ...  \n975     92436\n982     91411\n994     98874\n997     96914\n1000    90655\nName: Salary, Length: 96, dtype: int64\n\n\n\nUsing .between() method\n\n# Lower bound is Inclusive and Upper bound is exclusive\n  # NOT JUST FOR NUMERIC\n\n\n# NUMERIC EX\nbetween_90k_and_100k = emp[\"Salary\"].between(90000, 100000)\nemp[between_90k_and_100k] # Gives same result as above, just better to read, and more convenient\n\n\n  \n    \n\n\n\n\n\n\nFirst Name\nGender\nStart Date\nSalary\nMgmt\nTeam\n\n\n\n\n0\nDouglas\nMale\n1993-08-06\n90655\nTrue\nMarketing\n\n\n8\nAngela\nFemale\n2005-11-22\n95570\nTrue\nEngineering\n\n\n16\nJeremy\nMale\n2010-09-21\n90370\nFalse\nHR\n\n\n22\nJoshua\nNaN\n2012-03-08\n90816\nTrue\nIT\n\n\n24\nJohn\nMale\n1992-07-01\n97950\nFalse\nIT\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n975\nSusan\nFemale\n1995-04-07\n92436\nFalse\nSales\n\n\n982\nRose\nFemale\n1982-04-06\n91411\nTrue\nHR\n\n\n994\nGeorge\nMale\n2013-06-21\n98874\nTrue\nMarketing\n\n\n997\nRussell\nMale\n2013-05-20\n96914\nFalse\nProduct\n\n\n1000\nNaN\nNaN\nNaT\n90655\nTrue\nNaN\n\n\n\n\n\n96 rows × 6 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# NON NUMERIC\nname_starts_with_t = emp['First Name'].between('T', 'U')\nemp[name_starts_with_t] # Returns only obs with name that starts with t\n\n\n  \n    \n\n\n\n\n\n\nFirst Name\nGender\nStart Date\nSalary\nMgmt\nTeam\n\n\n\n\n1\nThomas\nMale\n1996-03-31\n61933\nTrue\nNaN\n\n\n28\nTerry\nMale\n1981-11-27\n124008\nTrue\nIT\n\n\n35\nTheresa\nFemale\n2006-10-10\n85182\nFalse\nSales\n\n\n52\nTodd\nMale\n1990-02-18\n49339\nTrue\nHR\n\n\n58\nTheresa\nFemale\n2010-04-11\n72670\nTrue\nEngineering\n\n\n70\nTodd\nNaN\n2003-06-10\n84692\nFalse\nIT\n\n\n74\nThomas\nMale\n1995-06-04\n62096\nFalse\nMarketing\n\n\n98\nTina\nFemale\n2016-06-16\n100705\nTrue\nMarketing\n\n\n113\nTina\nFemale\n2009-06-12\n114767\nTrue\nEngineering\n\n\n143\nTeresa\nNaN\n2016-01-28\n140013\nTrue\nEngineering\n\n\n163\nTerry\nMale\n1990-09-03\n52226\nFalse\nIT\n\n\n227\nTodd\nMale\n1999-04-13\n59728\nTrue\nIT\n\n\n270\nThomas\nMale\n2011-04-06\n103235\nTrue\nHR\n\n\n283\nTodd\nMale\n2009-03-11\n107281\nTrue\nEngineering\n\n\n286\nTodd\nMale\n1984-02-02\n69989\nTrue\nFinance\n\n\n291\nTammy\nFemale\n1984-11-11\n132839\nTrue\nIT\n\n\n334\nTodd\nMale\n2011-05-23\n85074\nFalse\nIT\n\n\n350\nThomas\nNaN\n1995-08-31\n41549\nFalse\nSales\n\n\n426\nTodd\nMale\n2016-03-16\n134408\nTrue\nHR\n\n\n451\nTerry\nNaN\n2016-07-15\n140002\nTrue\nMarketing\n\n\n460\nTina\nFemale\n2005-01-17\n88276\nFalse\nLegal\n\n\n597\nTeresa\nFemale\n1987-06-24\n69740\nFalse\nDistribution\n\n\n609\nTodd\nMale\n2010-02-16\n103405\nFalse\nSales\n\n\n613\nTeresa\nNaN\n1992-01-03\n63103\nFalse\nFinance\n\n\n664\nTimothy\nMale\n2010-10-06\n49473\nFalse\nLegal\n\n\n704\nThomas\nMale\n1991-09-07\n65251\nFalse\nDistribution\n\n\n706\nTodd\nMale\n1993-07-04\n128175\nTrue\nNaN\n\n\n718\nTerry\nMale\n2010-10-21\n58357\nFalse\nEngineering\n\n\n751\nTina\nFemale\n1999-08-06\n102841\nFalse\nIT\n\n\n762\nTerry\nMale\n2004-11-10\n35633\nTrue\nDistribution\n\n\n795\nTheresa\nNaN\n1995-10-07\n42025\nTrue\nHR\n\n\n805\nThomas\nMale\n1990-05-03\n111371\nTrue\nEngineering\n\n\n876\nTerry\nNaN\n1992-09-11\n41238\nFalse\nMarketing\n\n\n891\nTimothy\nMale\n1991-08-25\n92587\nFalse\nFinance\n\n\n915\nTodd\nMale\n1983-01-04\n115566\nTrue\nIT\n\n\n929\nTheresa\nFemale\n2001-04-27\n75661\nTrue\nLegal\n\n\n952\nTeresa\nFemale\n2013-01-22\n113425\nTrue\nHR\n\n\n967\nThomas\nMale\n2016-03-12\n105681\nFalse\nEngineering\n\n\n993\nTina\nFemale\n1997-05-15\n56450\nTrue\nEngineering"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#isna-and-notna-methods",
    "href": "posts/pandas/Pandas_Basics.html#isna-and-notna-methods",
    "title": "Pandas Basics",
    "section": "isna() and notna() methods",
    "text": "isna() and notna() methods\n\n# isna() returns boolean series (True if missing, False if not missing)\nemp['Team'].isna().value_counts() # Gives # of missing values\nemp[\"Start Date\"].isna().value_counts() # Gives # of missing values here as well\n\nFalse    999\nTrue       2\nName: Start Date, dtype: int64\n\n\n\n# notna() returns inverse series, (True here means that a value is NOT MISSING)\n  # Can also use tilde symbol (~) to invert boolean series\n\nemp[\"Team\"].notna().value_counts(dropna = False) # True means # of non missing values.\n\nTrue     957\nFalse     44\nName: Team, dtype: int64\n\n\n\n~emp['Team'].isna().value_counts() # Is equivalent of line above\n\nFalse   -958\nTrue     -45\nName: Team, dtype: int64"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#using-.dropna-to-remove-missing-values",
    "href": "posts/pandas/Pandas_Basics.html#using-.dropna-to-remove-missing-values",
    "title": "Pandas Basics",
    "section": "Using .dropna() to remove missing values",
    "text": "Using .dropna() to remove missing values\n\n# .dropna() method removes obs that hold any NaN or NaT values\n  # can use .dropna(how = \"all\") to remove obs in which all values are missing\n    # how = \"any\" is the default argument\n      # Can use subset = [] parameter to target obs with na vals in a specific variable(s)\n        # thresh = n parameter specifies minimum threshold of n non missing values that an obs must have to not be removed\n\n\nemp.info() # has 1001 obs, no na obs removal\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1001 entries, 0 to 1000\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  933 non-null    object        \n 1   Gender      854 non-null    category      \n 2   Start Date  999 non-null    datetime64[ns]\n 3   Salary      1001 non-null   int64         \n 4   Mgmt        1001 non-null   bool          \n 5   Team        957 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 27.0+ KB\n\n\n\nemp.dropna().info() # Now there are 762 obs, the obs with any na values removed\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 762 entries, 0 to 999\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  762 non-null    object        \n 1   Gender      762 non-null    category      \n 2   Start Date  762 non-null    datetime64[ns]\n 3   Salary      762 non-null    int64         \n 4   Mgmt        762 non-null    bool          \n 5   Team        762 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 26.5+ KB\n\n\n\nemp.dropna(how = 'all').info() # No obs are removed since no obs have ALL values as na\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1001 entries, 0 to 1000\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  933 non-null    object        \n 1   Gender      854 non-null    category      \n 2   Start Date  999 non-null    datetime64[ns]\n 3   Salary      1001 non-null   int64         \n 4   Mgmt        1001 non-null   bool          \n 5   Team        957 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 27.0+ KB\n\n\n\nemp.dropna(subset = ['Gender']).info() # Now there are 854 obs\n                                        #since obs with na values in gender variable are removed\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 854 entries, 0 to 999\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  794 non-null    object        \n 1   Gender      854 non-null    category      \n 2   Start Date  853 non-null    datetime64[ns]\n 3   Salary      854 non-null    int64         \n 4   Mgmt        854 non-null    bool          \n 5   Team        815 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 29.7+ KB\n\n\n\nemp.dropna(subset = ['First Name', 'Gender']).info() # Now 794 obs exist\n                                                      #since obs with na values in\n                                                        # Gender or First Name variables are removed\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 794 entries, 0 to 999\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  794 non-null    object        \n 1   Gender      794 non-null    category      \n 2   Start Date  793 non-null    datetime64[ns]\n 3   Salary      794 non-null    int64         \n 4   Mgmt        794 non-null    bool          \n 5   Team        763 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 27.6+ KB\n\n\n\nemp.dropna(thresh = 4).info() # 999 obs are kept since 2 obs had less than 4 non missing values\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 999 entries, 0 to 999\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  933 non-null    object        \n 1   Gender      854 non-null    category      \n 2   Start Date  998 non-null    datetime64[ns]\n 3   Salary      999 non-null    int64         \n 4   Mgmt        999 non-null    bool          \n 5   Team        957 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 34.6+ KB"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#using-.duplicated-method-to-identify-duplicates",
    "href": "posts/pandas/Pandas_Basics.html#using-.duplicated-method-to-identify-duplicates",
    "title": "Pandas Basics",
    "section": "Using .duplicated() method to identify duplicates",
    "text": "Using .duplicated() method to identify duplicates\n\n# Missing vals are common in messy data sets, so are duplicate values\n  # .duplicated() returns a boolean series that identifies duplicates in a variable\n\n  # Has keep parameter, decides which duplicate occurance to keep\n\n\nemp['Team'].duplicated() # Returns true if values are duplicated\n\n0       False\n1       False\n2       False\n3        True\n4       False\n        ...  \n996      True\n997      True\n998      True\n999      True\n1000     True\nName: Team, Length: 1001, dtype: bool\n\n\n\nemp['Team'].duplicated(keep = 'first') # Default argument, Keeps first occurance of each dupe value\n\n0       False\n1       False\n2       False\n3        True\n4       False\n        ...  \n996      True\n997      True\n998      True\n999      True\n1000     True\nName: Team, Length: 1001, dtype: bool\n\n\n\nemp['Team'].duplicated(keep = 'last') # Keeps last occurance of each dupe value\n\n0        True\n1        True\n2        True\n3        True\n4        True\n        ...  \n996     False\n997     False\n998     False\n999     False\n1000    False\nName: Team, Length: 1001, dtype: bool\n\n\n\n~emp['Team'].duplicated() # Returns True if value is NOT duplicated\n\n0        True\n1        True\n2        True\n3       False\n4        True\n        ...  \n996     False\n997     False\n998     False\n999     False\n1000    False\nName: Team, Length: 1001, dtype: bool"
  },
  {
    "objectID": "posts/pandas/Pandas_Basics.html#using-.drop_duplicates-method-to-remove-obs-with-duplicates",
    "href": "posts/pandas/Pandas_Basics.html#using-.drop_duplicates-method-to-remove-obs-with-duplicates",
    "title": "Pandas Basics",
    "section": "Using .drop_duplicates() method to remove obs with duplicates",
    "text": "Using .drop_duplicates() method to remove obs with duplicates\n\n# Removes obs in which all values are equal to those in a previously encountered observation\n  # ALSO accepts `subset` = parameter\n    # Use to find first occurance of each unique value in a certain variable\n      # Also accepts multiple subset variables and the keep parameter\n        # keep = 'first' is default, last keeps last occurance, False keeps no occurances that were duped\n\n\nemp.drop_duplicates().info() # No obs are dropped since no obs are exactly the same as another\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1001 entries, 0 to 1000\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   First Name  933 non-null    object        \n 1   Gender      854 non-null    category      \n 2   Start Date  999 non-null    datetime64[ns]\n 3   Salary      1001 non-null   int64         \n 4   Mgmt        1001 non-null   bool          \n 5   Team        957 non-null    category      \ndtypes: bool(1), category(2), datetime64[ns](1), int64(1), object(1)\nmemory usage: 34.7+ KB\n\n\n\ndupe = emp.drop_duplicates(subset = ['Team']).value_counts(dropna = False)\nlen(dupe) # Now there are 10 total obs\n# Now each unique value of Team has only one occurance\n\n10\n\n\n\nemp.drop_duplicates(subset = ['First Name'], keep = False)\n# Keeps only unique names that never occured more than once\n\n\n  \n    \n\n\n\n\n\n\nFirst Name\nGender\nStart Date\nSalary\nMgmt\nTeam\n\n\n\n\n5\nDennis\nMale\n1987-04-18\n115163\nFalse\nLegal\n\n\n8\nAngela\nFemale\n2005-11-22\n95570\nTrue\nEngineering\n\n\n33\nJean\nFemale\n1993-12-18\n119082\nFalse\nBusiness Dev\n\n\n190\nCarol\nFemale\n1996-03-19\n57783\nFalse\nFinance\n\n\n291\nTammy\nFemale\n1984-11-11\n132839\nTrue\nIT\n\n\n495\nEugene\nMale\n1984-05-24\n81077\nFalse\nSales\n\n\n688\nBrian\nMale\n2007-04-07\n93901\nTrue\nLegal\n\n\n832\nKeith\nMale\n2003-02-12\n120672\nFalse\nLegal\n\n\n887\nDavid\nMale\n2009-12-05\n92242\nFalse\nLegal\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nemp.query(\"`First Name` == 'Douglas' & Gender == 'Male'\")\n\n\n  \n    \n\n\n\n\n\n\nFirst Name\nGender\nStart Date\nSalary\nMgmt\nTeam\n\n\n\n\n0\nDouglas\nMale\n1993-08-06\n90655\nTrue\nMarketing\n\n\n217\nDouglas\nMale\n1999-09-03\n83341\nTrue\nIT\n\n\n322\nDouglas\nMale\n2002-01-08\n41428\nFalse\nProduct\n\n\n835\nDouglas\nMale\n2007-08-04\n132175\nFalse\nEngineering\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nExample of .drop_duplicates usage\n\n# Sample DataFrame with duplicate observations\ndata = {\n    'Name': ['John', 'Anna', 'John', 'Mike', 'Anna'],\n    'Age': [28, 23, 28, 32, 23],\n    'City': ['New York', 'Paris', 'New York', 'London', 'Paris']\n}\n\n# pd.DataFrame( Series, List, or Dict ) creates a DataFrame\ndf = pd.DataFrame(data)\ndf_unique = df.drop_duplicates()\n\n\ndf.info() # Has 5 obs total\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    5 non-null      object\n 1   Age     5 non-null      int64 \n 2   City    5 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 248.0+ bytes\n\n\n\ndf.drop_duplicates().info() # Now has 3 since there were two duplicate obs found and removed\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 3 entries, 0 to 3\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   City    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 96.0+ bytes"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html",
    "title": "HW5 - NFL Data",
    "section": "",
    "text": "My repository can be found here"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#description-of-variables-in-nfl-data-frame",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#description-of-variables-in-nfl-data-frame",
    "title": "HW5 - NFL Data",
    "section": "Description of Variables in NFL Data Frame:",
    "text": "Description of Variables in NFL Data Frame:\n\nplay_id: Play Id number (numeric), when combined with game_id provides id for single play\ngame_id: 10 digit id for an NFL game\ndrive: Drive number (numeric)\nweek: Season week\nposteam: abv for team with possession\nqtr: Quarter in the game (qtr 5 means overtime)\nhalf_seconds_remaining: seconds remaining in the half\ndown: Down for a given play\npass: 1 if a pass play, 0 if not\nwp: Estimate for winning probability for posteam given situation at beginning of the play"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#summary-statistics-for-nfl-data-frame",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#summary-statistics-for-nfl-data-frame",
    "title": "HW5 - NFL Data",
    "section": "Summary Statistics for NFL Data Frame",
    "text": "Summary Statistics for NFL Data Frame\n\nskim(NFL)\n\n\nData summary\n\n\nName\nNFL\n\n\nNumber of rows\n50147\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngame_id\n0\n1.00\n13\n15\n0\n284\n0\n\n\nposteam\n3720\n0.93\n2\n3\n0\n32\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nplay_id\n0\n1.00\n2057.86\n1194.22\n1\n1039.00\n2034.00\n3065.50\n5523\n▇▇▇▅▁\n\n\ndrive\n450\n0.99\n11.48\n6.59\n1\n6.00\n11.00\n17.00\n35\n▇▇▇▂▁\n\n\nweek\n0\n1.00\n9.91\n5.61\n1\n5.00\n10.00\n15.00\n22\n▇▆▆▆▃\n\n\nqtr\n0\n1.00\n2.58\n1.14\n1\n2.00\n3.00\n4.00\n5\n▆▇▆▇▁\n\n\ndown\n8543\n0.83\n2.00\n1.00\n1\n1.00\n2.00\n3.00\n4\n▇▆▁▃▂\n\n\nhalf_seconds_remaining\n0\n1.00\n796.94\n564.41\n0\n255.00\n774.00\n1285.00\n1800\n▇▅▅▅▅\n\n\npass\n0\n1.00\n0.45\n0.50\n0\n0.00\n0.00\n1.00\n1\n▇▁▁▁▆\n\n\nwp\n284\n0.99\n0.51\n0.29\n0\n0.29\n0.52\n0.73\n1\n▆▆▇▆▆"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2a",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2a",
    "title": "HW5 - NFL Data",
    "section": "Q2a",
    "text": "Q2a\n\nIn data.frame, NFL2022_stuffs, remove observations for which values of posteam is missing.\n\nAnswer:\n\nNFL &lt;- NFL %&gt;% \n  filter(!(is.na(posteam)))"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2b",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2b",
    "title": "HW5 - NFL Data",
    "section": "Q2b",
    "text": "Q2b\n\nSummarize the mean value of pass for each posteam when all the following conditions hold:\n\nwp is greater than 20% and less than 75%;\ndown is less than or equal to 2; and\nhalf_seconds_remaining is greater than 120.\n\n\nAnswer:\n\nNFL_2b &lt;- NFL %&gt;% \n  filter(wp &gt; 0.2 & wp &lt; 0.75 & down &lt;= 2 & half_seconds_remaining &gt; 120)"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2c",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2c",
    "title": "HW5 - NFL Data",
    "section": "Q2c",
    "text": "Q2c\n\nProvide both (1) a ggplot code with geom_point() using the resulting data.frame in Q2b and (2) a simple comments to describe the mean value of pass for each posteam.\n\nIn the ggplot, reorder the posteam categories based on the mean value of pass in ascending or in descending order.\n\n\nAnswer\n\nNFL_2c &lt;- NFL_2b %&gt;%\n  group_by(posteam) %&gt;% \n  mutate(mean_pass = mean(pass)) %&gt;% \n  ungroup() %&gt;% \n  mutate(posteam_fct = reorder(posteam, mean_pass))\n\nggplot(data = NFL_2c, mapping = aes(x = mean_pass, y = posteam_fct))+\n  geom_point()+\n  labs(title = 'Possessing Team and Pass Plays',\n       x = 'Percentage of Pass Plays',\n       y = 'Team with possession')+\n  theme(axis.text.y = element_text(hjust = 1))+\n  theme_minimal()\n\n\n\n\nOn the scatter plot there is a positive relationship between posteam and the percentage of pass plays  Teams such as CIN, KC, LAC. and BUF have a higher occurance of pass plays,  While teams such as ATL, WAS, CHI, and NO have a lower occurance of pass plays"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2d",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2d",
    "title": "HW5 - NFL Data",
    "section": "Q2d",
    "text": "Q2d\n\nConsider the following data.frame, NFL2022_epa:\n\n\nNFL2022_epa &lt;- read_csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\n\nCreate the data.frame, NFL2022_stuffs_EPA, that includes\n\nAll the variables in the data.frame, NFL;\nThe variables, passer, receiver, and epa, from the data.frame, NFL2022_epa. by joining the two data.frames.\n\nIn the resulting data.frame, NFL2022_stuffs_EPA, remove observations with NA in passer.\n\nAnswer:\n\nNFL2022_stuffs_EPA &lt;- NFL %&gt;% \n  left_join(NFL2022_epa)\nView(NFL2022_stuffs_EPA)\n\nNFL2022_stuffs_EPA &lt;- NFL2022_stuffs_EPA %&gt;% \n  filter(!(is.na(passer)))"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2e",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2e",
    "title": "HW5 - NFL Data",
    "section": "Q2e",
    "text": "Q2e\n\nProvide both (1) a single ggplot and (2) a simple comment to describe the NFL weekly trend of weekly mean value of epa for each of the following two passers,\n\n\"J.Allen\"\n\"P.Mahomes\"\n\n\nAnswer:\n\nNFL_2e &lt;- NFL2022_stuffs_EPA %&gt;% \n  filter(passer == \"J.Allen\" | passer == \"P.Mahomes\") %&gt;% \n  group_by(passer, week) %&gt;% \n  summarise(mean_epa = mean(epa))\n\nggplot(data = NFL_2e,\n       mapping = aes(x = week, y = mean_epa, color = passer))+\n  geom_line(size = 1, linejoin = 'round')+\n  geom_point(size = 1.5)+\n  theme(legend.position = 'top')+\n  labs(title = \"Mean EPA for Two NFL Passers\",\n       x = 'Week',\n       y = 'Mean EPA')+\n  scale_color_manual(values = c('blue', 'red'))\n\n\n\n\nP.Mahomes seems to have a higher mean EPA most of the weeks, while J.Allen seems to lag behind"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2f",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2f",
    "title": "HW5 - NFL Data",
    "section": "Q2f",
    "text": "Q2f\n\nCalculate the difference between the mean value of epa for \"J.Allen\" the mean value of epa for \"P.Mahomes\" for each value of week\n\nAnswer:\n\nNFL_2f &lt;- NFL_2e %&gt;% \n  spread(passer, mean_epa) %&gt;% \n  mutate(mean_epa_dif = J.Allen - P.Mahomes)"
  },
  {
    "objectID": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2g",
    "href": "posts/DANL 200 HW5 - NFL Data/danl200-hw5-noone-daniel.html#q2g",
    "title": "HW5 - NFL Data",
    "section": "Q2g",
    "text": "Q2g\n\nSummarize the resulting data.frame in Q2d, with the following four variables:\n\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop, and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks.)\nmean_epa: Mean value of epa in 2022 for each passer\nn_pass: Number of observations for each passer\n\nThen find the top 10 NFL passers in 2022 in terms of the mean value of epa, conditioning that n_pass must be greater than or equal to the third quantile level of n_pass.\n\nAnswer:\n\nNFL_2g &lt;- NFL2022_stuffs_EPA %&gt;% \n  group_by(posteam, passer) %&gt;% \n  summarise(mean_epa = mean(epa),\n            n_pass = n())\n\nQ3_n_pass &lt;- quantile(NFL_2g$n_pass, 0.75)\n\nNFL_2g_2 &lt;- NFL_2g %&gt;% \n  filter(n_pass &gt;= Q3_n_pass) %&gt;% \n  arrange(-mean_epa) %&gt;% \n  head(10)"
  },
  {
    "objectID": "personal_posts/web_scraping1/wep_scrape_tutorial.html",
    "href": "personal_posts/web_scraping1/wep_scrape_tutorial.html",
    "title": "Web Scraping using Beautiful Soup",
    "section": "",
    "text": "First you have to import the required modules\n\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n\nHave to set the URL to the website and"
  },
  {
    "objectID": "class_projects.html",
    "href": "class_projects.html",
    "title": "Class Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDANL 200 Project\n\n\nAir Quality Data as per the EPA regarding Criteria Air Pollutants\n\n\n\n\n\n\nJan 28, 2024\n\n\nDaniel Noone\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nWeb Scraping using Beautiful Soup\n\n\nFrom a tutorial on YouTube\n\n\n\n\n\n\nFeb 9, 2024\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "basic-python-intro.html",
    "href": "basic-python-intro.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#what-is-python",
    "href": "basic-python-intro.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#variables-and-data-types",
    "href": "basic-python-intro.html#variables-and-data-types",
    "title": "Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "basic-python-intro.html#control-structures",
    "href": "basic-python-intro.html#control-structures",
    "title": "Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "basic-python-intro.html#functions",
    "href": "basic-python-intro.html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "basic-python-intro.html#lists-and-dictionaries",
    "href": "basic-python-intro.html#lists-and-dictionaries",
    "title": "Introduction to Python",
    "section": "Lists and Dictionaries",
    "text": "Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "class_blog.html",
    "href": "class_blog.html",
    "title": "Class Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nPandas Basics\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nNAME\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nSpotify HW2\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPython Basics\n\n\nA quick look at the basics of python programming\n\n\n\n\n\n\nFeb 10, 2024\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nSpotify Listener Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nHW5 - NFL Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nBeer Market Data\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nNYC Restaurant Inspections\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nDaniel Noone\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome All!\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Daniel Noone is a Junior at SUNY Geneseo and is majoring in Data Analytics.  He was born and raised in Ontario, New York and loves to spend time outdoors."
  },
  {
    "objectID": "index.html#who-is-daniel-noone",
    "href": "index.html#who-is-daniel-noone",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Daniel Noone is a Junior at SUNY Geneseo and is majoring in Data Analytics.  He was born and raised in Ontario, New York and loves to spend time outdoors."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel A. Noone",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Data Analytics | Aug 2022 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel A. Noone",
    "section": "Experience",
    "text": "Experience\nExperience programming in R, as well as basic level knowledge of Python. Extensive experience with Microsoft Office programs such as Excel, Word, and PowerPoint."
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#description-of-variables-for-beer-data-frame",
    "href": "posts/Beer_Mkts/beer_mkts.html#description-of-variables-for-beer-data-frame",
    "title": "Beer Market Data",
    "section": "Description of Variables for beer Data Frame:",
    "text": "Description of Variables for beer Data Frame:\n\nhh: Identification number of the household;\nX_purchase_desc: details of item purchased by hh\nquantity: number of items purchased by hh\nbrand: Brand of beer purchased by hh\n\nBud Light,\nBusch Light,\nCoors Light,\nMiller Lite, or\nNatural Light\n\ndollar_spent: Dollar value total of purchase;\nbeer_floz: volume of beer (fl oz);\nprice_per_floz: price per fl oz\ncontainer: Type of container of beer\npromo: Was the item promoted? (coupon, etc…)\nmarket: Scan-track market (or state if rural);\nOther demographic data is present in further variables as well…"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#summary-statistics-of-beer-data-frame",
    "href": "posts/Beer_Mkts/beer_mkts.html#summary-statistics-of-beer-data-frame",
    "title": "Beer Market Data",
    "section": "Summary Statistics of beer Data Frame:",
    "text": "Summary Statistics of beer Data Frame:\n\nskim(beer)\n\n\nData summary\n\n\nName\nbeer\n\n\nNumber of rows\n73115\n\n\nNumber of columns\n24\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nlogical\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nX_purchase_desc\n0\n1\n12\n29\n0\n115\n0\n\n\nbrand\n0\n1\n9\n13\n0\n5\n0\n\n\ncontainer\n0\n1\n3\n30\n0\n7\n0\n\n\nmarket\n0\n1\n5\n20\n0\n92\n0\n\n\nbuyertype\n0\n1\n4\n7\n0\n3\n0\n\n\nincome\n0\n1\n5\n8\n0\n5\n0\n\n\nage\n0\n1\n3\n5\n0\n4\n0\n\n\nemployment\n0\n1\n4\n4\n0\n3\n0\n\n\ndegree\n0\n1\n2\n7\n0\n4\n0\n\n\ncow\n0\n1\n4\n25\n0\n4\n0\n\n\nrace\n0\n1\n5\n8\n0\n5\n0\n\n\ntvcable\n0\n1\n4\n7\n0\n3\n0\n\n\nnpeople\n0\n1\n1\n5\n0\n5\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\npromo\n0\n1\n0.20\nFAL: 58563, TRU: 14552\n\n\nchildrenUnder6\n0\n1\n0.07\nFAL: 68109, TRU: 5006\n\n\nchildren6to17\n0\n1\n0.20\nFAL: 58155, TRU: 14960\n\n\nmicrowave\n0\n1\n0.99\nTRU: 72676, FAL: 439\n\n\ndishwasher\n0\n1\n0.73\nTRU: 53258, FAL: 19857\n\n\nsinglefamilyhome\n0\n1\n0.81\nTRU: 59058, FAL: 14057\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhh\n0\n1\n17407721.61\n11582147.34\n2000235.00\n8223438.00\n8413624.00\n30171315.00\n30440718.00\n▂▇▁▁▇\n\n\nquantity\n0\n1\n1.32\n1.15\n1.00\n1.00\n1.00\n1.00\n48.00\n▇▁▁▁▁\n\n\ndollar_spent\n0\n1\n13.78\n8.72\n0.51\n8.97\n12.99\n16.38\n159.13\n▇▁▁▁▁\n\n\nbeer_floz\n0\n1\n265.93\n199.52\n12.00\n144.00\n216.00\n360.00\n9216.00\n▇▁▁▁▁\n\n\nprice_per_floz\n0\n1\n0.06\n0.01\n0.00\n0.05\n0.06\n0.06\n0.23\n▃▇▁▁▁"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#distribution-of-brand",
    "href": "posts/Beer_Mkts/beer_mkts.html#distribution-of-brand",
    "title": "Beer Market Data",
    "section": "Distribution of brand",
    "text": "Distribution of brand\n\nbeer1 &lt;- beer %&gt;% \n  group_by(brand) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(prop = n/sum(n),\n         brand_fct = reorder(brand, prop))\n\nggplot(data = beer1)+\n  geom_bar(aes(x = brand_fct, y = prop), stat = 'identity', fill = 'blue')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Frequency of Beer Brands',\n       x = 'Brand',\n       y = 'Proportion')\n\n\n\n\n\nComments on analyses…\n\nThe brands above are in an increasing order of proportion among the brands in beer data frame\n\nThe brands are ordered as follows…\n\nBusch Light\nNatural Light\nCoors Light\nMiller Lite\nBud Light"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#brand-and-dollar-spent",
    "href": "posts/Beer_Mkts/beer_mkts.html#brand-and-dollar-spent",
    "title": "Beer Market Data",
    "section": "brand and dollar spent",
    "text": "brand and dollar spent\n\nbeer2 &lt;- beer %&gt;% \n  group_by(brand) %&gt;% \n  summarise(dollar_spent_tot = sum(dollar_spent)) %&gt;% \n  mutate(brand_fct = reorder(brand, dollar_spent_tot)) %&gt;% \n  arrange(-dollar_spent_tot)\nView(beer2)\n\nggplot(beer2)+\n  geom_bar(aes(x = brand_fct, y = dollar_spent_tot), stat = 'identity', fill = 'red')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Dollars Spent Per Brand',\n       x = 'Brand',\n       y = 'Dollars Spent')\n\n\n\n\n\nComments on analyses….\n\nThe order is the same as the distribution of brand within the beer data frame\n\nJust now the y-axis is measuring dollars spent by each value of hh on each brand\n\nBud Light seems to be most popular all around"
  },
  {
    "objectID": "posts/Beer_Mkts/beer_mkts.html#container-and-brand",
    "href": "posts/Beer_Mkts/beer_mkts.html#container-and-brand",
    "title": "Beer Market Data",
    "section": "container and brand",
    "text": "container and brand\n\nbeer3 &lt;- beer %&gt;% \n  group_by(brand, container) %&gt;% \n  summarise(n = n())\nbeer3.5 &lt;- beer3 %&gt;% \n  group_by(brand) %&gt;% \n  slice_max(order_by = n, n = 2)\nggplot(data = beer3.5)+\n  geom_bar(aes(x = brand, y = n, fill = container), stat = 'identity', position = 'dodge')+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = 'Frequency of Cans and Bottles for Each Brand',\n       x = 'Brand',\n       y = 'Count')\n\n\n\nView(beer3.5)\n\n\nComments on analyses…\n\nThe bar chart above depicts the frequency of two container types (can or non refillable bottle) per each brand\n\nAll brands seem to sell more cans than bottles\n\nOf them Bud Light sells the most cans\n\nBud light also sells the most bottles"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#description-of-variables-for-nyc_rest_ins-data-frame",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#description-of-variables-for-nyc_rest_ins-data-frame",
    "title": "NYC Restaurant Inspections",
    "section": "Description of Variables for nyc_rest_ins data frame",
    "text": "Description of Variables for nyc_rest_ins data frame\n\nCAMIS: A unique identifier number for each restaurant\nDBA: Stands for “doing business as”, name of the restaurant\nBORO: Indentifies the NYC Borough that the restaurant is located in\n\nManhattan\nBronx\nBrooklyn\nQueens\nStaten Island\n\nSTREET: Street address of the restaurant\nCUISINE DESCRIPTION: Type of cuisine the restaurant sells\nINSPECTION DATE: The date the inspection was performed\nACTION: Indicated the action(s) taken as a result of the inspection\nVIOLATION CODE: Violation associated with the restaurant’s inspection\nVIOLATION DESCRIPTION: Descriprion of the violation associated with a restaurant’s inspection\nCRITICAL FLAG: Indicator of a critical violation as result of inspection\n\nCritical\nNot Critical\nNot Applicaple\n\nSCORE: Numeric Score for the inspection\nGrade: Letter Grade for the Inspection"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#summary-statisics-for-nyc_rest_ins",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#summary-statisics-for-nyc_rest_ins",
    "title": "NYC Restaurant Inspections",
    "section": "Summary Statisics for nyc_rest_ins:",
    "text": "Summary Statisics for nyc_rest_ins:\n\nskim(nyc_rest_ins)\n\n\nData summary\n\n\nName\nnyc_rest_ins\n\n\nNumber of rows\n17633\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDBA\n0\n1\n1\n75\n0\n13914\n0\n\n\nBORO\n0\n1\n5\n13\n0\n5\n0\n\n\nSTREET\n0\n1\n5\n40\n0\n2038\n0\n\n\nCUISINE.DESCRIPTION\n0\n1\n4\n30\n0\n87\n0\n\n\nINSPECTION.DATE\n0\n1\n10\n10\n0\n751\n0\n\n\nACTION\n0\n1\n33\n47\n0\n2\n0\n\n\nVIOLATION.CODE\n0\n1\n3\n5\n0\n60\n0\n\n\nVIOLATION.DESCRIPTION\n0\n1\n19\n940\n0\n101\n0\n\n\nCRITICAL.FLAG\n0\n1\n8\n12\n0\n2\n0\n\n\nGRADE\n0\n1\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCAMIS\n0\n1\n47112537.51\n4197913.0\n30191841\n41551354\n50049290\n50094455\n50133690\n▁▁▃▁▇\n\n\nSCORE\n0\n1\n10.44\n5.9\n0\n7\n10\n12\n86\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-boro",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-boro",
    "title": "NYC Restaurant Inspections",
    "section": "GRADE and BORO",
    "text": "GRADE and BORO\nGraphical representation of the proportion of each GRADE value in each of the 5 values in BORO\n\nnyc1 &lt;- nyc_rest_ins %&gt;% \n  group_by(BORO, GRADE) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\n\nggplot(data = nyc1,\n       mapping = aes(x = GRADE, y = prop))+\n  geom_bar(aes(fill = GRADE), stat = 'identity')+\n  facet_wrap(.~BORO)\n\n\n\n\n\nComments on analyses…\n\nAll five boroughs seem to have around equal proportions of each Grade"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#critical-flag-and-boro",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#critical-flag-and-boro",
    "title": "NYC Restaurant Inspections",
    "section": "CRITICAL FLAG and BORO",
    "text": "CRITICAL FLAG and BORO\nGraphical representation of the count of each value of CRITICAL FLAG per each value of BORO\n\nnyc2 &lt;- nyc_rest_ins %&gt;% \n  group_by(BORO, CRITICAL.FLAG) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\nView(nyc2)\n\nggplot(data = nyc2,\n       mapping = aes(x = CRITICAL.FLAG, y = n))+\n  geom_bar(aes(fill = CRITICAL.FLAG), stat = 'identity')+\n  facet_wrap(.~BORO)\n\n\n\n\n\nComments on analyses…\n\nThe proportion of each value of CRITICAL.FLAG within each of the values of BORO seems to be about equal\nThis faceted bar chart also gives info about the number of restaurants in each value of BORO within this data set. In order of most to least…\n\nManhattan has the most restaurants\nBrooklyn has the second most\nQueens the third most\nBronx second to last\nStaten Island is last"
  },
  {
    "objectID": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-critical-flag",
    "href": "posts/nyc_inspections/NYC_RES_INS.html#grade-and-critical-flag",
    "title": "NYC Restaurant Inspections",
    "section": "GRADE and CRITICAL FLAG",
    "text": "GRADE and CRITICAL FLAG\nGraphical representation of the proportion of each value of GRADE per value of CRITICAL FLAG\n\nnyc3 &lt;- nyc_rest_ins %&gt;% \n  group_by(GRADE, CRITICAL.FLAG, BORO) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(prop = n/sum(n))\nView(nyc3)\n\nggplot(data = nyc3)+\n  geom_bar(mapping = aes(x = CRITICAL.FLAG, y = prop, fill = GRADE), stat = 'identity', position = 'dodge')\n\n\n\n\n\nComments on analyses…\n\nThe proportion of each value of GRADE seems to be almost equal per value of CRITICAL FLAG\nThe proportion of grade A seems to be slightly less for restaurants with No Critical Flags\nThe proportions of grades B and C appear to be slightly less for restautants with Critical Flags"
  },
  {
    "objectID": "posts/Py_Basics/python_basics_HW1.html",
    "href": "posts/Py_Basics/python_basics_HW1.html",
    "title": "Python Basics",
    "section": "",
    "text": "1 Introduction\nThis blog post is meant to give a brief introduction of some of the main topics in basic python programming\n\n\n2 Variables & Assignment\n\nIn python programming, variables are just names for values not actual storage places. \nThese values can be singular such as a numeric value of 10, but there can also be multiple values of the same or varying data types \nan equal sign = is used to assign values to their respective variables (or objects)\n\n\n# Single value\nx = 10\n\n# Multiple values\ny = [1,2,3]\n\n#Multiple types\nz = ['a','b','c',1,2,3]\n\n\n\n3 Data Types in Python\nThere are multiple different data types recognized by the python language.\nThe most basic types are as follows:\n\n# Integer, numeric values with no decimals\nx = 10\n\n# Float, numeric values with decimals\ny = 10.99\n\n# String, character values surrounded by '' or \"\"\nz = 'hello, world' #OR \nz = \"hello, world\"\n\n# Boolean, logical values\nx = True\ny = False\n\n# Nothing, no value\nz = None\n\n# List, data container for any data type (even multiple types at once)\nlist_ = [10, 1.55, 'apple', True, False, None]\n\n# Tuple, non mutable data container\ntup = (2, 3 , 4)\n\n# Set\nset_ = {'a', 'b'}\n\n# Dictionary\ndict_ = {'first': 'a', 'second': 'b'}\n\n\n\n4 Operators\nMathematical operations can also be done in python using preset operators:\n\n# Addition\nx = 5 + 5 # x would have value of 10\ny = x + 5 # y would have value of 15\n\n# Subtraction\nx = 5 - 1 # x would have value of 4\ny = x - 2 # y would have value of 2\n\n# Multiplication and Exponents (* for multiplication, ** for exponents)\nx = 5 * 2**2 # x would have value of 20\ny = x**2 * 5 # y would have value of 2000\n\n# Division and Integer Division (if want int type out of operation, use //)\nx = 4/2 # x would have value of 2.0\ny = x/2 # y would have value of 1.0\n\nx = 8//2 # x would have value of 4 (int type not float)\ny = x//2 # y would have value of 2 (again, int not float)\n\n# Can also perform string concatenation\nstr_one = 'hello,'\nstr_two = ' world'\nstr_ = str_one + str_two # Result of summation is concatenation of str_one and str_two \n                          # = 'hello, world'\n\n\n\n5 Conditionals\nCan perform conditional operations in python\n\nIf statements\nIf else\netc…\n\n\n# Can use if and elif and else to formulate conditional operations\n# For example...\n\nname = 'David'\nscore = 99\n\nif name == 'David' and score &gt;= 90:\n  print('Great job, David!')\nelif name != 'David' and score &gt;= 90:\n  print('Great job, stranger!')\nelif name == 'David' and score &lt; 90:\n  print('You failed, David!')\nelif name != 'David' and score &lt; 90:\n  print('You failed, stranger!')\n\nGreat job, David!\n\n\n-&gt; Here the result is 'Great job, David!' since the name is 'David' and the score is 99\n\n\n6 Casting Variables\nCasting is when we explicitly assign a data type to a variable\n\nstr() will convert to string type\nint() will convert to integer type\nfloat() will convert to float type\n\n\nx = 10 # integer type\ny = '20' # string type\nz = 1.55 # float type\n\nx_ = float(x) # makes 10 into float type = 10.0\ny_ = int(y) # makes '20' into int type = 20\nz_ = int(z) # makes 1.55 into int type = 1 (DOES NOT ROUND)\nx_str = str(x) # makes 10 into str type = '10'\n\n\n\n7 Slicing Methods\n\nPython uses indexing to slice data\nThe index starts at 0, not 1\n\n\n\nAs seen in the image, negative values can also be used\nThe syntax is as follows\n\n[:] - will return the whole value\n[start :] - will return from a starting index value until the end\n[: end] - will return from the start until the end index value\n[start : end] - Will return from start value to end value\n[start : end: step] - will return from start to end by a certain step amount\n\n\n\nstring = 'abcdefghij'\n\nstring[:5] # returns 'abcde' (first 5 characters of string)\nstring[5:] # returns 'fghij' (last 5 characters of string)\nstring[0:5:2] # returns 'ace' (first 5 characters, but in steps of 2)\n\nlist_ = ['apple','banana','pear','strawberry','papaya','pineapple']\n\nlist_[:3] # returns ['apple', 'banana', 'pear'] (first 3 elements of list)\nlist_[3:] # returns ['strawberry', 'papaya', 'pineapple'] (last 3 elements of list)\nlist_[0::2] # returns ['apple', 'pear', 'papaya'] (whole list by steps of 2)\n\n['apple', 'pear', 'papaya']\n\n\n\n\n8 Importing Modules, Packages and Libraries\n\nModule - a bunch of related code saved in a file with extension .py\nPackage - directory of a collection of modules\nLibrary - a collection of packages\nimport is called on a module to be able to use it\n\nThen have to call module along with any functions associated\n\n\n\nimport pandas\n# could then use pandas.read_csv() to read a csv\n\n\nimport as is called on a module to give it a name to make things more efficient\n\n\nimport pandas as pd\n# Then could use pd.read_csv() instead of pandas.read_csv()\n\n\nfrom and import is used to load a specific function from a module\n\n\nfrom pandas import read_csv\n# then can just use read_csv() as if it were a base function\n\n\nIn order to install a module, package or library - have to call pip install in terminal or !pip install if on Google Colab\n\n!pip install is also used for Rstudio\n\n\n\n# !pip install itables"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2.html",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2.html",
    "title": "Spotify HW2",
    "section": "",
    "text": "import pandas as pd\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')"
  },
  {
    "objectID": "posts/Spotify_HW2_DANL 210/spotify_hw2.html#description-of-variables",
    "href": "posts/Spotify_HW2_DANL 210/spotify_hw2.html#description-of-variables",
    "title": "Spotify HW2",
    "section": "Description of Variables",
    "text": "Description of Variables\n\npid - A unique id number for each playlist\nplaylist_name - A name given to each playlist\npos - The position of song in the playlist\nartist_name - Name of the artist of the song\ntrack_name - Name of the song\nduration_ms - The length of the song in miliseconds\nalbum_name - Name of album the song is in"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html",
    "href": "posts2/DANL200 Project/project.html",
    "title": "DANL 200 Project",
    "section": "",
    "text": "These data sets were obtained from the EPA website here\nThese data sets (four in total) measure daily levels of four different gases in different locations\n\nThe gases measured are Ozone, SO2 (sulfur dioxide), CO (carbon monoxide), and NO2 (nitrogen dioxide)\nThese gases play a large role in the measurement of the Air Quality Index (AQI)\nThey are considered to be atmospheric pollutants\nMore information on Criteria Air Pollutants can be found here\n\nThis project will take a look at and analyze the measurements of these four atmospheric gases per state\n\nThese measurements were taken from 01/01/2023 to 09/30/2023\n\n\n\n\n\nozone &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\ozone_measures.csv\")\n\nso2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\so2_measures.csv\")\n\nco &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\co_measures.csv\")\n\nno2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\no2_measures.csv\")\n\n\nrmarkdown::paged_table(ozone)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(co)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2)\n\n\n\n  \n\n\n\n\n\n\n\nThe data sets include a lot of information that is not needed for my analysis\n\nHere I will alter the data sets to include only the information of importance\nThe data sets will then be row bound using rbind() to create a single data set from the four\n\nFor the purpose of continuity, the top 10 most frequent states in ozone data set will also be selected in the other three data sets\n\n\n\n\nst_top10_oz &lt;- ozone %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(10) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nozone_alt &lt;- ozone %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_oz$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\nView(ozone_alt)\n\n\nThe new data set ozone_alt includes data for only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_oz)\n\n\n\n  \n\n\nrmarkdown::paged_table(ozone_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_so2 &lt;- so2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;%\n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nso2_alt &lt;- so2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_so2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nThe new data set so2_alt includes data for again, only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_co &lt;- co %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nco_alt &lt;- co %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_co$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_co)\n\n\n\n  \n\n\nrmarkdown::paged_table(co_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_no2 &lt;- no2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nno2_alt &lt;- no2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_no2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_no2)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2_alt)\n\n\n\n  \n\n\n\n\n\n\n\n\nSince the variables are the same, all four data sets can be row bound together using rbind()\n\n\nair_qual &lt;- rbind(ozone_alt, so2_alt, co_alt, no2_alt)\nrmarkdown::paged_table(air_qual)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#loading-the-data-sets",
    "href": "posts2/DANL200 Project/project.html#loading-the-data-sets",
    "title": "DANL 200 Project",
    "section": "",
    "text": "ozone &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\ozone_measures.csv\")\n\nso2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\so2_measures.csv\")\n\nco &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\co_measures.csv\")\n\nno2 &lt;- read.csv(\"C:\\\\Users\\\\Dan\\\\OneDrive\\\\Documents\\\\dan-noone.github.io\\\\posts2\\\\DANL200 Project\\\\no2_measures.csv\")\n\n\nrmarkdown::paged_table(ozone)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(co)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#altering-the-four-data-sets",
    "href": "posts2/DANL200 Project/project.html#altering-the-four-data-sets",
    "title": "DANL 200 Project",
    "section": "",
    "text": "The data sets include a lot of information that is not needed for my analysis\n\nHere I will alter the data sets to include only the information of importance\nThe data sets will then be row bound using rbind() to create a single data set from the four\n\nFor the purpose of continuity, the top 10 most frequent states in ozone data set will also be selected in the other three data sets\n\n\n\n\nst_top10_oz &lt;- ozone %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  head(10) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nozone_alt &lt;- ozone %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_oz$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\nView(ozone_alt)\n\n\nThe new data set ozone_alt includes data for only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_oz)\n\n\n\n  \n\n\nrmarkdown::paged_table(ozone_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_so2 &lt;- so2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;%\n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nso2_alt &lt;- so2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_so2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nThe new data set so2_alt includes data for again, only the 10 most prevalent states in the ozone data set\n\n\nrmarkdown::paged_table(st_top10_so2)\n\n\n\n  \n\n\nrmarkdown::paged_table(so2_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_co &lt;- co %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nco_alt &lt;- co %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_co$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_co)\n\n\n\n  \n\n\nrmarkdown::paged_table(co_alt)\n\n\n\n  \n\n\n\n\n\n\n\nst_top10_no2 &lt;- no2 %&gt;% \n  filter(State.Name %in% st_top10_oz$State.Name) %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(n = n()) %&gt;% \n  arrange(-n) %&gt;% \n  mutate(state_fct = reorder(State.Name, n))\n\nno2_alt &lt;- no2 %&gt;% \n  select(State.Name, Date.Local, Parameter.Name, Units.of.Measure, Arithmetic.Mean) %&gt;% \n  filter(State.Name %in% st_top10_no2$state_fct) %&gt;% \n  rename(Measure = Arithmetic.Mean)\n\n\nrmarkdown::paged_table(st_top10_no2)\n\n\n\n  \n\n\nrmarkdown::paged_table(no2_alt)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#binding-all-four-data-sets-ozone_alt-so2_alt-co_alt-and-no2_alt-to-create-the-air_qual-data-set",
    "href": "posts2/DANL200 Project/project.html#binding-all-four-data-sets-ozone_alt-so2_alt-co_alt-and-no2_alt-to-create-the-air_qual-data-set",
    "title": "DANL 200 Project",
    "section": "",
    "text": "Since the variables are the same, all four data sets can be row bound together using rbind()\n\n\nair_qual &lt;- rbind(ozone_alt, so2_alt, co_alt, no2_alt)\nrmarkdown::paged_table(air_qual)"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#description-of-the-variables",
    "href": "posts2/DANL200 Project/project.html#description-of-the-variables",
    "title": "DANL 200 Project",
    "section": "2.1 Description of the Variables",
    "text": "2.1 Description of the Variables\n\nState.Name: A character variable whose values are one of the 10 states most frequent in the ozone data set\n\nArizona\nCalifornia\nColorado\nFlorida\nIllinois\nIndiana\nOhio\nPennsylvania\nTexas\nUtah\n\nDate.Local: The date the measurement of the atmospheric gas was taken\nParameter.Name: The atmospheric gas being measured\nUnits.of.Measure: Units in which the atmospheric gas was measured\n\nParts Per Million : Ozone, Carbon Monoxide\nParts Per Billion : Sulfur Dioxide, Nitrogen Dioxide\n\nMeasure: The mean measurement of the gas in the units provided in Units.of.Measure"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#distribution-of-each-value-of-state.name-per-value-of-parameter.name",
    "href": "posts2/DANL200 Project/project.html#distribution-of-each-value-of-state.name-per-value-of-parameter.name",
    "title": "DANL 200 Project",
    "section": "2.2 Distribution of each value of State.Name per value of Parameter.Name",
    "text": "2.2 Distribution of each value of State.Name per value of Parameter.Name\n\nThe distribution of each state in the ten provided, per each atmospheric gas being measured\n\n\nair_qual_state_dist &lt;- air_qual %&gt;% \n  group_by(State.Name, Parameter.Name) %&gt;% \n  summarise(n = n())\nView(air_qual_state_dist)\n\nggplot(data = air_qual_state_dist)+\n  geom_bar(aes(y = State.Name, x = n, fill = State.Name), stat = 'identity')+\n  facet_wrap(.~Parameter.Name)+\n  theme_gray()+\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n  labs(title = 'State Distribution per Gas',\n       x = 'Count',\n       y = 'State')\n\n\n\n\nAnalyses\n\nDepicted in the above figure is a faceted bar chart depicting the count of each state in which each atmospheric gas was measured\n\nIt seems as though carbon monoxide (CO), nitrogen dioxide (NO2), and ozone were measured most often in California\nSulfur dioxide (SO2) was measured most frequently in Ohio and secondly in Texas"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-ozone-measurement-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-ozone-measurement-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.3 The average measurement of the OZONE measurement per each State.Name value",
    "text": "2.3 The average measurement of the OZONE measurement per each State.Name value\n\nThe average measurement of ozone in parts per million (ppm) per each state in the top ten most frequent\n\n\nozone_alt_per_state &lt;- ozone_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppm = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppm))\n\nggplot(data = ozone_alt_per_state)+\n  geom_bar(aes(y = state_fct, x = mean_ppm, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Ozone Levels in ppm per State',\n       x = ' Mean ppm Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nIn the bar chart shown above depicts a decent variation in Ozone levels between the 10 states\n\nTexas, Florida, and California seem to have the lowest levels of Ozone\n\nColorado, Utah, and Arizona seem to have the highest levels of Ozone"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-sulfur-dioxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-sulfur-dioxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.4 The average measurement of the SULFUR DIOXIDE measurment per each State.Name value",
    "text": "2.4 The average measurement of the SULFUR DIOXIDE measurment per each State.Name value\n\nThe average measurment of sulfur dioxide (SO2) in parts per billion (ppb) per each state in the top ten most frequent\n\n\nso2_alt_per_state &lt;- so2_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppb = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppb))\n\nggplot(data = so2_alt_per_state)+\n  geom_bar(aes(x = mean_ppb, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Sulfur Dioxide Levels in ppb per State',\n       x = 'Mean ppb Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nThere seems to be even greater variation between each of the state’s Sulfur Dioxide (SO2) levels\n\nCalifornia, Arizona, and Ohio seem to have the lowest levels of SO2\n\nColorado, Texas, and Utah seem to have the highest levels of SO2"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-carbon-monoxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-carbon-monoxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.5 The average measurement of the CARBON MONOXIDE measurment per each State.Name value",
    "text": "2.5 The average measurement of the CARBON MONOXIDE measurment per each State.Name value\n\nThe average measurment of carbon monoxide (CO) in parts per million (ppm) per each state in the top ten most frequent\n\n\nco_alt_per_state &lt;- co_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppm = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppm))\n\nggplot(data = co_alt_per_state)+\n  geom_bar(aes(x = mean_ppm, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Carbon Monoxide Levels in ppm per State',\n       x = 'Mean ppm Value',\n       y = 'State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nIn the bar chart, there seems to be less variation in Carbon Monoxide (CO) levels as compared to Sulfur Dioxide (SO2)\n\nUtah, Ohio, and Illinois seem to have the lowest levels of CO\n\nIndiana, Florida, and Pennsylvania seem to have the highest levels of CO"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-nitrogen-dioxide-measurment-per-each-state.name-value",
    "href": "posts2/DANL200 Project/project.html#the-average-measurement-of-the-nitrogen-dioxide-measurment-per-each-state.name-value",
    "title": "DANL 200 Project",
    "section": "2.6 The average measurement of the NITROGEN DIOXIDE measurment per each State.Name value",
    "text": "2.6 The average measurement of the NITROGEN DIOXIDE measurment per each State.Name value\n\nThe average measurement of nitrogen dioxide (NO2) in parts per billion (ppb) per each state in the top ten most frequent\n\n\nno2_alt_per_state &lt;- no2_alt %&gt;% \n  group_by(State.Name) %&gt;% \n  summarise(mean_ppb = mean(Measure)) %&gt;% \n  mutate(state_fct = reorder(State.Name, mean_ppb))\n\nggplot(data = no2_alt_per_state)+\n  geom_bar(aes(x = mean_ppb, y = state_fct, fill = state_fct), stat = 'identity')+\n  theme_dark()+\n  labs(title = 'Nitrogen Dioxide Levels in ppb per State',\n       x = 'Mean ppb Value',\n       y = ' State',\n       fill = 'State')+\n  scale_fill_viridis_d(option = 'inferno')\n\n\n\n\nAnalyses\n\nThe variation of Nitrogen Dioxide (NO2) seems to be similar to that of Sulfur Dioxide (SO2)\n\nIndiana, Texas, and Utah seem to have the lowest levels of NO2\n\nArizona, Illinois, and Colorado seem to have the highest levels of NO2"
  },
  {
    "objectID": "posts2/DANL200 Project/project.html#what-do-these-levels-even-mean",
    "href": "posts2/DANL200 Project/project.html#what-do-these-levels-even-mean",
    "title": "DANL 200 Project",
    "section": "3.1 What do these levels even mean?",
    "text": "3.1 What do these levels even mean?\n\nThese four gases (Ozone, SO2, CO, and NO2) are considered to be Criteria Air Pollutants by the EPA\n\nBeing Criteria Air Pollutants (along with Lead (Pb), and Particulate Matter (PM)), they pose adverse health and environmental effects.\n\n\n\n\n3.1.1 Ozone\n\nGround-level ozone (as measured in the data), comes from reaction between pollutants\n\nThese pollutants are emitted by industrial facilities, electric utilities, and motor vehicles\n\nThis Ground-level ozone (in contrast with ozone that protects from UV rays), can pose health risks\n\nThese health risks include…\n\nInflammation of lining of the lungs\nReduced lung function\nCough, wheezing, chest pain, shortness of breath\nIncreased susceptibility to respiratory infection, and\nPremature mortality, to name a few.\n\n\n\nAnalyses Pertaining to the Data:\n\nFor the states such as Colorado, Utah, and Arizona some of these health effects could potentially be more frequent\n\nHowever, in states such as Texas, Florida, and California these health effects will most likely not be as prevalent\n\n\n\n\n3.1.2 Sulfur Dioxide (SO2)\n\nThis air pollutant comes primarily from fossil fuel combustion by industrial and electrical facilities\nShort-term exposure by asthmatic individuals may result in…\n\nWheezing\nChest Tightness\nShortness of Breath\nEtc…\n\nMostly affects the respiratory system\n\nAnalyses Pertaining to the Data:\n\nColorado would most likely experience the highest frequency of these health effects as the average measure of SO2 over the time frame is almost double that of Texas (the state with the second highest measurement)\n\nCalifornia is the state with the lowest average level of SO2, and would most likely see the least health effects\n\n\n\n\n3.1.3 Carbon Monoxide (CO)\n\nThe primary source of Carbon Monoxide is found to be fuel propelled means of transportation\nPrimary health risks as a result of exposure are…\n\nDecreased capacity of blood to carry oxygen\n\nWhich can cause myocardial ischemia (reduced oxygen to heart)\nand angina (chest pain)\n\n\n\nAnalyses Pertaining to the Data:\n\nStates such as Indiana, Florida, and Pennsylvania which had the highest average levels of CO, would most likely see the highest frequency of health effects\n\nUtah, which had the lowest average level of CO (almost half the average level of Indiana), would most likely experience the lowest frequency of health effects\n\n\n\n\n3.1.4 Nitrogen Dioxide (NO2)\n\nPrimarily emitted from cars, trucks, buses, power plants, and other engines/equipment\n\nNitrogen Oxide (NO) is emitted first and then rapidly oxidized into NO2 in the atmosphere\n\nHealth risks include mostly respiratory related symptoms and hospital visits\n\nAnalyses Pertaining to the Data:\n\nStates such as Arizona and Illinois had the highest average level of NO2 recorded, so they would most likely experience the highest frequency of these health effects\n\nStates such as Utah and Texas had an average recorded level of NO2 about half that of Arizona and Illinois, and would most likely experience the lowest frequency of health effects"
  }
]